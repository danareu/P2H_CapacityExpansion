{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module P2H_CapacityExpansion.\n",
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/git`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "include(\"./P2H_CapacityExpansion.jl\")\n",
    "cd(\"/cluster/home/danare/git\")\n",
    "Pkg.activate(\".\")\n",
    "using .P2H_CapacityExpansion\n",
    "using DataFrames\n",
    "using Parameters\n",
    "using Flux\n",
    "using Surrogates\n",
    "using ScikitLearn\n",
    "using LinearAlgebra, Random, Statistics\n",
    "using JuMP\n",
    "using XLSX\n",
    "using PlotlyJS\n",
    "using Clustering\n",
    "using CSV\n",
    "using Dates\n",
    "using StatsBase, MultivariateStats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/cluster/home/danare/git/P2H_CapacityExpansion/results/500_scenarios_V3.txt\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file = \"/cluster/home/danare/git/P2H_CapacityExpansion/results/aggregated_results/500_scenarios.txt\"\n",
    "file = \"/cluster/home/danare/git/P2H_CapacityExpansion/results/500_scenarios_V3.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Data into Training and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([10.0 16.0 … 23.0 0.0; 6.0 13.0 … 23.0 0.0; … ; 0.0 6.0 … 28.0 0.0; 18.0 424.0 … 83.0 0.0], [1532.0 43025.0 1.12446958e8; 3055.0 46093.0 1.27946124e8; … ; 1963.0 57969.0 1.51583049e8; 1257.0 40193.0 0.0], [32.0 36.0 … 22.0 0.0; 34.0 44.0 … 23.0 0.0; … ; 0.0 9.0 … 38.0 0.0; 12.0 15.0 … 23.0 0.0], [1385.0 44664.0 1.52371092e8; 1305.0 43858.0 1.40664468e8; … ; 1185.0 43358.0 9.1344118e7; 1624.0 43308.0 1.24961636e8])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_raw = P2H_CapacityExpansion.read_txt_file(file);\n",
    "df_raw = select(df_raw, Not(:ENS))\n",
    "X_train, y_train, X_test, y_test = P2H_CapacityExpansion.partitionTrainTest(df_raw, [:Cost,:Generation, :Emission], 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### scale the data ###\n",
    "X_train_scaled, μX, σX  = P2H_CapacityExpansion.scaling(X_train)\n",
    "X_test_scaled = (X_test .- μX) ./ σX\n",
    "y_train_scaled, μy, σy  = P2H_CapacityExpansion.scaling(y_train)\n",
    "    \n",
    "# remove np.nan #\n",
    "for i in eachindex(X_test_scaled)\n",
    "    if isnan(X_test_scaled[i])\n",
    "        X_test_scaled[i] = 0.0\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate though the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = Dict(\n",
    "    #\"RandomForest\" => P2H_CapacityExpansion.random_forest_sklearn,\n",
    "    #\"DecisionTree\" => P2H_CapacityExpansion.decision_tree_sklearn,\n",
    "    \"LinearRegression\" => P2H_CapacityExpansion.linear_regression_sklearn,\n",
    "    \"NeuralNetwork\" => P2H_CapacityExpansion.simple_neural_network_sklearn,\n",
    "    #\"GaussianProcesses\" => P2H_CapacityExpansion.gaussian_process,\n",
    "    #\"SVR\" => P2H_CapacityExpansion.svr_sklearn,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kmeans_subset (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function kmeans_subset(X, n)\n",
    "    R = kmeans(Matrix(X)', n) \n",
    "    idx = [findfirst(==(i), R.assignments) for i in 1:n]\n",
    "    return idx\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_models = Set([\"RandomForest\", \"NeuralNetwork\", \"DecisionTree\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork\n",
      "\n",
      "Iteration 68, loss = 0.04678217\n",
      "Iteration 69, loss = 0.04612555\n",
      "Iteration 70, loss = 0.04548684\n",
      "Iteration 71, loss = 0.04485768\n",
      "Iteration 72, loss = 0.04423961\n",
      "Iteration 73, loss = 0.04363117\n",
      "Iteration 74, loss = 0.04303329\n",
      "Iteration 75, loss = 0.04245425\n",
      "Iteration 76, loss = 0.04188239\n",
      "Iteration 77, loss = 0.04132197\n",
      "Iteration 78, loss = 0.04076942\n",
      "Iteration 79, loss = 0.04022826\n",
      "Iteration 80, loss = 0.03970065\n",
      "Iteration 81, loss = 0.03918445\n",
      "Iteration 82, loss = 0.03867562\n",
      "Iteration 83, loss = 0.03817742\n",
      "Iteration 84, loss = 0.03769026\n",
      "Iteration 85, loss = 0.03720477\n",
      "Iteration 86, loss = 0.03672549\n",
      "Iteration 87, loss = 0.03625366\n",
      "Iteration 88, loss = 0.03578939\n",
      "Iteration 89, loss = 0.03533138\n",
      "Iteration 90, loss = 0.03487438\n",
      "Iteration 91, loss = 0.03442659\n",
      "Iteration 92, loss = 0.03398742\n",
      "Iteration 93, loss = 0.03355855\n",
      "Iteration 94, loss = 0.03313404\n",
      "Iteration 95, loss = 0.03271315\n",
      "Iteration 96, loss = 0.03229692\n",
      "Iteration 97, loss = 0.03188413\n",
      "Iteration 98, loss = 0.03147974\n",
      "Iteration 99, loss = 0.03107847\n",
      "Iteration 100, loss = 0.03068026\n",
      "Iteration 101, loss = 0.03028645\n",
      "Iteration 102, loss = 0.02989771\n",
      "Iteration 103, loss = 0.02951356\n",
      "Iteration 104, loss = 0.02913250\n",
      "Iteration 105, loss = 0.02875657\n",
      "Iteration 106, loss = 0.02838311\n",
      "Iteration 107, loss = 0.02801257\n",
      "Iteration 108, loss = 0.02764524\n",
      "Iteration 109, loss = 0.02727792\n",
      "Iteration 110, loss = 0.02691685\n",
      "Iteration 111, loss = 0.02655993\n",
      "Iteration 112, loss = 0.02620773\n",
      "Iteration 113, loss = 0.02586292\n",
      "Iteration 114, loss = 0.02552317\n",
      "Iteration 115, loss = 0.02517941\n",
      "Iteration 116, loss = 0.02483283\n",
      "Iteration 117, loss = 0.02448887\n",
      "Iteration 118, loss = 0.02415595\n",
      "Iteration 119, loss = 0.02382351\n",
      "Iteration 120, loss = 0.02349771\n",
      "Iteration 121, loss = 0.02317046\n",
      "Iteration 122, loss = 0.02285127\n",
      "Iteration 123, loss = 0.02253702\n",
      "Iteration 124, loss = 0.02222554\n",
      "Iteration 125, loss = 0.02192336\n",
      "Iteration 126, loss = 0.02162824\n",
      "Iteration 127, loss = 0.02133284\n",
      "Iteration 128, loss = 0.02103690\n",
      "Iteration 129, loss = 0.02074376\n",
      "Iteration 130, loss = 0.02045135\n",
      "Iteration 131, loss = 0.02015804\n",
      "Iteration 132, loss = 0.01986461\n",
      "Iteration 133, loss = 0.01956971\n",
      "Iteration 134, loss = 0.01927629\n",
      "Iteration 135, loss = 0.01898556\n",
      "Iteration 136, loss = 0.01870026\n",
      "Iteration 137, loss = 0.01841736\n",
      "Iteration 138, loss = 0.01813951\n",
      "Iteration 139, loss = 0.01786365\n",
      "Iteration 140, loss = 0.01758810\n",
      "Iteration 141, loss = 0.01731450\n",
      "Iteration 142, loss = 0.01704448\n",
      "Iteration 143, loss = 0.01678135\n",
      "Iteration 144, loss = 0.01652047\n",
      "Iteration 145, loss = 0.01626227\n",
      "Iteration 146, loss = 0.01600628\n",
      "Iteration 147, loss = 0.01575545\n",
      "Iteration 148, loss = 0.01550809\n",
      "Iteration 149, loss = 0.01526121\n",
      "Iteration 150, loss = 0.01502070\n",
      "Iteration 151, loss = 0.01478510\n",
      "Iteration 152, loss = 0.01455427\n",
      "Iteration 153, loss = 0.01432695\n",
      "Iteration 154, loss = 0.01410402\n",
      "Iteration 155, loss = 0.01388913\n",
      "Iteration 156, loss = 0.01367001\n",
      "Iteration 157, loss = 0.01344519\n",
      "Iteration 158, loss = 0.01322512\n",
      "Iteration 159, loss = 0.01300873\n",
      "Iteration 160, loss = 0.01279490\n",
      "Iteration 161, loss = 0.01258450\n",
      "Iteration 162, loss = 0.01237533\n",
      "Iteration 163, loss = 0.01217089\n",
      "Iteration 164, loss = 0.01196823\n",
      "Iteration 165, loss = 0.01176838\n",
      "Iteration 166, loss = 0.01156937\n",
      "Iteration 167, loss = 0.01137410\n",
      "Iteration 168, loss = 0.01118193\n",
      "Iteration 169, loss = 0.01099164\n",
      "Iteration 170, loss = 0.01080572\n",
      "Iteration 171, loss = 0.01062175\n",
      "Iteration 172, loss = 0.01044069\n",
      "Iteration 173, loss = 0.01026160\n",
      "Iteration 174, loss = 0.01008547\n",
      "Iteration 175, loss = 0.00991254\n",
      "Iteration 176, loss = 0.00974613\n",
      "Iteration 177, loss = 0.00958317\n",
      "Iteration 178, loss = 0.00942061\n",
      "Iteration 179, loss = 0.00926205\n",
      "Iteration 180, loss = 0.00910796\n",
      "Iteration 181, loss = 0.00895682\n",
      "Iteration 182, loss = 0.00880967\n",
      "Iteration 183, loss = 0.00866987\n",
      "Iteration 184, loss = 0.00853185\n",
      "Iteration 185, loss = 0.00839372\n",
      "Iteration 186, loss = 0.00825726\n",
      "Iteration 187, loss = 0.00812203\n",
      "Iteration 188, loss = 0.00799115\n",
      "Iteration 189, loss = 0.00785932\n",
      "Iteration 190, loss = 0.00772906\n",
      "Iteration 191, loss = 0.00760139\n",
      "Iteration 192, loss = 0.00747530\n",
      "Iteration 193, loss = 0.00735143\n",
      "Iteration 194, loss = 0.00723170\n",
      "Iteration 195, loss = 0.00711335\n",
      "Iteration 196, loss = 0.00699682\n",
      "Iteration 197, loss = 0.00688248\n",
      "Iteration 198, loss = 0.00676937\n",
      "Iteration 199, loss = 0.00666283\n",
      "Iteration 200, loss = 0.00656005\n",
      "Iteration 201, loss = 0.00646171\n",
      "Iteration 202, loss = 0.00636606\n",
      "Iteration 203, loss = 0.00627123\n",
      "Iteration 204, loss = 0.00617715\n",
      "Iteration 205, loss = 0.00608351\n",
      "Iteration 206, loss = 0.00599155\n",
      "Iteration 207, loss = 0.00590479\n",
      "Iteration 208, loss = 0.00581805\n",
      "Iteration 209, loss = 0.00573435\n",
      "Iteration 210, loss = 0.00565028\n",
      "Iteration 211, loss = 0.00556903\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.50977421\n",
      "Iteration 2, loss = 0.47662997\n",
      "Iteration 3, loss = 0.44612999\n",
      "Iteration 4, loss = 0.41806228\n",
      "Iteration 5, loss = 0.39217340\n",
      "Iteration 6, loss = 0.36816627\n",
      "Iteration 7, loss = 0.34589869\n",
      "Iteration 8, loss = 0.32521570\n",
      "Iteration 9, loss = 0.30596530\n",
      "Iteration 10, loss = 0.28805088\n",
      "Iteration 11, loss = 0.27138621\n",
      "Iteration 12, loss = 0.25589554\n",
      "Iteration 13, loss = 0.24141587\n",
      "Iteration 14, loss = 0.22792749\n",
      "Iteration 15, loss = 0.21535736\n",
      "Iteration 16, loss = 0.20369147\n",
      "Iteration 17, loss = 0.19293182\n",
      "Iteration 18, loss = 0.18304918\n",
      "Iteration 19, loss = 0.17405210\n",
      "Iteration 20, loss = 0.16584552\n",
      "Iteration 21, loss = 0.15844623\n",
      "Iteration 22, loss = 0.15182784\n",
      "Iteration 23, loss = 0.14594281\n",
      "Iteration 24, loss = 0.14069063\n",
      "Iteration 25, loss = 0.13598639\n",
      "Iteration 26, loss = 0.13167287\n",
      "Iteration 27, loss = 0.12763034\n",
      "Iteration 28, loss = 0.12375993\n",
      "Iteration 29, loss = 0.11999109\n",
      "Iteration 30, loss = 0.11628171\n",
      "Iteration 31, loss = 0.11259913\n",
      "Iteration 32, loss = 0.10895395\n",
      "Iteration 33, loss = 0.10536040\n",
      "Iteration 34, loss = 0.10185259\n",
      "Iteration 35, loss = 0.09847912\n",
      "Iteration 36, loss = 0.09525443\n",
      "Iteration 37, loss = 0.09218049\n",
      "Iteration 38, loss = 0.08928405\n",
      "Iteration 39, loss = 0.08657967\n",
      "Iteration 40, loss = 0.08405255\n",
      "Iteration 41, loss = 0.08168823\n",
      "Iteration 42, loss = 0.07945923\n",
      "Iteration 43, loss = 0.07737766\n",
      "Iteration 44, loss = 0.07543840\n",
      "Iteration 45, loss = 0.07361982\n",
      "Iteration 46, loss = 0.07190921\n",
      "Iteration 47, loss = 0.07029137\n",
      "Iteration 48, loss = 0.06875905\n",
      "Iteration 49, loss = 0.06730075\n",
      "Iteration 50, loss = 0.06589467\n",
      "Iteration 51, loss = 0.06452007\n",
      "Iteration 52, loss = 0.06319300\n",
      "Iteration 53, loss = 0.06190883\n",
      "Iteration 54, loss = 0.06066235\n",
      "Iteration 55, loss = 0.05945453\n",
      "Iteration 56, loss = 0.05828853\n",
      "Iteration 57, loss = 0.05717068\n",
      "Iteration 58, loss = 0.05609211\n",
      "Iteration 59, loss = 0.05505919\n",
      "Iteration 60, loss = 0.05407971\n",
      "Iteration 61, loss = 0.05314839\n",
      "Iteration 62, loss = 0.05225275\n",
      "Iteration 63, loss = 0.05138431\n",
      "Iteration 64, loss = 0.05055181\n",
      "Iteration 65, loss = 0.04975513\n",
      "Iteration 66, loss = 0.04898839\n",
      "Iteration 67, loss = 0.04823845\n",
      "Iteration 68, loss = 0.04750409\n",
      "Iteration 69, loss = 0.04678261\n",
      "Iteration 70, loss = 0.04608744\n",
      "Iteration 71, loss = 0.04541598\n",
      "Iteration 72, loss = 0.04475570\n",
      "Iteration 73, loss = 0.04410713\n",
      "Iteration 74, loss = 0.04347926\n",
      "Iteration 75, loss = 0.04286446\n",
      "Iteration 76, loss = 0.04226149\n",
      "Iteration 77, loss = 0.04167066\n",
      "Iteration 78, loss = 0.04108625\n",
      "Iteration 79, loss = 0.04051257\n",
      "Iteration 80, loss = 0.03995764\n",
      "Iteration 81, loss = 0.03941921\n",
      "Iteration 82, loss = 0.03889704\n",
      "Iteration 83, loss = 0.03838279\n",
      "Iteration 84, loss = 0.03787473\n",
      "Iteration 85, loss = 0.03737278\n",
      "Iteration 86, loss = 0.03687601\n",
      "Iteration 87, loss = 0.03639087\n",
      "Iteration 88, loss = 0.03591670\n",
      "Iteration 89, loss = 0.03545227\n",
      "Iteration 90, loss = 0.03500655\n",
      "Iteration 91, loss = 0.03455627\n",
      "Iteration 92, loss = 0.03411124\n",
      "Iteration 93, loss = 0.03367531\n",
      "Iteration 94, loss = 0.03324902\n",
      "Iteration 95, loss = 0.03282766\n",
      "Iteration 96, loss = 0.03241120\n",
      "Iteration 97, loss = 0.03199835\n",
      "Iteration 98, loss = 0.03158633\n",
      "Iteration 99, loss = 0.03118390\n",
      "Iteration 100, loss = 0.03078220\n",
      "Iteration 101, loss = 0.03038673\n",
      "Iteration 102, loss = 0.02999597\n",
      "Iteration 103, loss = 0.02960579\n",
      "Iteration 104, loss = 0.02922850\n",
      "Iteration 105, loss = 0.02885662\n",
      "Iteration 106, loss = 0.02848719\n",
      "Iteration 107, loss = 0.02812213\n",
      "Iteration 108, loss = 0.02776352\n",
      "Iteration 109, loss = 0.02741048\n",
      "Iteration 110, loss = 0.02705601\n",
      "Iteration 111, loss = 0.02670208\n",
      "Iteration 112, loss = 0.02635088\n",
      "Iteration 113, loss = 0.02600916\n",
      "Iteration 114, loss = 0.02567069\n",
      "Iteration 115, loss = 0.02533231\n",
      "Iteration 116, loss = 0.02499812\n",
      "Iteration 117, loss = 0.02466587\n",
      "Iteration 118, loss = 0.02433390\n",
      "Iteration 119, loss = 0.02400094\n",
      "Iteration 120, loss = 0.02367215\n",
      "Iteration 121, loss = 0.02334808\n",
      "Iteration 122, loss = 0.02303347\n",
      "Iteration 123, loss = 0.02272192\n",
      "Iteration 124, loss = 0.02241357\n",
      "Iteration 125, loss = 0.02210282\n",
      "Iteration 126, loss = 0.02178779\n",
      "Iteration 127, loss = 0.02148419\n",
      "Iteration 128, loss = 0.02118133\n",
      "Iteration 129, loss = 0.02087489\n",
      "Iteration 130, loss = 0.02057122\n",
      "Iteration 131, loss = 0.02026855\n",
      "Iteration 132, loss = 0.01996725\n",
      "Iteration 133, loss = 0.01966910\n",
      "Iteration 134, loss = 0.01936621\n",
      "Iteration 135, loss = 0.01907146\n",
      "Iteration 136, loss = 0.01878080\n",
      "Iteration 137, loss = 0.01849607\n",
      "Iteration 138, loss = 0.01822024\n",
      "Iteration 139, loss = 0.01795259\n",
      "Iteration 140, loss = 0.01768655\n",
      "Iteration 141, loss = 0.01742168\n",
      "Iteration 142, loss = 0.01715389\n",
      "Iteration 143, loss = 0.01688815\n",
      "Iteration 144, loss = 0.01662981\n",
      "Iteration 145, loss = 0.01637762\n",
      "Iteration 146, loss = 0.01612584\n",
      "Iteration 147, loss = 0.01587789\n",
      "Iteration 148, loss = 0.01563443\n",
      "Iteration 149, loss = 0.01539225\n",
      "Iteration 150, loss = 0.01515279\n",
      "Iteration 151, loss = 0.01491309\n",
      "Iteration 152, loss = 0.01467632\n",
      "Iteration 153, loss = 0.01444408\n",
      "Iteration 154, loss = 0.01422039\n",
      "Iteration 155, loss = 0.01400800\n",
      "Iteration 156, loss = 0.01379304\n",
      "Iteration 157, loss = 0.01358205\n",
      "Iteration 158, loss = 0.01337127\n",
      "Iteration 159, loss = 0.01316115\n",
      "Iteration 160, loss = 0.01295445\n",
      "Iteration 161, loss = 0.01274819\n",
      "Iteration 162, loss = 0.01254488\n",
      "Iteration 163, loss = 0.01234605\n",
      "Iteration 164, loss = 0.01215172\n",
      "Iteration 165, loss = 0.01195682\n",
      "Iteration 166, loss = 0.01176382\n",
      "Iteration 167, loss = 0.01157383\n",
      "Iteration 168, loss = 0.01138687\n",
      "Iteration 169, loss = 0.01120579\n",
      "Iteration 170, loss = 0.01102539\n",
      "Iteration 171, loss = 0.01084525\n",
      "Iteration 172, loss = 0.01066676\n",
      "Iteration 173, loss = 0.01049013\n",
      "Iteration 174, loss = 0.01032228\n",
      "Iteration 175, loss = 0.01015571\n",
      "Iteration 176, loss = 0.00999379\n",
      "Iteration 177, loss = 0.00983015\n",
      "Iteration 178, loss = 0.00966928\n",
      "Iteration 179, loss = 0.00951612\n",
      "Iteration 180, loss = 0.00936333\n",
      "Iteration 181, loss = 0.00921153\n",
      "Iteration 182, loss = 0.00906142\n",
      "Iteration 183, loss = 0.00891726\n",
      "Iteration 184, loss = 0.00877605\n",
      "Iteration 185, loss = 0.00863471\n",
      "Iteration 186, loss = 0.00849642\n",
      "Iteration 187, loss = 0.00836080\n",
      "Iteration 188, loss = 0.00822566\n",
      "Iteration 189, loss = 0.00809807\n",
      "Iteration 190, loss = 0.00797380\n",
      "Iteration 191, loss = 0.00785383\n",
      "Iteration 192, loss = 0.00773311\n",
      "Iteration 193, loss = 0.00761440\n",
      "Iteration 194, loss = 0.00749795\n",
      "Iteration 195, loss = 0.00738306\n",
      "Iteration 196, loss = 0.00727053\n",
      "Iteration 197, loss = 0.00716144\n",
      "Iteration 198, loss = 0.00705111\n",
      "Iteration 199, loss = 0.00694446\n",
      "Iteration 200, loss = 0.00683885\n",
      "Iteration 201, loss = 0.00673634\n",
      "Iteration 202, loss = 0.00663631\n",
      "Iteration 203, loss = 0.00653652\n",
      "Iteration 204, loss = 0.00643983\n",
      "Iteration 205, loss = 0.00634467\n",
      "Iteration 206, loss = 0.00625417\n",
      "Iteration 207, loss = 0.00616466\n",
      "Iteration 208, loss = 0.00607487\n",
      "Iteration 209, loss = 0.00598830\n",
      "Iteration 210, loss = 0.00590324\n",
      "Iteration 211, loss = 0.00582128\n",
      "Iteration 212, loss = 0.00573844\n",
      "Iteration 213, loss = 0.00565823\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.50977421\n",
      "Iteration 2, loss = 0.47662997\n",
      "Iteration 3, loss = 0.44612999\n",
      "Iteration 4, loss = 0.41806228\n",
      "Iteration 5, loss = 0.39217340\n",
      "Iteration 6, loss = 0.36816627\n",
      "Iteration 7, loss = 0.34589869\n",
      "Iteration 8, loss = 0.32521570\n",
      "Iteration 9, loss = 0.30596530\n",
      "Iteration 10, loss = 0.28805088\n",
      "Iteration 11, loss = 0.27138621\n",
      "Iteration 12, loss = 0.25589554\n",
      "Iteration 13, loss = 0.24141587\n",
      "Iteration 14, loss = 0.22792749\n",
      "Iteration 15, loss = 0.21535736\n",
      "Iteration 16, loss = 0.20369147\n",
      "Iteration 17, loss = 0.19293182\n",
      "Iteration 18, loss = 0.18304918\n",
      "Iteration 19, loss = 0.17405210\n",
      "Iteration 20, loss = 0.16584552\n",
      "Iteration 21, loss = 0.15844623\n",
      "Iteration 22, loss = 0.15182784\n",
      "Iteration 23, loss = 0.14594281\n",
      "Iteration 24, loss = 0.14069063\n",
      "Iteration 25, loss = 0.13598639\n",
      "Iteration 26, loss = 0.13167287\n",
      "Iteration 27, loss = 0.12763034\n",
      "Iteration 28, loss = 0.12375993\n",
      "Iteration 29, loss = 0.11999109\n",
      "Iteration 30, loss = 0.11628171\n",
      "Iteration 31, loss = 0.11259913\n",
      "Iteration 32, loss = 0.10895395\n",
      "Iteration 33, loss = 0.10536040\n",
      "Iteration 34, loss = 0.10185259\n",
      "Iteration 35, loss = 0.09847912\n",
      "Iteration 36, loss = 0.09525443\n",
      "Iteration 37, loss = 0.09218049\n",
      "Iteration 38, loss = 0.08928405\n",
      "Iteration 39, loss = 0.08657967\n",
      "Iteration 40, loss = 0.08405255\n",
      "Iteration 41, loss = 0.08168823\n",
      "Iteration 42, loss = 0.07945923\n",
      "Iteration 43, loss = 0.07737766\n",
      "Iteration 44, loss = 0.07543840\n",
      "Iteration 45, loss = 0.07361982\n",
      "Iteration 46, loss = 0.07190921\n",
      "Iteration 47, loss = 0.07029137\n",
      "Iteration 48, loss = 0.06875905\n",
      "Iteration 49, loss = 0.06730075\n",
      "Iteration 50, loss = 0.06589467\n",
      "Iteration 51, loss = 0.06452007\n",
      "Iteration 52, loss = 0.06319300\n",
      "Iteration 53, loss = 0.06190883\n",
      "Iteration 54, loss = 0.06066235\n",
      "Iteration 55, loss = 0.05945453\n",
      "Iteration 56, loss = 0.05828853\n",
      "Iteration 57, loss = 0.05717068\n",
      "Iteration 58, loss = 0.05609211\n",
      "Iteration 59, loss = 0.05505919\n",
      "Iteration 60, loss = 0.05407971\n",
      "Iteration 61, loss = 0.05314839\n",
      "Iteration 62, loss = 0.05225275\n",
      "Iteration 63, loss = 0.05138431\n",
      "Iteration 64, loss = 0.05055181\n",
      "Iteration 65, loss = 0.04975513\n",
      "Iteration 66, loss = 0.04898839\n",
      "Iteration 67, loss = 0.04823845\n",
      "Iteration 68, loss = 0.04750409\n",
      "Iteration 69, loss = 0.04678261\n",
      "Iteration 70, loss = 0.04608744\n",
      "Iteration 71, loss = 0.04541598\n",
      "Iteration 72, loss = 0.04475570\n",
      "Iteration 73, loss = 0.04410713\n",
      "Iteration 74, loss = 0.04347926\n",
      "Iteration 75, loss = 0.04286446\n",
      "Iteration 76, loss = 0.04226149\n",
      "Iteration 77, loss = 0.04167066\n",
      "Iteration 78, loss = 0.04108625\n",
      "Iteration 79, loss = 0.04051257\n",
      "Iteration 80, loss = 0.03995764\n",
      "Iteration 81, loss = 0.03941921\n",
      "Iteration 82, loss = 0.03889704\n",
      "Iteration 83, loss = 0.03838279\n",
      "Iteration 84, loss = 0.03787473\n",
      "Iteration 85, loss = 0.03737278\n",
      "Iteration 86, loss = 0.03687601\n",
      "Iteration 87, loss = 0.03639087\n",
      "Iteration 88, loss = 0.03591670\n",
      "Iteration 89, loss = 0.03545227\n",
      "Iteration 90, loss = 0.03500655\n",
      "Iteration 91, loss = 0.03455627\n",
      "Iteration 92, loss = 0.03411124\n",
      "Iteration 93, loss = 0.03367531\n",
      "Iteration 94, loss = 0.03324902\n",
      "Iteration 95, loss = 0.03282766\n",
      "Iteration 96, loss = 0.03241120\n",
      "Iteration 97, loss = 0.03199835\n",
      "Iteration 98, loss = 0.03158633\n",
      "Iteration 99, loss = 0.03118390\n",
      "Iteration 100, loss = 0.03078220\n",
      "Iteration 101, loss = 0.03038673\n",
      "Iteration 102, loss = 0.02999597\n",
      "Iteration 103, loss = 0.02960579\n",
      "Iteration 104, loss = 0.02922850\n",
      "Iteration 105, loss = 0.02885662\n",
      "Iteration 106, loss = 0.02848719\n",
      "Iteration 107, loss = 0.02812213\n",
      "Iteration 108, loss = 0.02776352\n",
      "Iteration 109, loss = 0.02741048\n",
      "Iteration 110, loss = 0.02705601\n",
      "Iteration 111, loss = 0.02670208\n",
      "Iteration 112, loss = 0.02635088\n",
      "Iteration 113, loss = 0.02600916\n",
      "Iteration 114, loss = 0.02567069\n",
      "Iteration 115, loss = 0.02533231\n",
      "Iteration 116, loss = 0.02499812\n",
      "Iteration 117, loss = 0.02466587\n",
      "Iteration 118, loss = 0.02433390\n",
      "Iteration 119, loss = 0.02400094\n",
      "Iteration 120, loss = 0.02367215\n",
      "Iteration 121, loss = 0.02334808\n",
      "Iteration 122, loss = 0.02303347\n",
      "Iteration 123, loss = 0.02272192\n",
      "Iteration 124, loss = 0.02241357\n",
      "Iteration 125, loss = 0.02210282\n",
      "Iteration 126, loss = 0.02178779\n",
      "Iteration 127, loss = 0.02148419\n",
      "Iteration 128, loss = 0.02118133\n",
      "Iteration 129, loss = 0.02087489\n",
      "Iteration 130, loss = 0.02057122\n",
      "Iteration 131, loss = 0.02026855\n",
      "Iteration 132, loss = 0.01996725\n",
      "Iteration 133, loss = 0.01966910\n",
      "Iteration 134, loss = 0.01936621\n",
      "Iteration 135, loss = 0.01907146\n",
      "Iteration 136, loss = 0.01878080\n",
      "Iteration 137, loss = 0.01849607\n",
      "Iteration 138, loss = 0.01822024\n",
      "Iteration 139, loss = 0.01795259\n",
      "Iteration 140, loss = 0.01768655\n",
      "Iteration 141, loss = 0.01742168\n",
      "Iteration 142, loss = 0.01715389\n",
      "Iteration 143, loss = 0.01688815\n",
      "Iteration 144, loss = 0.01662981\n",
      "Iteration 145, loss = 0.01637762\n",
      "Iteration 146, loss = 0.01612584\n",
      "Iteration 147, loss = 0.01587789\n",
      "Iteration 148, loss = 0.01563443\n",
      "Iteration 149, loss = 0.01539225\n",
      "Iteration 150, loss = 0.01515279\n",
      "Iteration 151, loss = 0.01491309\n",
      "Iteration 152, loss = 0.01467632\n",
      "Iteration 153, loss = 0.01444408\n",
      "Iteration 154, loss = 0.01422039\n",
      "Iteration 155, loss = 0.01400800\n",
      "Iteration 156, loss = 0.01379304\n",
      "Iteration 157, loss = 0.01358205\n",
      "Iteration 158, loss = 0.01337127\n",
      "Iteration 159, loss = 0.01316115\n",
      "Iteration 160, loss = 0.01295445\n",
      "Iteration 161, loss = 0.01274819\n",
      "Iteration 162, loss = 0.01254488\n",
      "Iteration 163, loss = 0.01234605\n",
      "Iteration 164, loss = 0.01215172\n",
      "Iteration 165, loss = 0.01195682\n",
      "Iteration 166, loss = 0.01176382\n",
      "Iteration 167, loss = 0.01157383\n",
      "Iteration 168, loss = 0.01138687\n",
      "Iteration 169, loss = 0.01120579\n",
      "Iteration 170, loss = 0.01102539\n",
      "Iteration 171, loss = 0.01084525\n",
      "Iteration 172, loss = 0.01066676\n",
      "Iteration 173, loss = 0.01049013\n",
      "Iteration 174, loss = 0.01032228\n",
      "Iteration 175, loss = 0.01015571\n",
      "Iteration 176, loss = 0.00999379\n",
      "Iteration 177, loss = 0.00983015\n",
      "Iteration 178, loss = 0.00966928\n",
      "Iteration 179, loss = 0.00951612\n",
      "Iteration 180, loss = 0.00936333\n",
      "Iteration 181, loss = 0.00921153\n",
      "Iteration 182, loss = 0.00906142\n",
      "Iteration 183, loss = 0.00891726\n",
      "Iteration 184, loss = 0.00877605\n",
      "Iteration 185, loss = 0.00863471\n",
      "Iteration 186, loss = 0.00849642\n",
      "Iteration 187, loss = 0.00836080\n",
      "Iteration 188, loss = 0.00822566\n",
      "Iteration 189, loss = 0.00809807\n",
      "Iteration 190, loss = 0.00797380\n",
      "Iteration 191, loss = 0.00785383\n",
      "Iteration 192, loss = 0.00773311\n",
      "Iteration 193, loss = 0.00761440\n",
      "Iteration 194, loss = 0.00749795\n",
      "Iteration 195, loss = 0.00738306\n",
      "Iteration 196, loss = 0.00727053\n",
      "Iteration 197, loss = 0.00716144\n",
      "Iteration 198, loss = 0.00705111\n",
      "Iteration 199, loss = 0.00694446\n",
      "Iteration 200, loss = 0.00683885\n",
      "Iteration 201, loss = 0.00673634\n",
      "Iteration 202, loss = 0.00663631\n",
      "Iteration 203, loss = 0.00653652\n",
      "Iteration 204, loss = 0.00643983\n",
      "Iteration 205, loss = 0.00634467\n",
      "Iteration 206, loss = 0.00625417\n",
      "Iteration 207, loss = 0.00616466\n",
      "Iteration 208, loss = 0.00607487\n",
      "Iteration 209, loss = 0.00598830\n",
      "Iteration 210, loss = 0.00590324\n",
      "Iteration 211, loss = 0.00582128\n",
      "Iteration 212, loss = 0.00573844\n",
      "Iteration 213, loss = 0.00565823\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.50977421\n",
      "Iteration 2, loss = 0.47662997\n",
      "Iteration 3, loss = 0.44612999\n",
      "Iteration 4, loss = 0.41806228\n",
      "Iteration 5, loss = 0.39217340\n",
      "Iteration 6, loss = 0.36816627\n",
      "Iteration 7, loss = 0.34589869\n",
      "Iteration 8, loss = 0.32521570\n",
      "Iteration 9, loss = 0.30596530\n",
      "Iteration 10, loss = 0.28805088\n",
      "Iteration 11, loss = 0.27138621\n",
      "Iteration 12, loss = 0.25589554\n",
      "Iteration 13, loss = 0.24141587\n",
      "Iteration 14, loss = 0.22792749\n",
      "Iteration 15, loss = 0.21535736\n",
      "Iteration 16, loss = 0.20369147\n",
      "Iteration 17, loss = 0.19293182\n",
      "Iteration 18, loss = 0.18304918\n",
      "Iteration 19, loss = 0.17405210\n",
      "Iteration 20, loss = 0.16584552\n",
      "Iteration 21, loss = 0.15844623\n",
      "Iteration 22, loss = 0.15182784\n",
      "Iteration 23, loss = 0.14594281\n",
      "Iteration 24, loss = 0.14069063\n",
      "Iteration 25, loss = 0.13598639\n",
      "Iteration 26, loss = 0.13167287\n",
      "Iteration 27, loss = 0.12763034\n",
      "Iteration 28, loss = 0.12375993\n",
      "Iteration 29, loss = 0.11999109\n",
      "Iteration 30, loss = 0.11628171\n",
      "Iteration 31, loss = 0.11259913\n",
      "Iteration 32, loss = 0.10895395\n",
      "Iteration 33, loss = 0.10536040\n",
      "Iteration 34, loss = 0.10185259\n",
      "Iteration 35, loss = 0.09847912\n",
      "Iteration 36, loss = 0.09525443\n",
      "Iteration 37, loss = 0.09218049\n",
      "Iteration 38, loss = 0.08928405\n",
      "Iteration 39, loss = 0.08657967\n",
      "Iteration 40, loss = 0.08405255\n",
      "Iteration 41, loss = 0.08168823\n",
      "Iteration 42, loss = 0.07945923\n",
      "Iteration 43, loss = 0.07737766\n",
      "Iteration 44, loss = 0.07543840\n",
      "Iteration 45, loss = 0.07361982\n",
      "Iteration 46, loss = 0.07190921\n",
      "Iteration 47, loss = 0.07029137\n",
      "Iteration 48, loss = 0.06875905\n",
      "Iteration 49, loss = 0.06730075\n",
      "Iteration 50, loss = 0.06589467\n",
      "Iteration 51, loss = 0.06452007\n",
      "Iteration 52, loss = 0.06319300\n",
      "Iteration 53, loss = 0.06190883\n",
      "Iteration 54, loss = 0.06066235\n",
      "Iteration 55, loss = 0.05945453\n",
      "Iteration 56, loss = 0.05828853\n",
      "Iteration 57, loss = 0.05717068\n",
      "Iteration 58, loss = 0.05609211\n",
      "Iteration 59, loss = 0.05505919\n",
      "Iteration 60, loss = 0.05407971\n",
      "Iteration 61, loss = 0.05314839\n",
      "Iteration 62, loss = 0.05225275\n",
      "Iteration 63, loss = 0.05138431\n",
      "Iteration 64, loss = 0.05055181\n",
      "Iteration 65, loss = 0.04975513\n",
      "Iteration 66, loss = 0.04898839\n",
      "Iteration 67, loss = 0.04823845\n",
      "Iteration 68, loss = 0.04750409\n",
      "Iteration 69, loss = 0.04678261\n",
      "Iteration 70, loss = 0.04608744\n",
      "Iteration 71, loss = 0.04541598\n",
      "Iteration 72, loss = 0.04475570\n",
      "Iteration 73, loss = 0.04410713\n",
      "Iteration 74, loss = 0.04347926\n",
      "Iteration 75, loss = 0.04286446\n",
      "Iteration 76, loss = 0.04226149\n",
      "Iteration 77, loss = 0.04167066\n",
      "Iteration 78, loss = 0.04108625\n",
      "Iteration 79, loss = 0.04051257\n",
      "Iteration 80, loss = 0.03995764\n",
      "Iteration 81, loss = 0.03941921\n",
      "Iteration 82, loss = 0.03889704\n",
      "Iteration 83, loss = 0.03838279\n",
      "Iteration 84, loss = 0.03787473\n",
      "Iteration 85, loss = 0.03737278\n",
      "Iteration 86, loss = 0.03687601\n",
      "Iteration 87, loss = 0.03639087\n",
      "Iteration 88, loss = 0.03591670\n",
      "Iteration 89, loss = 0.03545227\n",
      "Iteration 90, loss = 0.03500655\n",
      "Iteration 91, loss = 0.03455627\n",
      "Iteration 92, loss = 0.03411124\n",
      "Iteration 93, loss = 0.03367531\n",
      "Iteration 94, loss = 0.03324902\n",
      "Iteration 95, loss = 0.03282766\n",
      "Iteration 96, loss = 0.03241120\n",
      "Iteration 97, loss = 0.03199835\n",
      "Iteration 98, loss = 0.03158633\n",
      "Iteration 99, loss = 0.03118390\n",
      "Iteration 100, loss = 0.03078220\n",
      "Iteration 101, loss = 0.03038673\n",
      "Iteration 102, loss = 0.02999597\n",
      "Iteration 103, loss = 0.02960579\n",
      "Iteration 104, loss = 0.02922850\n",
      "Iteration 105, loss = 0.02885662\n",
      "Iteration 106, loss = 0.02848719\n",
      "Iteration 107, loss = 0.02812213\n",
      "Iteration 108, loss = 0.02776352\n",
      "Iteration 109, loss = 0.02741048\n",
      "Iteration 110, loss = 0.02705601\n",
      "Iteration 111, loss = 0.02670208\n",
      "Iteration 112, loss = 0.02635088\n",
      "Iteration 113, loss = 0.02600916\n",
      "Iteration 114, loss = 0.02567069\n",
      "Iteration 115, loss = 0.02533231\n",
      "Iteration 116, loss = 0.02499812\n",
      "Iteration 117, loss = 0.02466587\n",
      "Iteration 118, loss = 0.02433390\n",
      "Iteration 119, loss = 0.02400094\n",
      "Iteration 120, loss = 0.02367215\n",
      "Iteration 121, loss = 0.02334808\n",
      "Iteration 122, loss = 0.02303347\n",
      "Iteration 123, loss = 0.02272192\n",
      "Iteration 124, loss = 0.02241357\n",
      "Iteration 125, loss = 0.02210282\n",
      "Iteration 126, loss = 0.02178779\n",
      "Iteration 127, loss = 0.02148419\n",
      "Iteration 128, loss = 0.02118133\n",
      "Iteration 129, loss = 0.02087489\n",
      "Iteration 130, loss = 0.02057122\n",
      "Iteration 131, loss = 0.02026855\n",
      "Iteration 132, loss = 0.01996725\n",
      "Iteration 133, loss = 0.01966910\n",
      "Iteration 134, loss = 0.01936621\n",
      "Iteration 135, loss = 0.01907146\n",
      "Iteration 136, loss = 0.01878080\n",
      "Iteration 137, loss = 0.01849607\n",
      "Iteration 138, loss = 0.01822024\n",
      "Iteration 139, loss = 0.01795259\n",
      "Iteration 140, loss = 0.01768655\n",
      "Iteration 141, loss = 0.01742168\n",
      "Iteration 142, loss = 0.01715389\n",
      "Iteration 143, loss = 0.01688815\n",
      "Iteration 144, loss = 0.01662981\n",
      "Iteration 145, loss = 0.01637762\n",
      "Iteration 146, loss = 0.01612584\n",
      "Iteration 147, loss = 0.01587789\n",
      "Iteration 148, loss = 0.01563443\n",
      "Iteration 149, loss = 0.01539225\n",
      "Iteration 150, loss = 0.01515279\n",
      "Iteration 151, loss = 0.01491309\n",
      "Iteration 152, loss = 0.01467632\n",
      "Iteration 153, loss = 0.01444408\n",
      "Iteration 154, loss = 0.01422039\n",
      "Iteration 155, loss = 0.01400800\n",
      "Iteration 156, loss = 0.01379304\n",
      "Iteration 157, loss = 0.01358205\n",
      "Iteration 158, loss = 0.01337127\n",
      "Iteration 159, loss = 0.01316115\n",
      "Iteration 160, loss = 0.01295445\n",
      "Iteration 161, loss = 0.01274819\n",
      "Iteration 162, loss = 0.01254488\n",
      "Iteration 163, loss = 0.01234605\n",
      "Iteration 164, loss = 0.01215172\n",
      "Iteration 165, loss = 0.01195682\n",
      "Iteration 166, loss = 0.01176382\n",
      "Iteration 167, loss = 0.01157383\n",
      "Iteration 168, loss = 0.01138687\n",
      "Iteration 169, loss = 0.01120579\n",
      "Iteration 170, loss = 0.01102539\n",
      "Iteration 171, loss = 0.01084525\n",
      "Iteration 172, loss = 0.01066676\n",
      "Iteration 173, loss = 0.01049013\n",
      "Iteration 174, loss = 0.01032228\n",
      "Iteration 175, loss = 0.01015571\n",
      "LinearRegression\n",
      "NeuralNetwork\n",
      "Iteration 176, loss = 0.00999379\n",
      "Iteration 177, loss = 0.00983015\n",
      "Iteration 178, loss = 0.00966928\n",
      "Iteration 179, loss = 0.00951612\n",
      "Iteration 180, loss = 0.00936333\n",
      "Iteration 181, loss = 0.00921153\n",
      "Iteration 182, loss = 0.00906142\n",
      "Iteration 183, loss = 0.00891726\n",
      "Iteration 184, loss = 0.00877605\n",
      "Iteration 185, loss = 0.00863471\n",
      "Iteration 186, loss = 0.00849642\n",
      "Iteration 187, loss = 0.00836080\n",
      "Iteration 188, loss = 0.00822566\n",
      "Iteration 189, loss = 0.00809807\n",
      "Iteration 190, loss = 0.00797380\n",
      "Iteration 191, loss = 0.00785383\n",
      "Iteration 192, loss = 0.00773311\n",
      "Iteration 193, loss = 0.00761440\n",
      "Iteration 194, loss = 0.00749795\n",
      "Iteration 195, loss = 0.00738306\n",
      "Iteration 196, loss = 0.00727053\n",
      "Iteration 197, loss = 0.00716144\n",
      "Iteration 198, loss = 0.00705111\n",
      "Iteration 199, loss = 0.00694446\n",
      "Iteration 200, loss = 0.00683885\n",
      "Iteration 201, loss = 0.00673634\n",
      "Iteration 202, loss = 0.00663631\n",
      "Iteration 203, loss = 0.00653652\n",
      "Iteration 204, loss = 0.00643983\n",
      "Iteration 205, loss = 0.00634467\n",
      "Iteration 206, loss = 0.00625417\n",
      "Iteration 207, loss = 0.00616466\n",
      "Iteration 208, loss = 0.00607487\n",
      "Iteration 209, loss = 0.00598830\n",
      "Iteration 210, loss = 0.00590324\n",
      "Iteration 211, loss = 0.00582128\n",
      "Iteration 212, loss = 0.00573844\n",
      "Iteration 213, loss = 0.00565823\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.50829378\n",
      "Iteration 2, loss = 0.47366733\n",
      "Iteration 3, loss = 0.44181858\n",
      "Iteration 4, loss = 0.41257371\n",
      "Iteration 5, loss = 0.38574493\n",
      "Iteration 6, loss = 0.36109634\n",
      "Iteration 7, loss = 0.33844530\n",
      "Iteration 8, loss = 0.31756191\n",
      "Iteration 9, loss = 0.29830382\n",
      "Iteration 10, loss = 0.28058427\n",
      "Iteration 11, loss = 0.26433148\n",
      "Iteration 12, loss = 0.24940187\n",
      "Iteration 13, loss = 0.23568972\n",
      "Iteration 14, loss = 0.22312329\n",
      "Iteration 15, loss = 0.21156959\n",
      "Iteration 16, loss = 0.20097514\n",
      "Iteration 17, loss = 0.19130660\n",
      "Iteration 18, loss = 0.18256412\n",
      "Iteration 19, loss = 0.17467468\n",
      "Iteration 20, loss = 0.16758302\n",
      "Iteration 21, loss = 0.16122928\n",
      "Iteration 22, loss = 0.15550386\n",
      "Iteration 23, loss = 0.15033139\n",
      "Iteration 24, loss = 0.14561808\n",
      "Iteration 25, loss = 0.14125497\n",
      "Iteration 26, loss = 0.13713957\n",
      "Iteration 27, loss = 0.13322762\n",
      "Iteration 28, loss = 0.12947340\n",
      "Iteration 29, loss = 0.12585251\n",
      "Iteration 30, loss = 0.12235074\n",
      "Iteration 31, loss = 0.11895889\n",
      "Iteration 32, loss = 0.11566384\n",
      "Iteration 33, loss = 0.11247290\n",
      "Iteration 34, loss = 0.10938265\n",
      "Iteration 35, loss = 0.10638732\n",
      "Iteration 36, loss = 0.10349234\n",
      "Iteration 37, loss = 0.10068582\n",
      "Iteration 38, loss = 0.09797185\n",
      "Iteration 39, loss = 0.09535395\n",
      "Iteration 40, loss = 0.09282452\n",
      "Iteration 41, loss = 0.09041333\n",
      "Iteration 42, loss = 0.08812040\n",
      "Iteration 43, loss = 0.08593922\n",
      "Iteration 44, loss = 0.08388360\n",
      "Iteration 45, loss = 0.08194477\n",
      "Iteration 46, loss = 0.08011380\n",
      "Iteration 47, loss = 0.07839336\n",
      "Iteration 48, loss = 0.07678096\n",
      "Iteration 49, loss = 0.07524097\n",
      "Iteration 50, loss = 0.07378603\n",
      "Iteration 51, loss = 0.07239323\n",
      "Iteration 52, loss = 0.07106016\n",
      "Iteration 53, loss = 0.06979503\n",
      "Iteration 54, loss = 0.06860161\n",
      "Iteration 55, loss = 0.06748017\n",
      "Iteration 56, loss = 0.06640886\n",
      "Iteration 57, loss = 0.06539534\n",
      "Iteration 58, loss = 0.06443221\n",
      "Iteration 59, loss = 0.06351850\n",
      "Iteration 60, loss = 0.06264869\n",
      "Iteration 61, loss = 0.06181828\n",
      "Iteration 62, loss = 0.06102532\n",
      "Iteration 63, loss = 0.06025813\n",
      "Iteration 64, loss = 0.05951955\n",
      "Iteration 65, loss = 0.05881570\n",
      "Iteration 66, loss = 0.05813197\n",
      "Iteration 67, loss = 0.05745692\n",
      "Iteration 68, loss = 0.05679761\n",
      "Iteration 69, loss = 0.05615647\n",
      "Iteration 70, loss = 0.05553098\n",
      "Iteration 71, loss = 0.05492172\n",
      "Iteration 72, loss = 0.05433290\n",
      "Iteration 73, loss = 0.05376204\n",
      "Iteration 74, loss = 0.05320806\n",
      "Iteration 75, loss = 0.05266257\n",
      "Iteration 76, loss = 0.05212698\n",
      "Iteration 77, loss = 0.05160204\n",
      "Iteration 78, loss = 0.05109008\n",
      "Iteration 79, loss = 0.05058746\n",
      "Iteration 80, loss = 0.05009206\n",
      "Iteration 81, loss = 0.04960561\n",
      "Iteration 82, loss = 0.04912714\n",
      "Iteration 83, loss = 0.04865255\n",
      "Iteration 84, loss = 0.04818238\n",
      "Iteration 85, loss = 0.04771979\n",
      "Iteration 86, loss = 0.04726968\n",
      "Iteration 87, loss = 0.04682774\n",
      "Iteration 88, loss = 0.04638480\n",
      "Iteration 89, loss = 0.04594537\n",
      "Iteration 90, loss = 0.04551041\n",
      "Iteration 91, loss = 0.04507857\n",
      "Iteration 92, loss = 0.04465192\n",
      "Iteration 93, loss = 0.04423105\n",
      "Iteration 94, loss = 0.04381138\n",
      "Iteration 95, loss = 0.04339177\n",
      "Iteration 96, loss = 0.04297614\n",
      "Iteration 97, loss = 0.04256666\n",
      "Iteration 98, loss = 0.04216350\n",
      "Iteration 99, loss = 0.04176210\n",
      "Iteration 100, loss = 0.04136343\n",
      "Iteration 101, loss = 0.04096515\n",
      "Iteration 102, loss = 0.04057179\n",
      "Iteration 103, loss = 0.04018570\n",
      "Iteration 104, loss = 0.03980442\n",
      "Iteration 105, loss = 0.03942205\n",
      "Iteration 106, loss = 0.03904222\n",
      "Iteration 107, loss = 0.03866372\n",
      "Iteration 108, loss = 0.03828774\n",
      "Iteration 109, loss = 0.03791729\n",
      "Iteration 110, loss = 0.03754999\n",
      "Iteration 111, loss = 0.03718420\n",
      "Iteration 112, loss = 0.03681989\n",
      "Iteration 113, loss = 0.03645775\n",
      "Iteration 114, loss = 0.03609687\n",
      "Iteration 115, loss = 0.03574031\n",
      "Iteration 116, loss = 0.03538479\n",
      "Iteration 117, loss = 0.03502884\n",
      "Iteration 118, loss = 0.03467877\n",
      "Iteration 119, loss = 0.03433591\n",
      "Iteration 120, loss = 0.03399421\n",
      "Iteration 121, loss = 0.03365210\n",
      "Iteration 122, loss = 0.03331195\n",
      "Iteration 123, loss = 0.03297302\n",
      "Iteration 124, loss = 0.03263690\n",
      "Iteration 125, loss = 0.03230758\n",
      "Iteration 126, loss = 0.03198186\n",
      "Iteration 127, loss = 0.03165928\n",
      "Iteration 128, loss = 0.03133713\n",
      "Iteration 129, loss = 0.03101565\n",
      "Iteration 130, loss = 0.03069374\n",
      "Iteration 131, loss = 0.03037464\n",
      "Iteration 132, loss = 0.03005651\n",
      "Iteration 133, loss = 0.02974268\n",
      "Iteration 134, loss = 0.02942990\n",
      "Iteration 135, loss = 0.02911976\n",
      "Iteration 136, loss = 0.02881562\n",
      "Iteration 137, loss = 0.02851381\n",
      "Iteration 138, loss = 0.02821242\n",
      "Iteration 139, loss = 0.02791598\n",
      "Iteration 140, loss = 0.02761972\n",
      "Iteration 141, loss = 0.02732588\n",
      "Iteration 142, loss = 0.02703808\n",
      "Iteration 143, loss = 0.02675132\n",
      "Iteration 144, loss = 0.02646676\n",
      "Iteration 145, loss = 0.02618749\n",
      "Iteration 146, loss = 0.02591221\n",
      "Iteration 147, loss = 0.02564090\n",
      "Iteration 148, loss = 0.02537181\n",
      "Iteration 149, loss = 0.02510316\n",
      "Iteration 150, loss = 0.02483533\n",
      "Iteration 151, loss = 0.02457343\n",
      "Iteration 152, loss = 0.02431324\n",
      "Iteration 153, loss = 0.02405479\n",
      "Iteration 154, loss = 0.02379744\n",
      "Iteration 155, loss = 0.02354211\n",
      "Iteration 156, loss = 0.02328894\n",
      "Iteration 157, loss = 0.02304052\n",
      "Iteration 158, loss = 0.02279245\n",
      "Iteration 159, loss = 0.02254673\n",
      "Iteration 160, loss = 0.02230355\n",
      "Iteration 161, loss = 0.02205931\n",
      "Iteration 162, loss = 0.02181808\n",
      "Iteration 163, loss = 0.02158136\n",
      "Iteration 164, loss = 0.02134960\n",
      "Iteration 165, loss = 0.02112484\n",
      "Iteration 166, loss = 0.02090247\n",
      "Iteration 167, loss = 0.02068142\n",
      "Iteration 168, loss = 0.02046417\n",
      "Iteration 169, loss = 0.02024688\n",
      "Iteration 170, loss = 0.02003603\n",
      "Iteration 171, loss = 0.01982219\n",
      "Iteration 172, loss = 0.01960667\n",
      "Iteration 173, loss = 0.01939538\n",
      "Iteration 174, loss = 0.01918959\n",
      "Iteration 175, loss = 0.01898845\n",
      "Iteration 176, loss = 0.01878745\n",
      "Iteration 177, loss = 0.01859305\n",
      "Iteration 178, loss = 0.01840027\n",
      "Iteration 179, loss = 0.01820876\n",
      "Iteration 180, loss = 0.01802111\n",
      "Iteration 181, loss = 0.01783598\n",
      "Iteration 182, loss = 0.01765346\n",
      "Iteration 183, loss = 0.01747578\n",
      "Iteration 184, loss = 0.01729679\n",
      "Iteration 185, loss = 0.01712093\n",
      "Iteration 186, loss = 0.01694630\n",
      "Iteration 187, loss = 0.01677359\n",
      "Iteration 188, loss = 0.01660491\n",
      "Iteration 189, loss = 0.01643894\n",
      "Iteration 190, loss = 0.01627440\n",
      "Iteration 191, loss = 0.01611586\n",
      "Iteration 192, loss = 0.01595802\n",
      "Iteration 193, loss = 0.01580151\n",
      "Iteration 194, loss = 0.01564615\n",
      "Iteration 195, loss = 0.01550078\n",
      "Iteration 196, loss = 0.01536275\n",
      "Iteration 197, loss = 0.01522444\n",
      "Iteration 198, loss = 0.01508735\n",
      "Iteration 199, loss = 0.01495342\n",
      "Iteration 200, loss = 0.01482118\n",
      "Iteration 201, loss = 0.01469013\n",
      "Iteration 202, loss = 0.01456002\n",
      "Iteration 203, loss = 0.01443004\n",
      "Iteration 204, loss = 0.01430097\n",
      "Iteration 205, loss = 0.01417211\n",
      "Iteration 206, loss = 0.01404496\n",
      "Iteration 207, loss = 0.01391704\n",
      "Iteration 208, loss = 0.01379095\n",
      "Iteration 209, loss = 0.01366789\n",
      "Iteration 210, loss = 0.01354851\n",
      "Iteration 211, loss = 0.01343381\n",
      "Iteration 212, loss = 0.01331783\n",
      "Iteration 213, loss = 0.01320376\n",
      "Iteration 214, loss = 0.01309137\n",
      "Iteration 215, loss = 0.01297844\n",
      "Iteration 216, loss = 0.01286555\n",
      "Iteration 217, loss = 0.01275469\n",
      "Iteration 218, loss = 0.01264613\n",
      "Iteration 219, loss = 0.01253841\n",
      "Iteration 220, loss = 0.01243274\n",
      "Iteration 221, loss = 0.01233176\n",
      "Iteration 222, loss = 0.01223044\n",
      "Iteration 223, loss = 0.01212758\n",
      "Iteration 224, loss = 0.01202480\n",
      "Iteration 225, loss = 0.01192625\n",
      "Iteration 226, loss = 0.01183993\n",
      "Iteration 227, loss = 0.01175548\n",
      "Iteration 228, loss = 0.01167017\n",
      "Iteration 229, loss = 0.01158366\n",
      "Iteration 230, loss = 0.01149835\n",
      "Iteration 231, loss = 0.01141246\n",
      "Iteration 232, loss = 0.01132664\n",
      "Iteration 233, loss = 0.01124268\n",
      "Iteration 234, loss = 0.01116255\n",
      "Iteration 235, loss = 0.01108076\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.50829378\n",
      "Iteration 2, loss = 0.47366733\n",
      "Iteration 3, loss = 0.44181858\n",
      "Iteration 4, loss = 0.41257371\n",
      "Iteration 5, loss = 0.38574493\n",
      "Iteration 6, loss = 0.36109634\n",
      "Iteration 7, loss = 0.33844530\n",
      "Iteration 8, loss = 0.31756191\n",
      "Iteration 9, loss = 0.29830382\n",
      "Iteration 10, loss = 0.28058427\n",
      "Iteration 11, loss = 0.26433148\n",
      "Iteration 12, loss = 0.24940187\n",
      "Iteration 13, loss = 0.23568972\n",
      "Iteration 14, loss = 0.22312329\n",
      "Iteration 15, loss = 0.21156959\n",
      "Iteration 16, loss = 0.20097514\n",
      "Iteration 17, loss = 0.19130660\n",
      "Iteration 18, loss = 0.18256412\n",
      "Iteration 19, loss = 0.17467468\n",
      "Iteration 20, loss = 0.16758302\n",
      "Iteration 21, loss = 0.16122928\n",
      "Iteration 22, loss = 0.15550386\n",
      "Iteration 23, loss = 0.15033139\n",
      "Iteration 24, loss = 0.14561808\n",
      "Iteration 25, loss = 0.14125497\n",
      "Iteration 26, loss = 0.13713957\n",
      "Iteration 27, loss = 0.13322762\n",
      "Iteration 28, loss = 0.12947340\n",
      "Iteration 29, loss = 0.12585251\n",
      "Iteration 30, loss = 0.12235074\n",
      "Iteration 31, loss = 0.11895889\n",
      "Iteration 32, loss = 0.11566384\n",
      "Iteration 33, loss = 0.11247290\n",
      "Iteration 34, loss = 0.10938265\n",
      "Iteration 35, loss = 0.10638732\n",
      "Iteration 36, loss = 0.10349234\n",
      "Iteration 37, loss = 0.10068582\n",
      "Iteration 38, loss = 0.09797185\n",
      "Iteration 39, loss = 0.09535395\n",
      "Iteration 40, loss = 0.09282452\n",
      "Iteration 41, loss = 0.09041333\n",
      "Iteration 42, loss = 0.08812040\n",
      "Iteration 43, loss = 0.08593922\n",
      "Iteration 44, loss = 0.08388360\n",
      "Iteration 45, loss = 0.08194477\n",
      "Iteration 46, loss = 0.08011380\n",
      "Iteration 47, loss = 0.07839336\n",
      "Iteration 48, loss = 0.07678096\n",
      "Iteration 49, loss = 0.07524097\n",
      "Iteration 50, loss = 0.07378603\n",
      "Iteration 51, loss = 0.07239323\n",
      "Iteration 52, loss = 0.07106016\n",
      "Iteration 53, loss = 0.06979503\n",
      "Iteration 54, loss = 0.06860161\n",
      "Iteration 55, loss = 0.06748017\n",
      "Iteration 56, loss = 0.06640886\n",
      "Iteration 57, loss = 0.06539534\n",
      "Iteration 58, loss = 0.06443221\n",
      "Iteration 59, loss = 0.06351850\n",
      "Iteration 60, loss = 0.06264869\n",
      "Iteration 61, loss = 0.06181828\n",
      "Iteration 62, loss = 0.06102532\n",
      "Iteration 63, loss = 0.06025813\n",
      "Iteration 64, loss = 0.05951955\n",
      "Iteration 65, loss = 0.05881570\n",
      "Iteration 66, loss = 0.05813197\n",
      "Iteration 67, loss = 0.05745692\n",
      "Iteration 68, loss = 0.05679761\n",
      "Iteration 69, loss = 0.05615647\n",
      "Iteration 70, loss = 0.05553098\n",
      "Iteration 71, loss = 0.05492172\n",
      "Iteration 72, loss = 0.05433290\n",
      "Iteration 73, loss = 0.05376204\n",
      "Iteration 74, loss = 0.05320806\n",
      "Iteration 75, loss = 0.05266257\n",
      "Iteration 76, loss = 0.05212698\n",
      "Iteration 77, loss = 0.05160204\n",
      "Iteration 78, loss = 0.05109008\n",
      "Iteration 79, loss = 0.05058746\n",
      "Iteration 80, loss = 0.05009206\n",
      "Iteration 81, loss = 0.04960561\n",
      "Iteration 82, loss = 0.04912714\n",
      "Iteration 83, loss = 0.04865255\n",
      "Iteration 84, loss = 0.04818238\n",
      "Iteration 85, loss = 0.04771979\n",
      "Iteration 86, loss = 0.04726968\n",
      "Iteration 87, loss = 0.04682774\n",
      "Iteration 88, loss = 0.04638480\n",
      "Iteration 89, loss = 0.04594537\n",
      "Iteration 90, loss = 0.04551041\n",
      "Iteration 91, loss = 0.04507857\n",
      "Iteration 92, loss = 0.04465192\n",
      "Iteration 93, loss = 0.04423105\n",
      "Iteration 94, loss = 0.04381138\n",
      "Iteration 95, loss = 0.04339177\n",
      "Iteration 96, loss = 0.04297614\n",
      "Iteration 97, loss = 0.04256666\n",
      "Iteration 98, loss = 0.04216350\n",
      "Iteration 99, loss = 0.04176210\n",
      "Iteration 100, loss = 0.04136343\n",
      "Iteration 101, loss = 0.04096515\n",
      "Iteration 102, loss = 0.04057179\n",
      "Iteration 103, loss = 0.04018570\n",
      "Iteration 104, loss = 0.03980442\n",
      "Iteration 105, loss = 0.03942205\n",
      "Iteration 106, loss = 0.03904222\n",
      "Iteration 107, loss = 0.03866372\n",
      "Iteration 108, loss = 0.03828774\n",
      "Iteration 109, loss = 0.03791729\n",
      "Iteration 110, loss = 0.03754999\n",
      "Iteration 111, loss = 0.03718420\n",
      "Iteration 112, loss = 0.03681989\n",
      "Iteration 113, loss = 0.03645775\n",
      "Iteration 114, loss = 0.03609687\n",
      "Iteration 115, loss = 0.03574031\n",
      "Iteration 116, loss = 0.03538479\n",
      "Iteration 117, loss = 0.03502884\n",
      "Iteration 118, loss = 0.03467877\n",
      "Iteration 119, loss = 0.03433591\n",
      "Iteration 120, loss = 0.03399421\n",
      "Iteration 121, loss = 0.03365210\n",
      "Iteration 122, loss = 0.03331195\n",
      "Iteration 123, loss = 0.03297302\n",
      "Iteration 124, loss = 0.03263690\n",
      "Iteration 125, loss = 0.03230758\n",
      "Iteration 126, loss = 0.03198186\n",
      "Iteration 127, loss = 0.03165928\n",
      "Iteration 128, loss = 0.03133713\n",
      "Iteration 129, loss = 0.03101565\n",
      "Iteration 130, loss = 0.03069374\n",
      "Iteration 131, loss = 0.03037464\n",
      "Iteration 132, loss = 0.03005651\n",
      "Iteration 133, loss = 0.02974268\n",
      "Iteration 134, loss = 0.02942990\n",
      "Iteration 135, loss = 0.02911976\n",
      "Iteration 136, loss = 0.02881562\n",
      "Iteration 137, loss = 0.02851381\n",
      "Iteration 138, loss = 0.02821242\n",
      "Iteration 139, loss = 0.02791598\n",
      "Iteration 140, loss = 0.02761972\n",
      "Iteration 141, loss = 0.02732588\n",
      "Iteration 142, loss = 0.02703808\n",
      "Iteration 143, loss = 0.02675132\n",
      "Iteration 144, loss = 0.02646676\n",
      "Iteration 145, loss = 0.02618749\n",
      "Iteration 146, loss = 0.02591221\n",
      "Iteration 147, loss = 0.02564090\n",
      "Iteration 148, loss = 0.02537181\n",
      "Iteration 149, loss = 0.02510316\n",
      "Iteration 150, loss = 0.02483533\n",
      "Iteration 151, loss = 0.02457343\n",
      "Iteration 152, loss = 0.02431324\n",
      "Iteration 153, loss = 0.02405479\n",
      "Iteration 154, loss = 0.02379744\n",
      "Iteration 155, loss = 0.02354211\n",
      "Iteration 156, loss = 0.02328894\n",
      "Iteration 157, loss = 0.02304052\n",
      "Iteration 158, loss = 0.02279245\n",
      "Iteration 159, loss = 0.02254673\n",
      "Iteration 160, loss = 0.02230355\n",
      "Iteration 161, loss = 0.02205931\n",
      "Iteration 162, loss = 0.02181808\n",
      "Iteration 163, loss = 0.02158136\n",
      "Iteration 164, loss = 0.02134960\n",
      "Iteration 165, loss = 0.02112484\n",
      "Iteration 166, loss = 0.02090247\n",
      "Iteration 167, loss = 0.02068142\n",
      "Iteration 168, loss = 0.02046417\n",
      "Iteration 169, loss = 0.02024688\n",
      "Iteration 170, loss = 0.02003603\n",
      "Iteration 171, loss = 0.01982219\n",
      "Iteration 172, loss = 0.01960667\n",
      "Iteration 173, loss = 0.01939538\n",
      "Iteration 174, loss = 0.01918959\n",
      "Iteration 175, loss = 0.01898845\n",
      "Iteration 176, loss = 0.01878745\n",
      "Iteration 177, loss = 0.01859305\n",
      "Iteration 178, loss = 0.01840027\n",
      "Iteration 179, loss = 0.01820876\n",
      "Iteration 180, loss = 0.01802111\n",
      "Iteration 181, loss = 0.01783598\n",
      "Iteration 182, loss = 0.01765346\n",
      "Iteration 183, loss = 0.01747578\n",
      "Iteration 184, loss = 0.01729679\n",
      "Iteration 185, loss = 0.01712093\n",
      "Iteration 186, loss = 0.01694630\n",
      "Iteration 187, loss = 0.01677359\n",
      "Iteration 188, loss = 0.01660491\n",
      "Iteration 189, loss = 0.01643894\n",
      "Iteration 190, loss = 0.01627440\n",
      "Iteration 191, loss = 0.01611586\n",
      "Iteration 192, loss = 0.01595802\n",
      "Iteration 193, loss = 0.01580151\n",
      "Iteration 194, loss = 0.01564615\n",
      "Iteration 195, loss = 0.01550078\n",
      "Iteration 196, loss = 0.01536275\n",
      "Iteration 197, loss = 0.01522444\n",
      "Iteration 198, loss = 0.01508735\n",
      "Iteration 199, loss = 0.01495342\n",
      "Iteration 200, loss = 0.01482118\n",
      "Iteration 201, loss = 0.01469013\n",
      "Iteration 202, loss = 0.01456002\n",
      "Iteration 203, loss = 0.01443004\n",
      "Iteration 204, loss = 0.01430097\n",
      "Iteration 205, loss = 0.01417211\n",
      "Iteration 206, loss = 0.01404496\n",
      "Iteration 207, loss = 0.01391704\n",
      "Iteration 208, loss = 0.01379095\n",
      "Iteration 209, loss = 0.01366789\n",
      "Iteration 210, loss = 0.01354851\n",
      "Iteration 211, loss = 0.01343381\n",
      "Iteration 212, loss = 0.01331783\n",
      "Iteration 213, loss = 0.01320376\n",
      "Iteration 214, loss = 0.01309137\n",
      "Iteration 215, loss = 0.01297844\n",
      "Iteration 216, loss = 0.01286555\n",
      "Iteration 217, loss = 0.01275469\n",
      "Iteration 218, loss = 0.01264613\n",
      "Iteration 219, loss = 0.01253841\n",
      "Iteration 220, loss = 0.01243274\n",
      "Iteration 221, loss = 0.01233176\n",
      "Iteration 222, loss = 0.01223044\n",
      "Iteration 223, loss = 0.01212758\n",
      "Iteration 224, loss = 0.01202480\n",
      "Iteration 225, loss = 0.01192625\n",
      "Iteration 226, loss = 0.01183993\n",
      "Iteration 227, loss = 0.01175548\n",
      "Iteration 228, loss = 0.01167017\n",
      "Iteration 229, loss = 0.01158366\n",
      "Iteration 230, loss = 0.01149835\n",
      "Iteration 231, loss = 0.01141246\n",
      "Iteration 232, loss = 0.01132664\n",
      "Iteration 233, loss = 0.01124268\n",
      "Iteration 234, loss = 0.01116255\n",
      "Iteration 235, loss = 0.01108076\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.50829378\n",
      "Iteration 2, loss = 0.47366733\n",
      "Iteration 3, loss = 0.44181858\n",
      "Iteration 4, loss = 0.41257371\n",
      "Iteration 5, loss = 0.38574493\n",
      "Iteration 6, loss = 0.36109634\n",
      "Iteration 7, loss = 0.33844530\n",
      "Iteration 8, loss = 0.31756191\n",
      "Iteration 9, loss = 0.29830382\n",
      "Iteration 10, loss = 0.28058427\n",
      "Iteration 11, loss = 0.26433148\n",
      "Iteration 12, loss = 0.24940187\n",
      "Iteration 13, loss = 0.23568972\n",
      "Iteration 14, loss = 0.22312329\n",
      "Iteration 15, loss = 0.21156959\n",
      "Iteration 16, loss = 0.20097514\n",
      "Iteration 17, loss = 0.19130660\n",
      "Iteration 18, loss = 0.18256412\n",
      "Iteration 19, loss = 0.17467468\n",
      "Iteration 20, loss = 0.16758302\n",
      "Iteration 21, loss = 0.16122928\n",
      "Iteration 22, loss = 0.15550386\n",
      "Iteration 23, loss = 0.15033139\n",
      "Iteration 24, loss = 0.14561808\n",
      "Iteration 25, loss = 0.14125497\n",
      "Iteration 26, loss = 0.13713957\n",
      "Iteration 27, loss = 0.13322762\n",
      "Iteration 28, loss = 0.12947340\n",
      "Iteration 29, loss = 0.12585251\n",
      "Iteration 30, loss = 0.12235074\n",
      "Iteration 31, loss = 0.11895889\n",
      "Iteration 32, loss = 0.11566384\n",
      "Iteration 33, loss = 0.11247290\n",
      "Iteration 34, loss = 0.10938265\n",
      "Iteration 35, loss = 0.10638732\n",
      "Iteration 36, loss = 0.10349234\n",
      "Iteration 37, loss = 0.10068582\n",
      "Iteration 38, loss = 0.09797185\n",
      "Iteration 39, loss = 0.09535395\n",
      "Iteration 40, loss = 0.09282452\n",
      "Iteration 41, loss = 0.09041333\n",
      "Iteration 42, loss = 0.08812040\n",
      "Iteration 43, loss = 0.08593922\n",
      "Iteration 44, loss = 0.08388360\n",
      "Iteration 45, loss = 0.08194477\n",
      "Iteration 46, loss = 0.08011380\n",
      "Iteration 47, loss = 0.07839336\n",
      "Iteration 48, loss = 0.07678096\n",
      "Iteration 49, loss = 0.07524097\n",
      "Iteration 50, loss = 0.07378603\n",
      "Iteration 51, loss = 0.07239323\n",
      "Iteration 52, loss = 0.07106016\n",
      "Iteration 53, loss = 0.06979503\n",
      "Iteration 54, loss = 0.06860161\n",
      "Iteration 55, loss = 0.06748017\n",
      "Iteration 56, loss = 0.06640886\n",
      "Iteration 57, loss = 0.06539534\n",
      "Iteration 58, loss = 0.06443221\n",
      "Iteration 59, loss = 0.06351850\n",
      "Iteration 60, loss = 0.06264869\n",
      "Iteration 61, loss = 0.06181828\n",
      "Iteration 62, loss = 0.06102532\n",
      "Iteration 63, loss = 0.06025813\n",
      "Iteration 64, loss = 0.05951955\n",
      "Iteration 65, loss = 0.05881570\n",
      "Iteration 66, loss = 0.05813197\n",
      "Iteration 67, loss = 0.05745692\n",
      "Iteration 68, loss = 0.05679761\n",
      "Iteration 69, loss = 0.05615647\n",
      "Iteration 70, loss = 0.05553098\n",
      "Iteration 71, loss = 0.05492172\n",
      "Iteration 72, loss = 0.05433290\n",
      "Iteration 73, loss = 0.05376204\n",
      "Iteration 74, loss = 0.05320806\n",
      "Iteration 75, loss = 0.05266257\n",
      "Iteration 76, loss = 0.05212698\n",
      "Iteration 77, loss = 0.05160204\n",
      "Iteration 78, loss = 0.05109008\n",
      "Iteration 79, loss = 0.05058746\n",
      "Iteration 80, loss = 0.05009206\n",
      "Iteration 81, loss = 0.04960561\n",
      "Iteration 82, loss = 0.04912714\n",
      "Iteration 83, loss = 0.04865255\n",
      "Iteration 84, loss = 0.04818238\n",
      "Iteration 85, loss = 0.04771979\n",
      "Iteration 86, loss = 0.04726968\n",
      "Iteration 87, loss = 0.04682774\n",
      "Iteration 88, loss = 0.04638480\n",
      "Iteration 89, loss = 0.04594537\n",
      "Iteration 90, loss = 0.04551041\n",
      "Iteration 91, loss = 0.04507857\n",
      "Iteration 92, loss = 0.04465192\n",
      "Iteration 93, loss = 0.04423105\n",
      "Iteration 94, loss = 0.04381138\n",
      "Iteration 95, loss = 0.04339177\n",
      "Iteration 96, loss = 0.04297614\n",
      "Iteration 97, loss = 0.04256666\n",
      "Iteration 98, loss = 0.04216350\n",
      "Iteration 99, loss = 0.04176210\n",
      "Iteration 100, loss = 0.04136343\n",
      "Iteration 101, loss = 0.04096515\n",
      "Iteration 102, loss = 0.04057179\n",
      "Iteration 103, loss = 0.04018570\n",
      "Iteration 104, loss = 0.03980442\n",
      "Iteration 105, loss = 0.03942205\n",
      "Iteration 106, loss = 0.03904222\n",
      "Iteration 107, loss = 0.03866372\n",
      "Iteration 108, loss = 0.03828774\n",
      "Iteration 109, loss = 0.03791729\n",
      "Iteration 110, loss = 0.03754999\n",
      "Iteration 111, loss = 0.03718420\n",
      "Iteration 112, loss = 0.03681989\n",
      "Iteration 113, loss = 0.03645775\n",
      "Iteration 114, loss = 0.03609687\n",
      "Iteration 115, loss = 0.03574031\n",
      "Iteration 116, loss = 0.03538479\n",
      "Iteration 117, loss = 0.03502884\n",
      "Iteration 118, loss = 0.03467877\n",
      "Iteration 119, loss = 0.03433591\n",
      "Iteration 120, loss = 0.03399421\n",
      "Iteration 121, loss = 0.03365210\n",
      "Iteration 122, loss = 0.03331195\n",
      "Iteration 123, loss = 0.03297302\n",
      "Iteration 124, loss = 0.03263690\n",
      "Iteration 125, loss = 0.03230758\n",
      "Iteration 126, loss = 0.03198186\n",
      "Iteration 127, loss = 0.03165928\n",
      "Iteration 128, loss = 0.03133713\n",
      "Iteration 129, loss = 0.03101565\n",
      "Iteration 130, loss = 0.03069374\n",
      "Iteration 131, loss = 0.03037464\n",
      "Iteration 132, loss = 0.03005651\n",
      "Iteration 133, loss = 0.02974268\n",
      "Iteration 134, loss = 0.02942990\n",
      "Iteration 135, loss = 0.02911976\n",
      "Iteration 136, loss = 0.02881562\n",
      "Iteration 137, loss = 0.02851381\n",
      "Iteration 138, loss = 0.02821242\n",
      "Iteration 139, loss = 0.02791598\n",
      "Iteration 140, loss = 0.02761972\n",
      "Iteration 141, loss = 0.02732588\n",
      "Iteration 142, loss = 0.02703808\n",
      "Iteration 143, loss = 0.02675132\n",
      "Iteration 144, loss = 0.02646676\n",
      "Iteration 145, loss = 0.02618749\n",
      "Iteration 146, loss = 0.02591221\n",
      "Iteration 147, loss = 0.02564090\n",
      "Iteration 148, loss = 0.02537181\n",
      "Iteration 149, loss = 0.02510316\n",
      "Iteration 150, loss = 0.02483533\n",
      "Iteration 151, loss = 0.02457343\n",
      "Iteration 152, loss = 0.02431324\n",
      "Iteration 153, loss = 0.02405479\n",
      "Iteration 154, loss = 0.02379744\n",
      "Iteration 155, loss = 0.02354211\n",
      "Iteration 156, loss = 0.02328894\n",
      "Iteration 157, loss = 0.02304052\n",
      "Iteration 158, loss = 0.02279245\n",
      "Iteration 159, loss = 0.02254673\n",
      "Iteration 160, loss = 0.02230355\n",
      "Iteration 161, loss = 0.02205931\n",
      "Iteration 162, loss = 0.02181808\n",
      "Iteration 163, loss = 0.02158136\n",
      "Iteration 164, loss = 0.02134960\n",
      "Iteration 165, loss = 0.02112484\n",
      "Iteration 166, loss = 0.02090247\n",
      "Iteration 167, loss = 0.02068142\n",
      "Iteration 168, loss = 0.02046417\n",
      "Iteration 169, loss = 0.02024688\n",
      "Iteration 170, loss = 0.02003603\n",
      "Iteration 171, loss = 0.01982219\n",
      "Iteration 172, loss = 0.01960667\n",
      "Iteration 173, loss = 0.01939538\n",
      "Iteration 174, loss = 0.01918959\n",
      "Iteration 175, loss = 0.01898845\n",
      "Iteration 176, loss = 0.01878745\n",
      "Iteration 177, loss = 0.01859305\n",
      "Iteration 178, loss = 0.01840027\n",
      "Iteration 179, loss = 0.01820876\n",
      "Iteration 180, loss = 0.01802111\n",
      "Iteration 181, loss = 0.01783598\n",
      "Iteration 182, loss = 0.01765346\n",
      "Iteration 183, loss = 0.01747578\n",
      "Iteration 184, loss = 0.01729679\n",
      "Iteration 185, loss = 0.01712093\n",
      "Iteration 186, loss = 0.01694630\n",
      "Iteration 187, loss = 0.01677359\n",
      "Iteration 188, loss = 0.01660491\n",
      "Iteration 189, loss = 0.01643894\n",
      "Iteration 190, loss = 0.01627440\n",
      "Iteration 191, loss = 0.01611586\n",
      "Iteration 192, loss = 0.01595802\n",
      "Iteration 193, loss = 0.01580151\n",
      "Iteration 194, loss = 0.01564615\n",
      "Iteration 195, loss = 0.01550078\n",
      "Iteration 196, loss = 0.01536275\n",
      "Iteration 197, loss = 0.01522444\n",
      "Iteration 198, loss = 0.01508735\n",
      "Iteration 199, loss = 0.01495342\n",
      "Iteration 200, loss = 0.01482118\n",
      "Iteration 201, loss = 0.01469013\n",
      "Iteration 202, loss = 0.01456002\n",
      "Iteration 203, loss = 0.01443004\n",
      "Iteration 204, loss = 0.01430097\n",
      "Iteration 205, loss = 0.01417211\n",
      "Iteration 206, loss = 0.01404496\n",
      "Iteration 207, loss = 0.01391704\n",
      "Iteration 208, loss = 0.01379095\n",
      "Iteration 209, loss = 0.01366789\n",
      "Iteration 210, loss = 0.01354851\n",
      "Iteration 211, loss = 0.01343381\n",
      "Iteration 212, loss = 0.01331783\n",
      "Iteration 213, loss = 0.01320376\n",
      "Iteration 214, loss = 0.01309137\n",
      "Iteration 215, loss = 0.01297844\n",
      "Iteration 216, loss = 0.01286555\n",
      "Iteration 217, loss = 0.01275469\n",
      "Iteration 218, loss = 0.01264613\n",
      "Iteration 219, loss = 0.01253841\n",
      "Iteration 220, loss = 0.01243274\n",
      "Iteration 221, loss = 0.01233176\n",
      "Iteration 222, loss = 0.01223044\n",
      "Iteration 223, loss = 0.01212758\n",
      "Iteration 224, loss = 0.01202480\n",
      "Iteration 225, loss = 0.01192625\n",
      "Iteration 226, loss = 0.01183993\n",
      "Iteration 227, loss = 0.01175548\n",
      "Iteration 228, loss = 0.01167017\n",
      "Iteration 229, loss = 0.01158366\n",
      "Iteration 230, loss = 0.01149835\n",
      "Iteration 231, loss = 0.01141246\n",
      "Iteration 232, loss = 0.01132664\n",
      "Iteration 233, loss = 0.01124268\n",
      "Iteration 234, loss = 0.01116255\n",
      "Iteration 235, loss = 0.01108076\n",
      "LinearRegression\n",
      "NeuralNetwork\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51373657\n",
      "Iteration 2, loss = 0.45189658\n",
      "Iteration 3, loss = 0.39857901\n",
      "Iteration 4, loss = 0.35469052\n",
      "Iteration 5, loss = 0.31739088\n",
      "Iteration 6, loss = 0.28147383\n",
      "Iteration 7, loss = 0.25663505\n",
      "Iteration 8, loss = 0.23280220\n",
      "Iteration 9, loss = 0.21522562\n",
      "Iteration 10, loss = 0.20065343\n",
      "Iteration 11, loss = 0.18877210\n",
      "Iteration 12, loss = 0.17733359\n",
      "Iteration 13, loss = 0.16805855\n",
      "Iteration 14, loss = 0.15829857\n",
      "Iteration 15, loss = 0.14934063\n",
      "Iteration 16, loss = 0.14103989\n",
      "Iteration 17, loss = 0.13200512\n",
      "Iteration 18, loss = 0.12474633\n",
      "Iteration 19, loss = 0.11805786\n",
      "Iteration 20, loss = 0.11244535\n",
      "Iteration 21, loss = 0.10751595\n",
      "Iteration 22, loss = 0.10298381\n",
      "Iteration 23, loss = 0.09891512\n",
      "Iteration 24, loss = 0.09475974\n",
      "Iteration 25, loss = 0.09070656\n",
      "Iteration 26, loss = 0.08671481\n",
      "Iteration 27, loss = 0.08267319\n",
      "Iteration 28, loss = 0.07886303\n",
      "Iteration 29, loss = 0.07494402\n",
      "Iteration 30, loss = 0.07182287\n",
      "Iteration 31, loss = 0.06893344\n",
      "Iteration 32, loss = 0.06609894\n",
      "Iteration 33, loss = 0.06397135\n",
      "Iteration 34, loss = 0.06191610\n",
      "Iteration 35, loss = 0.05995807\n",
      "Iteration 36, loss = 0.05837565\n",
      "Iteration 37, loss = 0.05664996\n",
      "Iteration 38, loss = 0.05483299\n",
      "Iteration 39, loss = 0.05344013\n",
      "Iteration 40, loss = 0.05242207\n",
      "Iteration 41, loss = 0.05118947\n",
      "Iteration 42, loss = 0.05000890\n",
      "Iteration 43, loss = 0.04914655\n",
      "Iteration 44, loss = 0.04850207\n",
      "Iteration 45, loss = 0.04767599\n",
      "Iteration 46, loss = 0.04670437\n",
      "Iteration 47, loss = 0.04606381\n",
      "Iteration 48, loss = 0.04584476\n",
      "Iteration 49, loss = 0.04499342\n",
      "Iteration 50, loss = 0.04405259\n",
      "Iteration 51, loss = 0.04327589\n",
      "Iteration 52, loss = 0.04278518\n",
      "Iteration 53, loss = 0.04211646\n",
      "Iteration 54, loss = 0.04122839\n",
      "Iteration 55, loss = 0.04047285\n",
      "Iteration 56, loss = 0.03982264\n",
      "Iteration 57, loss = 0.03916307\n",
      "Iteration 58, loss = 0.03877577\n",
      "Iteration 59, loss = 0.03797318\n",
      "Iteration 60, loss = 0.03760106\n",
      "Iteration 61, loss = 0.03730900\n",
      "Iteration 62, loss = 0.03703096\n",
      "Iteration 63, loss = 0.03625182\n",
      "Iteration 64, loss = 0.03526328\n",
      "Iteration 65, loss = 0.03506083\n",
      "Iteration 66, loss = 0.03505675\n",
      "Iteration 67, loss = 0.03487710\n",
      "Iteration 68, loss = 0.03457666\n",
      "Iteration 69, loss = 0.03426343\n",
      "Iteration 70, loss = 0.03382872\n",
      "Iteration 71, loss = 0.03336865\n",
      "Iteration 72, loss = 0.03268961\n",
      "Iteration 73, loss = 0.03241960\n",
      "Iteration 74, loss = 0.03196415\n",
      "Iteration 75, loss = 0.03121095\n",
      "Iteration 76, loss = 0.03064771\n",
      "Iteration 77, loss = 0.03095881\n",
      "Iteration 78, loss = 0.03081818\n",
      "Iteration 79, loss = 0.03016426\n",
      "Iteration 80, loss = 0.02943331\n",
      "Iteration 81, loss = 0.02907466\n",
      "Iteration 82, loss = 0.02861862\n",
      "Iteration 83, loss = 0.02836684\n",
      "Iteration 84, loss = 0.02792782\n",
      "Iteration 85, loss = 0.02748967\n",
      "Iteration 86, loss = 0.02710637\n",
      "Iteration 87, loss = 0.02685248\n",
      "Iteration 88, loss = 0.02658459\n",
      "Iteration 89, loss = 0.02632990\n",
      "Iteration 90, loss = 0.02610295\n",
      "Iteration 91, loss = 0.02577655\n",
      "Iteration 92, loss = 0.02553563\n",
      "Iteration 93, loss = 0.02550257\n",
      "Iteration 94, loss = 0.02514326\n",
      "Iteration 95, loss = 0.02469602\n",
      "Iteration 96, loss = 0.02426111\n",
      "Iteration 97, loss = 0.02404043\n",
      "Iteration 98, loss = 0.02386450\n",
      "Iteration 99, loss = 0.02362677\n",
      "Iteration 100, loss = 0.02341173\n",
      "Iteration 101, loss = 0.02358088\n",
      "Iteration 102, loss = 0.02291126\n",
      "Iteration 103, loss = 0.02270808\n",
      "Iteration 104, loss = 0.02287617\n",
      "Iteration 105, loss = 0.02301715\n",
      "Iteration 106, loss = 0.02262047\n",
      "Iteration 107, loss = 0.02218034\n",
      "Iteration 108, loss = 0.02221283\n",
      "Iteration 109, loss = 0.02201399\n",
      "Iteration 110, loss = 0.02156785\n",
      "Iteration 111, loss = 0.02128050\n",
      "Iteration 112, loss = 0.02121488\n",
      "Iteration 113, loss = 0.02103984\n",
      "Iteration 114, loss = 0.02080409\n",
      "Iteration 115, loss = 0.02046942\n",
      "Iteration 116, loss = 0.02036072\n",
      "Iteration 117, loss = 0.02021794\n",
      "Iteration 118, loss = 0.02021137\n",
      "Iteration 119, loss = 0.02019993\n",
      "Iteration 120, loss = 0.02013236\n",
      "Iteration 121, loss = 0.01983329\n",
      "Iteration 122, loss = 0.01940152\n",
      "Iteration 123, loss = 0.01951531\n",
      "Iteration 124, loss = 0.01969092\n",
      "Iteration 125, loss = 0.01962311\n",
      "Iteration 126, loss = 0.01938340\n",
      "Iteration 127, loss = 0.01915121\n",
      "Iteration 128, loss = 0.01896982\n",
      "Iteration 129, loss = 0.01871172\n",
      "Iteration 130, loss = 0.01854831\n",
      "Iteration 131, loss = 0.01900479\n",
      "Iteration 132, loss = 0.01911257\n",
      "Iteration 133, loss = 0.01831836\n",
      "Iteration 134, loss = 0.01858969\n",
      "Iteration 135, loss = 0.01845061\n",
      "Iteration 136, loss = 0.01786277\n",
      "Iteration 137, loss = 0.01822729\n",
      "Iteration 138, loss = 0.01879183\n",
      "Iteration 139, loss = 0.01774872\n",
      "Iteration 140, loss = 0.01757933\n",
      "Iteration 141, loss = 0.01889935\n",
      "Iteration 142, loss = 0.01849149\n",
      "Iteration 143, loss = 0.01739646\n",
      "Iteration 144, loss = 0.01733876\n",
      "Iteration 145, loss = 0.01737619\n",
      "Iteration 146, loss = 0.01729092\n",
      "Iteration 147, loss = 0.01709274\n",
      "Iteration 148, loss = 0.01687804\n",
      "Iteration 149, loss = 0.01683178\n",
      "Iteration 150, loss = 0.01685801\n",
      "Iteration 151, loss = 0.01659326\n",
      "Iteration 152, loss = 0.01644483\n",
      "Iteration 153, loss = 0.01647714\n",
      "Iteration 154, loss = 0.01628153\n",
      "Iteration 155, loss = 0.01613065\n",
      "Iteration 156, loss = 0.01620555\n",
      "Iteration 157, loss = 0.01607587\n",
      "Iteration 158, loss = 0.01590880\n",
      "Iteration 159, loss = 0.01606813\n",
      "Iteration 160, loss = 0.01606592\n",
      "Iteration 161, loss = 0.01582887\n",
      "Iteration 162, loss = 0.01570644\n",
      "Iteration 163, loss = 0.01599672\n",
      "Iteration 164, loss = 0.01581298\n",
      "Iteration 165, loss = 0.01549039\n",
      "Iteration 166, loss = 0.01539329\n",
      "Iteration 167, loss = 0.01551809\n",
      "Iteration 168, loss = 0.01564635\n",
      "Iteration 169, loss = 0.01609129\n",
      "Iteration 170, loss = 0.01565867\n",
      "Iteration 171, loss = 0.01588619\n",
      "Iteration 172, loss = 0.01552448\n",
      "Iteration 173, loss = 0.01499205\n",
      "Iteration 174, loss = 0.01508632\n",
      "Iteration 175, loss = 0.01532426\n",
      "Iteration 176, loss = 0.01492335\n",
      "Iteration 177, loss = 0.01449252\n",
      "Iteration 178, loss = 0.01482612\n",
      "Iteration 179, loss = 0.01507288\n",
      "Iteration 180, loss = 0.01497091\n",
      "Iteration 181, loss = 0.01445480\n",
      "Iteration 182, loss = 0.01466428\n",
      "Iteration 183, loss = 0.01491721\n",
      "Iteration 184, loss = 0.01456705\n",
      "Iteration 185, loss = 0.01436334\n",
      "Iteration 186, loss = 0.01434654\n",
      "Iteration 187, loss = 0.01421495\n",
      "Iteration 188, loss = 0.01403716\n",
      "Iteration 189, loss = 0.01413206\n",
      "Iteration 190, loss = 0.01416981\n",
      "Iteration 191, loss = 0.01395786\n",
      "Iteration 192, loss = 0.01372863\n",
      "Iteration 193, loss = 0.01366540\n",
      "Iteration 194, loss = 0.01360524\n",
      "Iteration 195, loss = 0.01351209\n",
      "Iteration 196, loss = 0.01351376\n",
      "Iteration 197, loss = 0.01347515\n",
      "Iteration 198, loss = 0.01353842\n",
      "Iteration 199, loss = 0.01343990\n",
      "Iteration 200, loss = 0.01334369\n",
      "Iteration 201, loss = 0.01339390\n",
      "Iteration 202, loss = 0.01335115\n",
      "Iteration 203, loss = 0.01328207\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51373657\n",
      "Iteration 2, loss = 0.45189658\n",
      "Iteration 3, loss = 0.39857901\n",
      "Iteration 4, loss = 0.35469052\n",
      "Iteration 5, loss = 0.31739088\n",
      "Iteration 6, loss = 0.28147383\n",
      "Iteration 7, loss = 0.25663505\n",
      "Iteration 8, loss = 0.23280220\n",
      "Iteration 9, loss = 0.21522562\n",
      "Iteration 10, loss = 0.20065343\n",
      "Iteration 11, loss = 0.18877210\n",
      "Iteration 12, loss = 0.17733359\n",
      "Iteration 13, loss = 0.16805855\n",
      "Iteration 14, loss = 0.15829857\n",
      "Iteration 15, loss = 0.14934063\n",
      "Iteration 16, loss = 0.14103989\n",
      "Iteration 17, loss = 0.13200512\n",
      "Iteration 18, loss = 0.12474633\n",
      "Iteration 19, loss = 0.11805786\n",
      "Iteration 20, loss = 0.11244535\n",
      "Iteration 21, loss = 0.10751595\n",
      "Iteration 22, loss = 0.10298381\n",
      "Iteration 23, loss = 0.09891512\n",
      "Iteration 24, loss = 0.09475974\n",
      "Iteration 25, loss = 0.09070656\n",
      "Iteration 26, loss = 0.08671481\n",
      "Iteration 27, loss = 0.08267319\n",
      "Iteration 28, loss = 0.07886303\n",
      "Iteration 29, loss = 0.07494402\n",
      "Iteration 30, loss = 0.07182287\n",
      "Iteration 31, loss = 0.06893344\n",
      "Iteration 32, loss = 0.06609894\n",
      "Iteration 33, loss = 0.06397135\n",
      "Iteration 34, loss = 0.06191610\n",
      "Iteration 35, loss = 0.05995807\n",
      "Iteration 36, loss = 0.05837565\n",
      "Iteration 37, loss = 0.05664996\n",
      "Iteration 38, loss = 0.05483299\n",
      "Iteration 39, loss = 0.05344013\n",
      "Iteration 40, loss = 0.05242207\n",
      "Iteration 41, loss = 0.05118947\n",
      "Iteration 42, loss = 0.05000890\n",
      "Iteration 43, loss = 0.04914655\n",
      "Iteration 44, loss = 0.04850207\n",
      "Iteration 45, loss = 0.04767599\n",
      "Iteration 46, loss = 0.04670437\n",
      "Iteration 47, loss = 0.04606381\n",
      "Iteration 48, loss = 0.04584476\n",
      "Iteration 49, loss = 0.04499342\n",
      "Iteration 50, loss = 0.04405259\n",
      "Iteration 51, loss = 0.04327589\n",
      "Iteration 52, loss = 0.04278518\n",
      "Iteration 53, loss = 0.04211646\n",
      "Iteration 54, loss = 0.04122839\n",
      "Iteration 55, loss = 0.04047285\n",
      "Iteration 56, loss = 0.03982264\n",
      "Iteration 57, loss = 0.03916307\n",
      "Iteration 58, loss = 0.03877577\n",
      "Iteration 59, loss = 0.03797318\n",
      "Iteration 60, loss = 0.03760106\n",
      "Iteration 61, loss = 0.03730900\n",
      "Iteration 62, loss = 0.03703096\n",
      "Iteration 63, loss = 0.03625182\n",
      "Iteration 64, loss = 0.03526328\n",
      "Iteration 65, loss = 0.03506083\n",
      "Iteration 66, loss = 0.03505675\n",
      "Iteration 67, loss = 0.03487710\n",
      "Iteration 68, loss = 0.03457666\n",
      "Iteration 69, loss = 0.03426343\n",
      "Iteration 70, loss = 0.03382872\n",
      "Iteration 71, loss = 0.03336865\n",
      "Iteration 72, loss = 0.03268961\n",
      "Iteration 73, loss = 0.03241960\n",
      "Iteration 74, loss = 0.03196415\n",
      "Iteration 75, loss = 0.03121095\n",
      "Iteration 76, loss = 0.03064771\n",
      "Iteration 77, loss = 0.03095881\n",
      "Iteration 78, loss = 0.03081818\n",
      "Iteration 79, loss = 0.03016426\n",
      "Iteration 80, loss = 0.02943331\n",
      "Iteration 81, loss = 0.02907466\n",
      "Iteration 82, loss = 0.02861862\n",
      "Iteration 83, loss = 0.02836684\n",
      "Iteration 84, loss = 0.02792782\n",
      "Iteration 85, loss = 0.02748967\n",
      "Iteration 86, loss = 0.02710637\n",
      "Iteration 87, loss = 0.02685248\n",
      "Iteration 88, loss = 0.02658459\n",
      "Iteration 89, loss = 0.02632990\n",
      "Iteration 90, loss = 0.02610295\n",
      "Iteration 91, loss = 0.02577655\n",
      "Iteration 92, loss = 0.02553563\n",
      "Iteration 93, loss = 0.02550257\n",
      "Iteration 94, loss = 0.02514326\n",
      "Iteration 95, loss = 0.02469602\n",
      "Iteration 96, loss = 0.02426111\n",
      "Iteration 97, loss = 0.02404043\n",
      "Iteration 98, loss = 0.02386450\n",
      "Iteration 99, loss = 0.02362677\n",
      "Iteration 100, loss = 0.02341173\n",
      "Iteration 101, loss = 0.02358088\n",
      "Iteration 102, loss = 0.02291126\n",
      "Iteration 103, loss = 0.02270808\n",
      "Iteration 104, loss = 0.02287617\n",
      "Iteration 105, loss = 0.02301715\n",
      "Iteration 106, loss = 0.02262047\n",
      "Iteration 107, loss = 0.02218034\n",
      "Iteration 108, loss = 0.02221283\n",
      "Iteration 109, loss = 0.02201399\n",
      "Iteration 110, loss = 0.02156785\n",
      "Iteration 111, loss = 0.02128050\n",
      "Iteration 112, loss = 0.02121488\n",
      "Iteration 113, loss = 0.02103984\n",
      "Iteration 114, loss = 0.02080409\n",
      "Iteration 115, loss = 0.02046942\n",
      "Iteration 116, loss = 0.02036072\n",
      "Iteration 117, loss = 0.02021794\n",
      "Iteration 118, loss = 0.02021137\n",
      "Iteration 119, loss = 0.02019993\n",
      "Iteration 120, loss = 0.02013236\n",
      "Iteration 121, loss = 0.01983329\n",
      "Iteration 122, loss = 0.01940152\n",
      "Iteration 123, loss = 0.01951531\n",
      "Iteration 124, loss = 0.01969092\n",
      "Iteration 125, loss = 0.01962311\n",
      "Iteration 126, loss = 0.01938340\n",
      "Iteration 127, loss = 0.01915121\n",
      "Iteration 128, loss = 0.01896982\n",
      "Iteration 129, loss = 0.01871172\n",
      "Iteration 130, loss = 0.01854831\n",
      "Iteration 131, loss = 0.01900479\n",
      "Iteration 132, loss = 0.01911257\n",
      "Iteration 133, loss = 0.01831836\n",
      "Iteration 134, loss = 0.01858969\n",
      "Iteration 135, loss = 0.01845061\n",
      "Iteration 136, loss = 0.01786277\n",
      "Iteration 137, loss = 0.01822729\n",
      "Iteration 138, loss = 0.01879183\n",
      "Iteration 139, loss = 0.01774872\n",
      "Iteration 140, loss = 0.01757933\n",
      "Iteration 141, loss = 0.01889935\n",
      "Iteration 142, loss = 0.01849149\n",
      "Iteration 143, loss = 0.01739646\n",
      "Iteration 144, loss = 0.01733876\n",
      "Iteration 145, loss = 0.01737619\n",
      "Iteration 146, loss = 0.01729092\n",
      "Iteration 147, loss = 0.01709274\n",
      "Iteration 148, loss = 0.01687804\n",
      "Iteration 149, loss = 0.01683178\n",
      "Iteration 150, loss = 0.01685801\n",
      "Iteration 151, loss = 0.01659326\n",
      "Iteration 152, loss = 0.01644483\n",
      "Iteration 153, loss = 0.01647714\n",
      "Iteration 154, loss = 0.01628153\n",
      "Iteration 155, loss = 0.01613065\n",
      "Iteration 156, loss = 0.01620555\n",
      "Iteration 157, loss = 0.01607587\n",
      "Iteration 158, loss = 0.01590880\n",
      "Iteration 159, loss = 0.01606813\n",
      "Iteration 160, loss = 0.01606592\n",
      "Iteration 161, loss = 0.01582887\n",
      "Iteration 162, loss = 0.01570644\n",
      "Iteration 163, loss = 0.01599672\n",
      "Iteration 164, loss = 0.01581298\n",
      "Iteration 165, loss = 0.01549039\n",
      "Iteration 166, loss = 0.01539329\n",
      "Iteration 167, loss = 0.01551809\n",
      "Iteration 168, loss = 0.01564635\n",
      "Iteration 169, loss = 0.01609129\n",
      "Iteration 170, loss = 0.01565867\n",
      "Iteration 171, loss = 0.01588619\n",
      "Iteration 172, loss = 0.01552448\n",
      "Iteration 173, loss = 0.01499205\n",
      "Iteration 174, loss = 0.01508632\n",
      "Iteration 175, loss = 0.01532426\n",
      "Iteration 176, loss = 0.01492335\n",
      "Iteration 177, loss = 0.01449252\n",
      "Iteration 178, loss = 0.01482612\n",
      "Iteration 179, loss = 0.01507288\n",
      "Iteration 180, loss = 0.01497091\n",
      "Iteration 181, loss = 0.01445480\n",
      "Iteration 182, loss = 0.01466428\n",
      "Iteration 183, loss = 0.01491721\n",
      "Iteration 184, loss = 0.01456705\n",
      "Iteration 185, loss = 0.01436334\n",
      "Iteration 186, loss = 0.01434654\n",
      "Iteration 187, loss = 0.01421495\n",
      "Iteration 188, loss = 0.01403716\n",
      "Iteration 189, loss = 0.01413206\n",
      "Iteration 190, loss = 0.01416981\n",
      "Iteration 191, loss = 0.01395786\n",
      "Iteration 192, loss = 0.01372863\n",
      "Iteration 193, loss = 0.01366540\n",
      "Iteration 194, loss = 0.01360524\n",
      "Iteration 195, loss = 0.01351209\n",
      "Iteration 196, loss = 0.01351376\n",
      "Iteration 197, loss = 0.01347515\n",
      "Iteration 198, loss = 0.01353842\n",
      "Iteration 199, loss = 0.01343990\n",
      "Iteration 200, loss = 0.01334369\n",
      "Iteration 201, loss = 0.01339390\n",
      "Iteration 202, loss = 0.01335115\n",
      "Iteration 203, loss = 0.01328207\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51373657\n",
      "Iteration 2, loss = 0.45189658\n",
      "Iteration 3, loss = 0.39857901\n",
      "Iteration 4, loss = 0.35469052\n",
      "Iteration 5, loss = 0.31739088\n",
      "Iteration 6, loss = 0.28147383\n",
      "Iteration 7, loss = 0.25663505\n",
      "Iteration 8, loss = 0.23280220\n",
      "Iteration 9, loss = 0.21522562\n",
      "Iteration 10, loss = 0.20065343\n",
      "Iteration 11, loss = 0.18877210\n",
      "Iteration 12, loss = 0.17733359\n",
      "Iteration 13, loss = 0.16805855\n",
      "Iteration 14, loss = 0.15829857\n",
      "Iteration 15, loss = 0.14934063\n",
      "Iteration 16, loss = 0.14103989\n",
      "Iteration 17, loss = 0.13200512\n",
      "Iteration 18, loss = 0.12474633\n",
      "Iteration 19, loss = 0.11805786\n",
      "Iteration 20, loss = 0.11244535\n",
      "Iteration 21, loss = 0.10751595\n",
      "Iteration 22, loss = 0.10298381\n",
      "Iteration 23, loss = 0.09891512\n",
      "Iteration 24, loss = 0.09475974\n",
      "Iteration 25, loss = 0.09070656\n",
      "Iteration 26, loss = 0.08671481\n",
      "Iteration 27, loss = 0.08267319\n",
      "Iteration 28, loss = 0.07886303\n",
      "Iteration 29, loss = 0.07494402\n",
      "Iteration 30, loss = 0.07182287\n",
      "Iteration 31, loss = 0.06893344\n",
      "Iteration 32, loss = 0.06609894\n",
      "Iteration 33, loss = 0.06397135\n",
      "Iteration 34, loss = 0.06191610\n",
      "Iteration 35, loss = 0.05995807\n",
      "Iteration 36, loss = 0.05837565\n",
      "Iteration 37, loss = 0.05664996\n",
      "Iteration 38, loss = 0.05483299\n",
      "Iteration 39, loss = 0.05344013\n",
      "Iteration 40, loss = 0.05242207\n",
      "Iteration 41, loss = 0.05118947\n",
      "Iteration 42, loss = 0.05000890\n",
      "Iteration 43, loss = 0.04914655\n",
      "Iteration 44, loss = 0.04850207\n",
      "Iteration 45, loss = 0.04767599\n",
      "Iteration 46, loss = 0.04670437\n",
      "Iteration 47, loss = 0.04606381\n",
      "Iteration 48, loss = 0.04584476\n",
      "Iteration 49, loss = 0.04499342\n",
      "Iteration 50, loss = 0.04405259\n",
      "Iteration 51, loss = 0.04327589\n",
      "Iteration 52, loss = 0.04278518\n",
      "Iteration 53, loss = 0.04211646\n",
      "Iteration 54, loss = 0.04122839\n",
      "Iteration 55, loss = 0.04047285\n",
      "Iteration 56, loss = 0.03982264\n",
      "Iteration 57, loss = 0.03916307\n",
      "Iteration 58, loss = 0.03877577\n",
      "Iteration 59, loss = 0.03797318\n",
      "Iteration 60, loss = 0.03760106\n",
      "Iteration 61, loss = 0.03730900\n",
      "Iteration 62, loss = 0.03703096\n",
      "Iteration 63, loss = 0.03625182\n",
      "Iteration 64, loss = 0.03526328\n",
      "Iteration 65, loss = 0.03506083\n",
      "Iteration 66, loss = 0.03505675\n",
      "Iteration 67, loss = 0.03487710\n",
      "Iteration 68, loss = 0.03457666\n",
      "Iteration 69, loss = 0.03426343\n",
      "Iteration 70, loss = 0.03382872\n",
      "Iteration 71, loss = 0.03336865\n",
      "Iteration 72, loss = 0.03268961\n",
      "Iteration 73, loss = 0.03241960\n",
      "Iteration 74, loss = 0.03196415\n",
      "Iteration 75, loss = 0.03121095\n",
      "Iteration 76, loss = 0.03064771\n",
      "Iteration 77, loss = 0.03095881\n",
      "Iteration 78, loss = 0.03081818\n",
      "Iteration 79, loss = 0.03016426\n",
      "Iteration 80, loss = 0.02943331\n",
      "Iteration 81, loss = 0.02907466\n",
      "Iteration 82, loss = 0.02861862\n",
      "Iteration 83, loss = 0.02836684\n",
      "Iteration 84, loss = 0.02792782\n",
      "Iteration 85, loss = 0.02748967\n",
      "Iteration 86, loss = 0.02710637\n",
      "Iteration 87, loss = 0.02685248\n",
      "Iteration 88, loss = 0.02658459\n",
      "Iteration 89, loss = 0.02632990\n",
      "Iteration 90, loss = 0.02610295\n",
      "Iteration 91, loss = 0.02577655\n",
      "LinearRegression\n",
      "NeuralNetwork\n",
      "Iteration 92, loss = 0.02553563\n",
      "Iteration 93, loss = 0.02550257\n",
      "Iteration 94, loss = 0.02514326\n",
      "Iteration 95, loss = 0.02469602\n",
      "Iteration 96, loss = 0.02426111\n",
      "Iteration 97, loss = 0.02404043\n",
      "Iteration 98, loss = 0.02386450\n",
      "Iteration 99, loss = 0.02362677\n",
      "Iteration 100, loss = 0.02341173\n",
      "Iteration 101, loss = 0.02358088\n",
      "Iteration 102, loss = 0.02291126\n",
      "Iteration 103, loss = 0.02270808\n",
      "Iteration 104, loss = 0.02287617\n",
      "Iteration 105, loss = 0.02301715\n",
      "Iteration 106, loss = 0.02262047\n",
      "Iteration 107, loss = 0.02218034\n",
      "Iteration 108, loss = 0.02221283\n",
      "Iteration 109, loss = 0.02201399\n",
      "Iteration 110, loss = 0.02156785\n",
      "Iteration 111, loss = 0.02128050\n",
      "Iteration 112, loss = 0.02121488\n",
      "Iteration 113, loss = 0.02103984\n",
      "Iteration 114, loss = 0.02080409\n",
      "Iteration 115, loss = 0.02046942\n",
      "Iteration 116, loss = 0.02036072\n",
      "Iteration 117, loss = 0.02021794\n",
      "Iteration 118, loss = 0.02021137\n",
      "Iteration 119, loss = 0.02019993\n",
      "Iteration 120, loss = 0.02013236\n",
      "Iteration 121, loss = 0.01983329\n",
      "Iteration 122, loss = 0.01940152\n",
      "Iteration 123, loss = 0.01951531\n",
      "Iteration 124, loss = 0.01969092\n",
      "Iteration 125, loss = 0.01962311\n",
      "Iteration 126, loss = 0.01938340\n",
      "Iteration 127, loss = 0.01915121\n",
      "Iteration 128, loss = 0.01896982\n",
      "Iteration 129, loss = 0.01871172\n",
      "Iteration 130, loss = 0.01854831\n",
      "Iteration 131, loss = 0.01900479\n",
      "Iteration 132, loss = 0.01911257\n",
      "Iteration 133, loss = 0.01831836\n",
      "Iteration 134, loss = 0.01858969\n",
      "Iteration 135, loss = 0.01845061\n",
      "Iteration 136, loss = 0.01786277\n",
      "Iteration 137, loss = 0.01822729\n",
      "Iteration 138, loss = 0.01879183\n",
      "Iteration 139, loss = 0.01774872\n",
      "Iteration 140, loss = 0.01757933\n",
      "Iteration 141, loss = 0.01889935\n",
      "Iteration 142, loss = 0.01849149\n",
      "Iteration 143, loss = 0.01739646\n",
      "Iteration 144, loss = 0.01733876\n",
      "Iteration 145, loss = 0.01737619\n",
      "Iteration 146, loss = 0.01729092\n",
      "Iteration 147, loss = 0.01709274\n",
      "Iteration 148, loss = 0.01687804\n",
      "Iteration 149, loss = 0.01683178\n",
      "Iteration 150, loss = 0.01685801\n",
      "Iteration 151, loss = 0.01659326\n",
      "Iteration 152, loss = 0.01644483\n",
      "Iteration 153, loss = 0.01647714\n",
      "Iteration 154, loss = 0.01628153\n",
      "Iteration 155, loss = 0.01613065\n",
      "Iteration 156, loss = 0.01620555\n",
      "Iteration 157, loss = 0.01607587\n",
      "Iteration 158, loss = 0.01590880\n",
      "Iteration 159, loss = 0.01606813\n",
      "Iteration 160, loss = 0.01606592\n",
      "Iteration 161, loss = 0.01582887\n",
      "Iteration 162, loss = 0.01570644\n",
      "Iteration 163, loss = 0.01599672\n",
      "Iteration 164, loss = 0.01581298\n",
      "Iteration 165, loss = 0.01549039\n",
      "Iteration 166, loss = 0.01539329\n",
      "Iteration 167, loss = 0.01551809\n",
      "Iteration 168, loss = 0.01564635\n",
      "Iteration 169, loss = 0.01609129\n",
      "Iteration 170, loss = 0.01565867\n",
      "Iteration 171, loss = 0.01588619\n",
      "Iteration 172, loss = 0.01552448\n",
      "Iteration 173, loss = 0.01499205\n",
      "Iteration 174, loss = 0.01508632\n",
      "Iteration 175, loss = 0.01532426\n",
      "Iteration 176, loss = 0.01492335\n",
      "Iteration 177, loss = 0.01449252\n",
      "Iteration 178, loss = 0.01482612\n",
      "Iteration 179, loss = 0.01507288\n",
      "Iteration 180, loss = 0.01497091\n",
      "Iteration 181, loss = 0.01445480\n",
      "Iteration 182, loss = 0.01466428\n",
      "Iteration 183, loss = 0.01491721\n",
      "Iteration 184, loss = 0.01456705\n",
      "Iteration 185, loss = 0.01436334\n",
      "Iteration 186, loss = 0.01434654\n",
      "Iteration 187, loss = 0.01421495\n",
      "Iteration 188, loss = 0.01403716\n",
      "Iteration 189, loss = 0.01413206\n",
      "Iteration 190, loss = 0.01416981\n",
      "Iteration 191, loss = 0.01395786\n",
      "Iteration 192, loss = 0.01372863\n",
      "Iteration 193, loss = 0.01366540\n",
      "Iteration 194, loss = 0.01360524\n",
      "Iteration 195, loss = 0.01351209\n",
      "Iteration 196, loss = 0.01351376\n",
      "Iteration 197, loss = 0.01347515\n",
      "Iteration 198, loss = 0.01353842\n",
      "Iteration 199, loss = 0.01343990\n",
      "Iteration 200, loss = 0.01334369\n",
      "Iteration 201, loss = 0.01339390\n",
      "Iteration 202, loss = 0.01335115\n",
      "Iteration 203, loss = 0.01328207\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51556659\n",
      "Iteration 2, loss = 0.44817999\n",
      "Iteration 3, loss = 0.39029337\n",
      "Iteration 4, loss = 0.34409820\n",
      "Iteration 5, loss = 0.30531585\n",
      "Iteration 6, loss = 0.27237018\n",
      "Iteration 7, loss = 0.24511378\n",
      "Iteration 8, loss = 0.22213900\n",
      "Iteration 9, loss = 0.20596676\n",
      "Iteration 10, loss = 0.19041943\n",
      "Iteration 11, loss = 0.17947129\n",
      "Iteration 12, loss = 0.16751617\n",
      "Iteration 13, loss = 0.15766137\n",
      "Iteration 14, loss = 0.14806763\n",
      "Iteration 15, loss = 0.13860775\n",
      "Iteration 16, loss = 0.13042332\n",
      "Iteration 17, loss = 0.12279082\n",
      "Iteration 18, loss = 0.11515034\n",
      "Iteration 19, loss = 0.10908599\n",
      "Iteration 20, loss = 0.10364131\n",
      "Iteration 21, loss = 0.09890879\n",
      "Iteration 22, loss = 0.09363769\n",
      "Iteration 23, loss = 0.08870948\n",
      "Iteration 24, loss = 0.08388513\n",
      "Iteration 25, loss = 0.07997548\n",
      "Iteration 26, loss = 0.07568418\n",
      "Iteration 27, loss = 0.07251007\n",
      "Iteration 28, loss = 0.06951674\n",
      "Iteration 29, loss = 0.06717370\n",
      "Iteration 30, loss = 0.06505287\n",
      "Iteration 31, loss = 0.06296371\n",
      "Iteration 32, loss = 0.06092196\n",
      "Iteration 33, loss = 0.05932387\n",
      "Iteration 34, loss = 0.05769004\n",
      "Iteration 35, loss = 0.05615698\n",
      "Iteration 36, loss = 0.05480815\n",
      "Iteration 37, loss = 0.05350275\n",
      "Iteration 38, loss = 0.05229238\n",
      "Iteration 39, loss = 0.05102621\n",
      "Iteration 40, loss = 0.04985443\n",
      "Iteration 41, loss = 0.04880103\n",
      "Iteration 42, loss = 0.04778362\n",
      "Iteration 43, loss = 0.04675387\n",
      "Iteration 44, loss = 0.04577378\n",
      "Iteration 45, loss = 0.04479436\n",
      "Iteration 46, loss = 0.04399634\n",
      "Iteration 47, loss = 0.04313643\n",
      "Iteration 48, loss = 0.04219363\n",
      "Iteration 49, loss = 0.04143736\n",
      "Iteration 50, loss = 0.04054657\n",
      "Iteration 51, loss = 0.03979233\n",
      "Iteration 52, loss = 0.03904431\n",
      "Iteration 53, loss = 0.03846540\n",
      "Iteration 54, loss = 0.03770472\n",
      "Iteration 55, loss = 0.03685611\n",
      "Iteration 56, loss = 0.03612964\n",
      "Iteration 57, loss = 0.03551326\n",
      "Iteration 58, loss = 0.03479409\n",
      "Iteration 59, loss = 0.03422761\n",
      "Iteration 60, loss = 0.03357417\n",
      "Iteration 61, loss = 0.03299107\n",
      "Iteration 62, loss = 0.03248150\n",
      "Iteration 63, loss = 0.03180977\n",
      "Iteration 64, loss = 0.03135942\n",
      "Iteration 65, loss = 0.03082973\n",
      "Iteration 66, loss = 0.03036933\n",
      "Iteration 67, loss = 0.02977609\n",
      "Iteration 68, loss = 0.02928319\n",
      "Iteration 69, loss = 0.02870666\n",
      "Iteration 70, loss = 0.02821743\n",
      "Iteration 71, loss = 0.02781389\n",
      "Iteration 72, loss = 0.02735581\n",
      "Iteration 73, loss = 0.02687813\n",
      "Iteration 74, loss = 0.02652350\n",
      "Iteration 75, loss = 0.02605111\n",
      "Iteration 76, loss = 0.02567096\n",
      "Iteration 77, loss = 0.02519565\n",
      "Iteration 78, loss = 0.02490204\n",
      "Iteration 79, loss = 0.02440321\n",
      "Iteration 80, loss = 0.02409550\n",
      "Iteration 81, loss = 0.02376613\n",
      "Iteration 82, loss = 0.02342106\n",
      "Iteration 83, loss = 0.02298518\n",
      "Iteration 84, loss = 0.02282823\n",
      "Iteration 85, loss = 0.02231925\n",
      "Iteration 86, loss = 0.02206565\n",
      "Iteration 87, loss = 0.02165800\n",
      "Iteration 88, loss = 0.02137842\n",
      "Iteration 89, loss = 0.02107215\n",
      "Iteration 90, loss = 0.02086459\n",
      "Iteration 91, loss = 0.02047746\n",
      "Iteration 92, loss = 0.02024621\n",
      "Iteration 93, loss = 0.01997605\n",
      "Iteration 94, loss = 0.01978677\n",
      "Iteration 95, loss = 0.01961353\n",
      "Iteration 96, loss = 0.01931586\n",
      "Iteration 97, loss = 0.01898876\n",
      "Iteration 98, loss = 0.01870623\n",
      "Iteration 99, loss = 0.01852368\n",
      "Iteration 100, loss = 0.01834490\n",
      "Iteration 101, loss = 0.01815942\n",
      "Iteration 102, loss = 0.01783890\n",
      "Iteration 103, loss = 0.01767194\n",
      "Iteration 104, loss = 0.01742851\n",
      "Iteration 105, loss = 0.01725978\n",
      "Iteration 106, loss = 0.01709566\n",
      "Iteration 107, loss = 0.01684319\n",
      "Iteration 108, loss = 0.01668652\n",
      "Iteration 109, loss = 0.01648677\n",
      "Iteration 110, loss = 0.01640858\n",
      "Iteration 111, loss = 0.01609164\n",
      "Iteration 112, loss = 0.01606061\n",
      "Iteration 113, loss = 0.01591007\n",
      "Iteration 114, loss = 0.01563708\n",
      "Iteration 115, loss = 0.01549996\n",
      "Iteration 116, loss = 0.01540191\n",
      "Iteration 117, loss = 0.01519859\n",
      "Iteration 118, loss = 0.01508032\n",
      "Iteration 119, loss = 0.01489573\n",
      "Iteration 120, loss = 0.01484101\n",
      "Iteration 121, loss = 0.01468244\n",
      "Iteration 122, loss = 0.01450453\n",
      "Iteration 123, loss = 0.01442395\n",
      "Iteration 124, loss = 0.01429112\n",
      "Iteration 125, loss = 0.01421960\n",
      "Iteration 126, loss = 0.01399691\n",
      "Iteration 127, loss = 0.01376688\n",
      "Iteration 128, loss = 0.01368434\n",
      "Iteration 129, loss = 0.01359479\n",
      "Iteration 130, loss = 0.01339500\n",
      "Iteration 131, loss = 0.01340038\n",
      "Iteration 132, loss = 0.01325806\n",
      "Iteration 133, loss = 0.01306315\n",
      "Iteration 134, loss = 0.01301029\n",
      "Iteration 135, loss = 0.01282166\n",
      "Iteration 136, loss = 0.01274899\n",
      "Iteration 137, loss = 0.01259627\n",
      "Iteration 138, loss = 0.01245936\n",
      "Iteration 139, loss = 0.01242227\n",
      "Iteration 140, loss = 0.01224396\n",
      "Iteration 141, loss = 0.01227009\n",
      "Iteration 142, loss = 0.01198443\n",
      "Iteration 143, loss = 0.01194874\n",
      "Iteration 144, loss = 0.01182333\n",
      "Iteration 145, loss = 0.01168041\n",
      "Iteration 146, loss = 0.01156478\n",
      "Iteration 147, loss = 0.01151508\n",
      "Iteration 148, loss = 0.01140835\n",
      "Iteration 149, loss = 0.01125099\n",
      "Iteration 150, loss = 0.01125147\n",
      "Iteration 151, loss = 0.01123149\n",
      "Iteration 152, loss = 0.01100134\n",
      "Iteration 153, loss = 0.01087021\n",
      "Iteration 154, loss = 0.01081811\n",
      "Iteration 155, loss = 0.01077146\n",
      "Iteration 156, loss = 0.01060924\n",
      "Iteration 157, loss = 0.01052923\n",
      "Iteration 158, loss = 0.01046992\n",
      "Iteration 159, loss = 0.01037309\n",
      "Iteration 160, loss = 0.01037224\n",
      "Iteration 161, loss = 0.01016598\n",
      "Iteration 162, loss = 0.01024066\n",
      "Iteration 163, loss = 0.01005653\n",
      "Iteration 164, loss = 0.01010319\n",
      "Iteration 165, loss = 0.00995200\n",
      "Iteration 166, loss = 0.00994425\n",
      "Iteration 167, loss = 0.00984887\n",
      "Iteration 168, loss = 0.00971036\n",
      "Iteration 169, loss = 0.00965695\n",
      "Iteration 170, loss = 0.00958068\n",
      "Iteration 171, loss = 0.00950545\n",
      "Iteration 172, loss = 0.00941906\n",
      "Iteration 173, loss = 0.00935455\n",
      "Iteration 174, loss = 0.00929578\n",
      "Iteration 175, loss = 0.00921770\n",
      "Iteration 176, loss = 0.00922560\n",
      "Iteration 177, loss = 0.00907043\n",
      "Iteration 178, loss = 0.00905549\n",
      "Iteration 179, loss = 0.00892010\n",
      "Iteration 180, loss = 0.00888504\n",
      "Iteration 181, loss = 0.00886618\n",
      "Iteration 182, loss = 0.00875611\n",
      "Iteration 183, loss = 0.00865924\n",
      "Iteration 184, loss = 0.00861326\n",
      "Iteration 185, loss = 0.00857411\n",
      "Iteration 186, loss = 0.00851498\n",
      "Iteration 187, loss = 0.00848269\n",
      "Iteration 188, loss = 0.00837595\n",
      "Iteration 189, loss = 0.00830502\n",
      "Iteration 190, loss = 0.00834774\n",
      "Iteration 191, loss = 0.00821146\n",
      "Iteration 192, loss = 0.00816716\n",
      "Iteration 193, loss = 0.00810232\n",
      "Iteration 194, loss = 0.00801269\n",
      "Iteration 195, loss = 0.00800203\n",
      "Iteration 196, loss = 0.00792818\n",
      "Iteration 197, loss = 0.00782164\n",
      "Iteration 198, loss = 0.00778250\n",
      "Iteration 199, loss = 0.00773247\n",
      "Iteration 200, loss = 0.00769972\n",
      "Iteration 201, loss = 0.00773141\n",
      "Iteration 202, loss = 0.00762154\n",
      "Iteration 203, loss = 0.00755571\n",
      "Iteration 204, loss = 0.00755872\n",
      "Iteration 205, loss = 0.00746445\n",
      "Iteration 206, loss = 0.00745076\n",
      "Iteration 207, loss = 0.00747445\n",
      "Iteration 208, loss = 0.00731810\n",
      "Iteration 209, loss = 0.00727788\n",
      "Iteration 210, loss = 0.00723559\n",
      "Iteration 211, loss = 0.00728459\n",
      "Iteration 212, loss = 0.00715273\n",
      "Iteration 213, loss = 0.00715583\n",
      "Iteration 214, loss = 0.00700143\n",
      "Iteration 215, loss = 0.00702019\n",
      "Iteration 216, loss = 0.00706463\n",
      "Iteration 217, loss = 0.00696770\n",
      "Iteration 218, loss = 0.00695653\n",
      "Iteration 219, loss = 0.00685519\n",
      "Iteration 220, loss = 0.00681657\n",
      "Iteration 221, loss = 0.00674160\n",
      "Iteration 222, loss = 0.00674575\n",
      "Iteration 223, loss = 0.00667717\n",
      "Iteration 224, loss = 0.00668483\n",
      "Iteration 225, loss = 0.00663946\n",
      "Iteration 226, loss = 0.00662310\n",
      "Iteration 227, loss = 0.00652021\n",
      "Iteration 228, loss = 0.00645662\n",
      "Iteration 229, loss = 0.00645622\n",
      "Iteration 230, loss = 0.00639945\n",
      "Iteration 231, loss = 0.00650955\n",
      "Iteration 232, loss = 0.00628308\n",
      "Iteration 233, loss = 0.00634257\n",
      "Iteration 234, loss = 0.00634472\n",
      "Iteration 235, loss = 0.00625963\n",
      "Iteration 236, loss = 0.00625797\n",
      "Iteration 237, loss = 0.00619026\n",
      "Iteration 238, loss = 0.00618240\n",
      "Iteration 239, loss = 0.00615102\n",
      "Iteration 240, loss = 0.00611513\n",
      "Iteration 241, loss = 0.00607734\n",
      "Iteration 242, loss = 0.00602775\n",
      "Iteration 243, loss = 0.00605562\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51556659\n",
      "Iteration 2, loss = 0.44817999\n",
      "Iteration 3, loss = 0.39029337\n",
      "Iteration 4, loss = 0.34409820\n",
      "Iteration 5, loss = 0.30531585\n",
      "Iteration 6, loss = 0.27237018\n",
      "Iteration 7, loss = 0.24511378\n",
      "Iteration 8, loss = 0.22213900\n",
      "Iteration 9, loss = 0.20596676\n",
      "Iteration 10, loss = 0.19041943\n",
      "Iteration 11, loss = 0.17947129\n",
      "Iteration 12, loss = 0.16751617\n",
      "Iteration 13, loss = 0.15766137\n",
      "Iteration 14, loss = 0.14806763\n",
      "Iteration 15, loss = 0.13860775\n",
      "Iteration 16, loss = 0.13042332\n",
      "Iteration 17, loss = 0.12279082\n",
      "Iteration 18, loss = 0.11515034\n",
      "Iteration 19, loss = 0.10908599\n",
      "Iteration 20, loss = 0.10364131\n",
      "Iteration 21, loss = 0.09890879\n",
      "Iteration 22, loss = 0.09363769\n",
      "Iteration 23, loss = 0.08870948\n",
      "Iteration 24, loss = 0.08388513\n",
      "Iteration 25, loss = 0.07997548\n",
      "Iteration 26, loss = 0.07568418\n",
      "Iteration 27, loss = 0.07251007\n",
      "Iteration 28, loss = 0.06951674\n",
      "Iteration 29, loss = 0.06717370\n",
      "Iteration 30, loss = 0.06505287\n",
      "Iteration 31, loss = 0.06296371\n",
      "Iteration 32, loss = 0.06092196\n",
      "Iteration 33, loss = 0.05932387\n",
      "Iteration 34, loss = 0.05769004\n",
      "Iteration 35, loss = 0.05615698\n",
      "Iteration 36, loss = 0.05480815\n",
      "Iteration 37, loss = 0.05350275\n",
      "Iteration 38, loss = 0.05229238\n",
      "Iteration 39, loss = 0.05102621\n",
      "Iteration 40, loss = 0.04985443\n",
      "Iteration 41, loss = 0.04880103\n",
      "Iteration 42, loss = 0.04778362\n",
      "Iteration 43, loss = 0.04675387\n",
      "Iteration 44, loss = 0.04577378\n",
      "Iteration 45, loss = 0.04479436\n",
      "Iteration 46, loss = 0.04399634\n",
      "Iteration 47, loss = 0.04313643\n",
      "Iteration 48, loss = 0.04219363\n",
      "Iteration 49, loss = 0.04143736\n",
      "Iteration 50, loss = 0.04054657\n",
      "Iteration 51, loss = 0.03979233\n",
      "Iteration 52, loss = 0.03904431\n",
      "Iteration 53, loss = 0.03846540\n",
      "Iteration 54, loss = 0.03770472\n",
      "Iteration 55, loss = 0.03685611\n",
      "Iteration 56, loss = 0.03612964\n",
      "Iteration 57, loss = 0.03551326\n",
      "Iteration 58, loss = 0.03479409\n",
      "Iteration 59, loss = 0.03422761\n",
      "Iteration 60, loss = 0.03357417\n",
      "Iteration 61, loss = 0.03299107\n",
      "Iteration 62, loss = 0.03248150\n",
      "Iteration 63, loss = 0.03180977\n",
      "Iteration 64, loss = 0.03135942\n",
      "Iteration 65, loss = 0.03082973\n",
      "Iteration 66, loss = 0.03036933\n",
      "Iteration 67, loss = 0.02977609\n",
      "Iteration 68, loss = 0.02928319\n",
      "Iteration 69, loss = 0.02870666\n",
      "Iteration 70, loss = 0.02821743\n",
      "Iteration 71, loss = 0.02781389\n",
      "Iteration 72, loss = 0.02735581\n",
      "Iteration 73, loss = 0.02687813\n",
      "Iteration 74, loss = 0.02652350\n",
      "Iteration 75, loss = 0.02605111\n",
      "Iteration 76, loss = 0.02567096\n",
      "Iteration 77, loss = 0.02519565\n",
      "Iteration 78, loss = 0.02490204\n",
      "Iteration 79, loss = 0.02440321\n",
      "Iteration 80, loss = 0.02409550\n",
      "Iteration 81, loss = 0.02376613\n",
      "Iteration 82, loss = 0.02342106\n",
      "Iteration 83, loss = 0.02298518\n",
      "Iteration 84, loss = 0.02282823\n",
      "Iteration 85, loss = 0.02231925\n",
      "Iteration 86, loss = 0.02206565\n",
      "Iteration 87, loss = 0.02165800\n",
      "Iteration 88, loss = 0.02137842\n",
      "Iteration 89, loss = 0.02107215\n",
      "Iteration 90, loss = 0.02086459\n",
      "Iteration 91, loss = 0.02047746\n",
      "Iteration 92, loss = 0.02024621\n",
      "Iteration 93, loss = 0.01997605\n",
      "Iteration 94, loss = 0.01978677\n",
      "Iteration 95, loss = 0.01961353\n",
      "Iteration 96, loss = 0.01931586\n",
      "Iteration 97, loss = 0.01898876\n",
      "Iteration 98, loss = 0.01870623\n",
      "Iteration 99, loss = 0.01852368\n",
      "Iteration 100, loss = 0.01834490\n",
      "Iteration 101, loss = 0.01815942\n",
      "Iteration 102, loss = 0.01783890\n",
      "Iteration 103, loss = 0.01767194\n",
      "Iteration 104, loss = 0.01742851\n",
      "Iteration 105, loss = 0.01725978\n",
      "Iteration 106, loss = 0.01709566\n",
      "Iteration 107, loss = 0.01684319\n",
      "Iteration 108, loss = 0.01668652\n",
      "Iteration 109, loss = 0.01648677\n",
      "Iteration 110, loss = 0.01640858\n",
      "Iteration 111, loss = 0.01609164\n",
      "Iteration 112, loss = 0.01606061\n",
      "Iteration 113, loss = 0.01591007\n",
      "Iteration 114, loss = 0.01563708\n",
      "Iteration 115, loss = 0.01549996\n",
      "Iteration 116, loss = 0.01540191\n",
      "Iteration 117, loss = 0.01519859\n",
      "Iteration 118, loss = 0.01508032\n",
      "Iteration 119, loss = 0.01489573\n",
      "Iteration 120, loss = 0.01484101\n",
      "Iteration 121, loss = 0.01468244\n",
      "Iteration 122, loss = 0.01450453\n",
      "Iteration 123, loss = 0.01442395\n",
      "Iteration 124, loss = 0.01429112\n",
      "Iteration 125, loss = 0.01421960\n",
      "Iteration 126, loss = 0.01399691\n",
      "Iteration 127, loss = 0.01376688\n",
      "Iteration 128, loss = 0.01368434\n",
      "Iteration 129, loss = 0.01359479\n",
      "Iteration 130, loss = 0.01339500\n",
      "Iteration 131, loss = 0.01340038\n",
      "Iteration 132, loss = 0.01325806\n",
      "Iteration 133, loss = 0.01306315\n",
      "Iteration 134, loss = 0.01301029\n",
      "Iteration 135, loss = 0.01282166\n",
      "Iteration 136, loss = 0.01274899\n",
      "Iteration 137, loss = 0.01259627\n",
      "Iteration 138, loss = 0.01245936\n",
      "Iteration 139, loss = 0.01242227\n",
      "Iteration 140, loss = 0.01224396\n",
      "Iteration 141, loss = 0.01227009\n",
      "Iteration 142, loss = 0.01198443\n",
      "Iteration 143, loss = 0.01194874\n",
      "Iteration 144, loss = 0.01182333\n",
      "Iteration 145, loss = 0.01168041\n",
      "Iteration 146, loss = 0.01156478\n",
      "Iteration 147, loss = 0.01151508\n",
      "Iteration 148, loss = 0.01140835\n",
      "Iteration 149, loss = 0.01125099\n",
      "Iteration 150, loss = 0.01125147\n",
      "Iteration 151, loss = 0.01123149\n",
      "Iteration 152, loss = 0.01100134\n",
      "Iteration 153, loss = 0.01087021\n",
      "Iteration 154, loss = 0.01081811\n",
      "Iteration 155, loss = 0.01077146\n",
      "Iteration 156, loss = 0.01060924\n",
      "Iteration 157, loss = 0.01052923\n",
      "Iteration 158, loss = 0.01046992\n",
      "Iteration 159, loss = 0.01037309\n",
      "Iteration 160, loss = 0.01037224\n",
      "Iteration 161, loss = 0.01016598\n",
      "Iteration 162, loss = 0.01024066\n",
      "Iteration 163, loss = 0.01005653\n",
      "Iteration 164, loss = 0.01010319\n",
      "Iteration 165, loss = 0.00995200\n",
      "Iteration 166, loss = 0.00994425\n",
      "Iteration 167, loss = 0.00984887\n",
      "Iteration 168, loss = 0.00971036\n",
      "Iteration 169, loss = 0.00965695\n",
      "Iteration 170, loss = 0.00958068\n",
      "Iteration 171, loss = 0.00950545\n",
      "Iteration 172, loss = 0.00941906\n",
      "Iteration 173, loss = 0.00935455\n",
      "Iteration 174, loss = 0.00929578\n",
      "Iteration 175, loss = 0.00921770\n",
      "Iteration 176, loss = 0.00922560\n",
      "Iteration 177, loss = 0.00907043\n",
      "Iteration 178, loss = 0.00905549\n",
      "Iteration 179, loss = 0.00892010\n",
      "Iteration 180, loss = 0.00888504\n",
      "Iteration 181, loss = 0.00886618\n",
      "Iteration 182, loss = 0.00875611\n",
      "Iteration 183, loss = 0.00865924\n",
      "Iteration 184, loss = 0.00861326\n",
      "Iteration 185, loss = 0.00857411\n",
      "Iteration 186, loss = 0.00851498\n",
      "Iteration 187, loss = 0.00848269\n",
      "Iteration 188, loss = 0.00837595\n",
      "Iteration 189, loss = 0.00830502\n",
      "Iteration 190, loss = 0.00834774\n",
      "Iteration 191, loss = 0.00821146\n",
      "Iteration 192, loss = 0.00816716\n",
      "Iteration 193, loss = 0.00810232\n",
      "Iteration 194, loss = 0.00801269\n",
      "Iteration 195, loss = 0.00800203\n",
      "Iteration 196, loss = 0.00792818\n",
      "Iteration 197, loss = 0.00782164\n",
      "Iteration 198, loss = 0.00778250\n",
      "Iteration 199, loss = 0.00773247\n",
      "Iteration 200, loss = 0.00769972\n",
      "Iteration 201, loss = 0.00773141\n",
      "Iteration 202, loss = 0.00762154\n",
      "Iteration 203, loss = 0.00755571\n",
      "Iteration 204, loss = 0.00755872\n",
      "Iteration 205, loss = 0.00746445\n",
      "Iteration 206, loss = 0.00745076\n",
      "Iteration 207, loss = 0.00747445\n",
      "Iteration 208, loss = 0.00731810\n",
      "Iteration 209, loss = 0.00727788\n",
      "Iteration 210, loss = 0.00723559\n",
      "Iteration 211, loss = 0.00728459\n",
      "Iteration 212, loss = 0.00715273\n",
      "Iteration 213, loss = 0.00715583\n",
      "Iteration 214, loss = 0.00700143\n",
      "Iteration 215, loss = 0.00702019\n",
      "Iteration 216, loss = 0.00706463\n",
      "Iteration 217, loss = 0.00696770\n",
      "Iteration 218, loss = 0.00695653\n",
      "Iteration 219, loss = 0.00685519\n",
      "Iteration 220, loss = 0.00681657\n",
      "Iteration 221, loss = 0.00674160\n",
      "Iteration 222, loss = 0.00674575\n",
      "Iteration 223, loss = 0.00667717\n",
      "Iteration 224, loss = 0.00668483\n",
      "Iteration 225, loss = 0.00663946\n",
      "Iteration 226, loss = 0.00662310\n",
      "Iteration 227, loss = 0.00652021\n",
      "Iteration 228, loss = 0.00645662\n",
      "Iteration 229, loss = 0.00645622\n",
      "Iteration 230, loss = 0.00639945\n",
      "Iteration 231, loss = 0.00650955\n",
      "Iteration 232, loss = 0.00628308\n",
      "Iteration 233, loss = 0.00634257\n",
      "Iteration 234, loss = 0.00634472\n",
      "Iteration 235, loss = 0.00625963\n",
      "Iteration 236, loss = 0.00625797\n",
      "Iteration 237, loss = 0.00619026\n",
      "Iteration 238, loss = 0.00618240\n",
      "Iteration 239, loss = 0.00615102\n",
      "Iteration 240, loss = 0.00611513\n",
      "Iteration 241, loss = 0.00607734\n",
      "Iteration 242, loss = 0.00602775\n",
      "Iteration 243, loss = 0.00605562\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51556659\n",
      "Iteration 2, loss = 0.44817999\n",
      "Iteration 3, loss = 0.39029337\n",
      "Iteration 4, loss = 0.34409820\n",
      "Iteration 5, loss = 0.30531585\n",
      "Iteration 6, loss = 0.27237018\n",
      "Iteration 7, loss = 0.24511378\n",
      "Iteration 8, loss = 0.22213900\n",
      "Iteration 9, loss = 0.20596676\n",
      "Iteration 10, loss = 0.19041943\n",
      "Iteration 11, loss = 0.17947129\n",
      "Iteration 12, loss = 0.16751617\n",
      "Iteration 13, loss = 0.15766137\n",
      "Iteration 14, loss = 0.14806763\n",
      "Iteration 15, loss = 0.13860775\n",
      "Iteration 16, loss = 0.13042332\n",
      "Iteration 17, loss = 0.12279082\n",
      "Iteration 18, loss = 0.11515034\n",
      "Iteration 19, loss = 0.10908599\n",
      "Iteration 20, loss = 0.10364131\n",
      "Iteration 21, loss = 0.09890879\n",
      "Iteration 22, loss = 0.09363769\n",
      "Iteration 23, loss = 0.08870948\n",
      "Iteration 24, loss = 0.08388513\n",
      "Iteration 25, loss = 0.07997548\n",
      "Iteration 26, loss = 0.07568418\n",
      "Iteration 27, loss = 0.07251007\n",
      "Iteration 28, loss = 0.06951674\n",
      "Iteration 29, loss = 0.06717370\n",
      "Iteration 30, loss = 0.06505287\n",
      "Iteration 31, loss = 0.06296371\n",
      "Iteration 32, loss = 0.06092196\n",
      "Iteration 33, loss = 0.05932387\n",
      "Iteration 34, loss = 0.05769004\n",
      "Iteration 35, loss = 0.05615698\n",
      "Iteration 36, loss = 0.05480815\n",
      "Iteration 37, loss = 0.05350275\n",
      "Iteration 38, loss = 0.05229238\n",
      "Iteration 39, loss = 0.05102621\n",
      "Iteration 40, loss = 0.04985443\n",
      "Iteration 41, loss = 0.04880103\n",
      "Iteration 42, loss = 0.04778362\n",
      "Iteration 43, loss = 0.04675387\n",
      "Iteration 44, loss = 0.04577378\n",
      "Iteration 45, loss = 0.04479436\n",
      "Iteration 46, loss = 0.04399634\n",
      "Iteration 47, loss = 0.04313643\n",
      "Iteration 48, loss = 0.04219363\n",
      "Iteration 49, loss = 0.04143736\n",
      "Iteration 50, loss = 0.04054657\n",
      "Iteration 51, loss = 0.03979233\n",
      "Iteration 52, loss = 0.03904431\n",
      "Iteration 53, loss = 0.03846540\n",
      "Iteration 54, loss = 0.03770472\n",
      "Iteration 55, loss = 0.03685611\n",
      "Iteration 56, loss = 0.03612964\n",
      "Iteration 57, loss = 0.03551326\n",
      "Iteration 58, loss = 0.03479409\n",
      "Iteration 59, loss = 0.03422761\n",
      "Iteration 60, loss = 0.03357417\n",
      "Iteration 61, loss = 0.03299107\n",
      "Iteration 62, loss = 0.03248150\n",
      "Iteration 63, loss = 0.03180977\n",
      "Iteration 64, loss = 0.03135942\n",
      "Iteration 65, loss = 0.03082973\n",
      "Iteration 66, loss = 0.03036933\n",
      "Iteration 67, loss = 0.02977609\n",
      "Iteration 68, loss = 0.02928319\n",
      "Iteration 69, loss = 0.02870666\n",
      "Iteration 70, loss = 0.02821743\n",
      "Iteration 71, loss = 0.02781389\n",
      "Iteration 72, loss = 0.02735581\n",
      "Iteration 73, loss = 0.02687813\n",
      "Iteration 74, loss = 0.02652350\n",
      "Iteration 75, loss = 0.02605111\n",
      "Iteration 76, loss = 0.02567096\n",
      "Iteration 77, loss = 0.02519565\n",
      "Iteration 78, loss = 0.02490204\n",
      "Iteration 79, loss = 0.02440321\n",
      "Iteration 80, loss = 0.02409550\n",
      "Iteration 81, loss = 0.02376613\n",
      "Iteration 82, loss = 0.02342106\n",
      "Iteration 83, loss = 0.02298518\n",
      "Iteration 84, loss = 0.02282823\n",
      "Iteration 85, loss = 0.02231925\n",
      "Iteration 86, loss = 0.02206565\n",
      "Iteration 87, loss = 0.02165800\n",
      "Iteration 88, loss = 0.02137842\n",
      "Iteration 89, loss = 0.02107215\n",
      "Iteration 90, loss = 0.02086459\n",
      "Iteration 91, loss = 0.02047746\n",
      "Iteration 92, loss = 0.02024621\n",
      "Iteration 93, loss = 0.01997605\n",
      "Iteration 94, loss = 0.01978677\n",
      "Iteration 95, loss = 0.01961353\n",
      "Iteration 96, loss = 0.01931586\n",
      "Iteration 97, loss = 0.01898876\n",
      "Iteration 98, loss = 0.01870623\n",
      "Iteration 99, loss = 0.01852368\n",
      "Iteration 100, loss = 0.01834490\n",
      "Iteration 101, loss = 0.01815942\n",
      "Iteration 102, loss = 0.01783890\n",
      "Iteration 103, loss = 0.01767194\n",
      "Iteration 104, loss = 0.01742851\n",
      "Iteration 105, loss = 0.01725978\n",
      "Iteration 106, loss = 0.01709566\n",
      "Iteration 107, loss = 0.01684319\n",
      "Iteration 108, loss = 0.01668652\n",
      "Iteration 109, loss = 0.01648677\n",
      "Iteration 110, loss = 0.01640858\n",
      "Iteration 111, loss = 0.01609164\n",
      "Iteration 112, loss = 0.01606061\n",
      "Iteration 113, loss = 0.01591007\n",
      "Iteration 114, loss = 0.01563708\n",
      "Iteration 115, loss = 0.01549996\n",
      "Iteration 116, loss = 0.01540191\n",
      "Iteration 117, loss = 0.01519859\n",
      "Iteration 118, loss = 0.01508032\n",
      "Iteration 119, loss = 0.01489573\n",
      "Iteration 120, loss = 0.01484101\n",
      "Iteration 121, loss = 0.01468244\n",
      "Iteration 122, loss = 0.01450453\n",
      "Iteration 123, loss = 0.01442395\n",
      "Iteration 124, loss = 0.01429112\n",
      "Iteration 125, loss = 0.01421960\n",
      "Iteration 126, loss = 0.01399691\n",
      "Iteration 127, loss = 0.01376688\n",
      "Iteration 128, loss = 0.01368434\n",
      "Iteration 129, loss = 0.01359479\n",
      "Iteration 130, loss = 0.01339500\n",
      "Iteration 131, loss = 0.01340038\n",
      "Iteration 132, loss = 0.01325806\n",
      "Iteration 133, loss = 0.01306315\n",
      "Iteration 134, loss = 0.01301029\n",
      "Iteration 135, loss = 0.01282166\n",
      "Iteration 136, loss = 0.01274899\n",
      "Iteration 137, loss = 0.01259627\n",
      "Iteration 138, loss = 0.01245936\n",
      "Iteration 139, loss = 0.01242227\n",
      "Iteration 140, loss = 0.01224396\n",
      "Iteration 141, loss = 0.01227009\n",
      "Iteration 142, loss = 0.01198443\n",
      "Iteration 143, loss = 0.01194874\n",
      "Iteration 144, loss = 0.01182333\n",
      "Iteration 145, loss = 0.01168041\n",
      "Iteration 146, loss = 0.01156478\n",
      "Iteration 147, loss = 0.01151508\n",
      "LinearRegression\n",
      "NeuralNetwork\n",
      "Iteration 148, loss = 0.01140835\n",
      "Iteration 149, loss = 0.01125099\n",
      "Iteration 150, loss = 0.01125147\n",
      "Iteration 151, loss = 0.01123149\n",
      "Iteration 152, loss = 0.01100134\n",
      "Iteration 153, loss = 0.01087021\n",
      "Iteration 154, loss = 0.01081811\n",
      "Iteration 155, loss = 0.01077146\n",
      "Iteration 156, loss = 0.01060924\n",
      "Iteration 157, loss = 0.01052923\n",
      "Iteration 158, loss = 0.01046992\n",
      "Iteration 159, loss = 0.01037309\n",
      "Iteration 160, loss = 0.01037224\n",
      "Iteration 161, loss = 0.01016598\n",
      "Iteration 162, loss = 0.01024066\n",
      "Iteration 163, loss = 0.01005653\n",
      "Iteration 164, loss = 0.01010319\n",
      "Iteration 165, loss = 0.00995200\n",
      "Iteration 166, loss = 0.00994425\n",
      "Iteration 167, loss = 0.00984887\n",
      "Iteration 168, loss = 0.00971036\n",
      "Iteration 169, loss = 0.00965695\n",
      "Iteration 170, loss = 0.00958068\n",
      "Iteration 171, loss = 0.00950545\n",
      "Iteration 172, loss = 0.00941906\n",
      "Iteration 173, loss = 0.00935455\n",
      "Iteration 174, loss = 0.00929578\n",
      "Iteration 175, loss = 0.00921770\n",
      "Iteration 176, loss = 0.00922560\n",
      "Iteration 177, loss = 0.00907043\n",
      "Iteration 178, loss = 0.00905549\n",
      "Iteration 179, loss = 0.00892010\n",
      "Iteration 180, loss = 0.00888504\n",
      "Iteration 181, loss = 0.00886618\n",
      "Iteration 182, loss = 0.00875611\n",
      "Iteration 183, loss = 0.00865924\n",
      "Iteration 184, loss = 0.00861326\n",
      "Iteration 185, loss = 0.00857411\n",
      "Iteration 186, loss = 0.00851498\n",
      "Iteration 187, loss = 0.00848269\n",
      "Iteration 188, loss = 0.00837595\n",
      "Iteration 189, loss = 0.00830502\n",
      "Iteration 190, loss = 0.00834774\n",
      "Iteration 191, loss = 0.00821146\n",
      "Iteration 192, loss = 0.00816716\n",
      "Iteration 193, loss = 0.00810232\n",
      "Iteration 194, loss = 0.00801269\n",
      "Iteration 195, loss = 0.00800203\n",
      "Iteration 196, loss = 0.00792818\n",
      "Iteration 197, loss = 0.00782164\n",
      "Iteration 198, loss = 0.00778250\n",
      "Iteration 199, loss = 0.00773247\n",
      "Iteration 200, loss = 0.00769972\n",
      "Iteration 201, loss = 0.00773141\n",
      "Iteration 202, loss = 0.00762154\n",
      "Iteration 203, loss = 0.00755571\n",
      "Iteration 204, loss = 0.00755872\n",
      "Iteration 205, loss = 0.00746445\n",
      "Iteration 206, loss = 0.00745076\n",
      "Iteration 207, loss = 0.00747445\n",
      "Iteration 208, loss = 0.00731810\n",
      "Iteration 209, loss = 0.00727788\n",
      "Iteration 210, loss = 0.00723559\n",
      "Iteration 211, loss = 0.00728459\n",
      "Iteration 212, loss = 0.00715273\n",
      "Iteration 213, loss = 0.00715583\n",
      "Iteration 214, loss = 0.00700143\n",
      "Iteration 215, loss = 0.00702019\n",
      "Iteration 216, loss = 0.00706463\n",
      "Iteration 217, loss = 0.00696770\n",
      "Iteration 218, loss = 0.00695653\n",
      "Iteration 219, loss = 0.00685519\n",
      "Iteration 220, loss = 0.00681657\n",
      "Iteration 221, loss = 0.00674160\n",
      "Iteration 222, loss = 0.00674575\n",
      "Iteration 223, loss = 0.00667717\n",
      "Iteration 224, loss = 0.00668483\n",
      "Iteration 225, loss = 0.00663946\n",
      "Iteration 226, loss = 0.00662310\n",
      "Iteration 227, loss = 0.00652021\n",
      "Iteration 228, loss = 0.00645662\n",
      "Iteration 229, loss = 0.00645622\n",
      "Iteration 230, loss = 0.00639945\n",
      "Iteration 231, loss = 0.00650955\n",
      "Iteration 232, loss = 0.00628308\n",
      "Iteration 233, loss = 0.00634257\n",
      "Iteration 234, loss = 0.00634472\n",
      "Iteration 235, loss = 0.00625963\n",
      "Iteration 236, loss = 0.00625797\n",
      "Iteration 237, loss = 0.00619026\n",
      "Iteration 238, loss = 0.00618240\n",
      "Iteration 239, loss = 0.00615102\n",
      "Iteration 240, loss = 0.00611513\n",
      "Iteration 241, loss = 0.00607734\n",
      "Iteration 242, loss = 0.00602775\n",
      "Iteration 243, loss = 0.00605562\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.50467804\n",
      "Iteration 2, loss = 0.44527974\n",
      "Iteration 3, loss = 0.38938013\n",
      "Iteration 4, loss = 0.34304221\n",
      "Iteration 5, loss = 0.30258171\n",
      "Iteration 6, loss = 0.27098466\n",
      "Iteration 7, loss = 0.24659348\n",
      "Iteration 8, loss = 0.22366588\n",
      "Iteration 9, loss = 0.20550789\n",
      "Iteration 10, loss = 0.19088796\n",
      "Iteration 11, loss = 0.17898061\n",
      "Iteration 12, loss = 0.16761214\n",
      "Iteration 13, loss = 0.15981754\n",
      "Iteration 14, loss = 0.14979675\n",
      "Iteration 15, loss = 0.14176500\n",
      "Iteration 16, loss = 0.13461794\n",
      "Iteration 17, loss = 0.12764167\n",
      "Iteration 18, loss = 0.12154637\n",
      "Iteration 19, loss = 0.11586971\n",
      "Iteration 20, loss = 0.11134555\n",
      "Iteration 21, loss = 0.10591356\n",
      "Iteration 22, loss = 0.10141388\n",
      "Iteration 23, loss = 0.09683521\n",
      "Iteration 24, loss = 0.09185671\n",
      "Iteration 25, loss = 0.08769609\n",
      "Iteration 26, loss = 0.08388884\n",
      "Iteration 27, loss = 0.08082246\n",
      "Iteration 28, loss = 0.07812952\n",
      "Iteration 29, loss = 0.07539350\n",
      "Iteration 30, loss = 0.07330925\n",
      "Iteration 31, loss = 0.07125730\n",
      "Iteration 32, loss = 0.06942806\n",
      "Iteration 33, loss = 0.06767196\n",
      "Iteration 34, loss = 0.06594720\n",
      "Iteration 35, loss = 0.06454731\n",
      "Iteration 36, loss = 0.06309678\n",
      "Iteration 37, loss = 0.06172796\n",
      "Iteration 38, loss = 0.06072243\n",
      "Iteration 39, loss = 0.05939271\n",
      "Iteration 40, loss = 0.05821568\n",
      "Iteration 41, loss = 0.05720882\n",
      "Iteration 42, loss = 0.05626213\n",
      "Iteration 43, loss = 0.05536883\n",
      "Iteration 44, loss = 0.05430171\n",
      "Iteration 45, loss = 0.05336965\n",
      "Iteration 46, loss = 0.05246419\n",
      "Iteration 47, loss = 0.05168302\n",
      "Iteration 48, loss = 0.05090088\n",
      "Iteration 49, loss = 0.05016543\n",
      "Iteration 50, loss = 0.04943856\n",
      "Iteration 51, loss = 0.04867026\n",
      "Iteration 52, loss = 0.04794822\n",
      "Iteration 53, loss = 0.04730791\n",
      "Iteration 54, loss = 0.04667276\n",
      "Iteration 55, loss = 0.04601793\n",
      "Iteration 56, loss = 0.04541146\n",
      "Iteration 57, loss = 0.04487536\n",
      "Iteration 58, loss = 0.04418485\n",
      "Iteration 59, loss = 0.04359120\n",
      "Iteration 60, loss = 0.04304607\n",
      "Iteration 61, loss = 0.04249100\n",
      "Iteration 62, loss = 0.04206799\n",
      "Iteration 63, loss = 0.04138942\n",
      "Iteration 64, loss = 0.04085790\n",
      "Iteration 65, loss = 0.04049363\n",
      "Iteration 66, loss = 0.03980656\n",
      "Iteration 67, loss = 0.03932079\n",
      "Iteration 68, loss = 0.03881866\n",
      "Iteration 69, loss = 0.03853547\n",
      "Iteration 70, loss = 0.03785575\n",
      "Iteration 71, loss = 0.03752754\n",
      "Iteration 72, loss = 0.03690948\n",
      "Iteration 73, loss = 0.03641659\n",
      "Iteration 74, loss = 0.03598370\n",
      "Iteration 75, loss = 0.03559970\n",
      "Iteration 76, loss = 0.03511711\n",
      "Iteration 77, loss = 0.03472232\n",
      "Iteration 78, loss = 0.03427386\n",
      "Iteration 79, loss = 0.03391820\n",
      "Iteration 80, loss = 0.03344686\n",
      "Iteration 81, loss = 0.03299416\n",
      "Iteration 82, loss = 0.03257343\n",
      "Iteration 83, loss = 0.03219852\n",
      "Iteration 84, loss = 0.03174093\n",
      "Iteration 85, loss = 0.03130480\n",
      "Iteration 86, loss = 0.03101305\n",
      "Iteration 87, loss = 0.03072862\n",
      "Iteration 88, loss = 0.03019314\n",
      "Iteration 89, loss = 0.02986892\n",
      "Iteration 90, loss = 0.02953226\n",
      "Iteration 91, loss = 0.02918433\n",
      "Iteration 92, loss = 0.02882139\n",
      "Iteration 93, loss = 0.02852839\n",
      "Iteration 94, loss = 0.02833734\n",
      "Iteration 95, loss = 0.02783972\n",
      "Iteration 96, loss = 0.02753461\n",
      "Iteration 97, loss = 0.02721079\n",
      "Iteration 98, loss = 0.02696409\n",
      "Iteration 99, loss = 0.02665300\n",
      "Iteration 100, loss = 0.02631018\n",
      "Iteration 101, loss = 0.02602130\n",
      "Iteration 102, loss = 0.02590725\n",
      "Iteration 103, loss = 0.02547850\n",
      "Iteration 104, loss = 0.02530535\n",
      "Iteration 105, loss = 0.02503066\n",
      "Iteration 106, loss = 0.02460091\n",
      "Iteration 107, loss = 0.02432973\n",
      "Iteration 108, loss = 0.02404586\n",
      "Iteration 109, loss = 0.02382859\n",
      "Iteration 110, loss = 0.02353941\n",
      "Iteration 111, loss = 0.02330243\n",
      "Iteration 112, loss = 0.02299304\n",
      "Iteration 113, loss = 0.02286255\n",
      "Iteration 114, loss = 0.02259941\n",
      "Iteration 115, loss = 0.02227418\n",
      "Iteration 116, loss = 0.02203463\n",
      "Iteration 117, loss = 0.02181191\n",
      "Iteration 118, loss = 0.02154019\n",
      "Iteration 119, loss = 0.02126574\n",
      "Iteration 120, loss = 0.02114297\n",
      "Iteration 121, loss = 0.02095100\n",
      "Iteration 122, loss = 0.02069563\n",
      "Iteration 123, loss = 0.02042870\n",
      "Iteration 124, loss = 0.02020785\n",
      "Iteration 125, loss = 0.01996166\n",
      "Iteration 126, loss = 0.01973499\n",
      "Iteration 127, loss = 0.01953662\n",
      "Iteration 128, loss = 0.01936526\n",
      "Iteration 129, loss = 0.01903161\n",
      "Iteration 130, loss = 0.01893182\n",
      "Iteration 131, loss = 0.01871965\n",
      "Iteration 132, loss = 0.01847769\n",
      "Iteration 133, loss = 0.01829050\n",
      "Iteration 134, loss = 0.01812029\n",
      "Iteration 135, loss = 0.01799989\n",
      "Iteration 136, loss = 0.01766174\n",
      "Iteration 137, loss = 0.01748214\n",
      "Iteration 138, loss = 0.01752231\n",
      "Iteration 139, loss = 0.01716048\n",
      "Iteration 140, loss = 0.01692472\n",
      "Iteration 141, loss = 0.01682232\n",
      "Iteration 142, loss = 0.01659175\n",
      "Iteration 143, loss = 0.01655902\n",
      "Iteration 144, loss = 0.01623707\n",
      "Iteration 145, loss = 0.01612450\n",
      "Iteration 146, loss = 0.01596948\n",
      "Iteration 147, loss = 0.01574289\n",
      "Iteration 148, loss = 0.01566418\n",
      "Iteration 149, loss = 0.01547624\n",
      "Iteration 150, loss = 0.01528831\n",
      "Iteration 151, loss = 0.01517822\n",
      "Iteration 152, loss = 0.01505331\n",
      "Iteration 153, loss = 0.01487886\n",
      "Iteration 154, loss = 0.01482794\n",
      "Iteration 155, loss = 0.01470608\n",
      "Iteration 156, loss = 0.01465688\n",
      "Iteration 157, loss = 0.01446696\n",
      "Iteration 158, loss = 0.01425508\n",
      "Iteration 159, loss = 0.01409853\n",
      "Iteration 160, loss = 0.01395053\n",
      "Iteration 161, loss = 0.01383900\n",
      "Iteration 162, loss = 0.01372264\n",
      "Iteration 163, loss = 0.01357804\n",
      "Iteration 164, loss = 0.01361213\n",
      "Iteration 165, loss = 0.01337983\n",
      "Iteration 166, loss = 0.01326477\n",
      "Iteration 167, loss = 0.01308833\n",
      "Iteration 168, loss = 0.01300637\n",
      "Iteration 169, loss = 0.01286755\n",
      "Iteration 170, loss = 0.01273256\n",
      "Iteration 171, loss = 0.01266439\n",
      "Iteration 172, loss = 0.01256939\n",
      "Iteration 173, loss = 0.01241124\n",
      "Iteration 174, loss = 0.01237013\n",
      "Iteration 175, loss = 0.01219945\n",
      "Iteration 176, loss = 0.01214531\n",
      "Iteration 177, loss = 0.01200370\n",
      "Iteration 178, loss = 0.01194647\n",
      "Iteration 179, loss = 0.01184341\n",
      "Iteration 180, loss = 0.01177691\n",
      "Iteration 181, loss = 0.01168371\n",
      "Iteration 182, loss = 0.01152357\n",
      "Iteration 183, loss = 0.01151414\n",
      "Iteration 184, loss = 0.01137420\n",
      "Iteration 185, loss = 0.01129106\n",
      "Iteration 186, loss = 0.01119152\n",
      "Iteration 187, loss = 0.01110113\n",
      "Iteration 188, loss = 0.01113444\n",
      "Iteration 189, loss = 0.01097527\n",
      "Iteration 190, loss = 0.01088639\n",
      "Iteration 191, loss = 0.01077971\n",
      "Iteration 192, loss = 0.01068184\n",
      "Iteration 193, loss = 0.01061771\n",
      "Iteration 194, loss = 0.01050827\n",
      "Iteration 195, loss = 0.01045467\n",
      "Iteration 196, loss = 0.01037063\n",
      "Iteration 197, loss = 0.01031851\n",
      "Iteration 198, loss = 0.01022013\n",
      "Iteration 199, loss = 0.01015127\n",
      "Iteration 200, loss = 0.01008982\n",
      "Iteration 201, loss = 0.01008441\n",
      "Iteration 202, loss = 0.01003011\n",
      "Iteration 203, loss = 0.00990786\n",
      "Iteration 204, loss = 0.00982987\n",
      "Iteration 205, loss = 0.00976791\n",
      "Iteration 206, loss = 0.00971666\n",
      "Iteration 207, loss = 0.00963777\n",
      "Iteration 208, loss = 0.00965244\n",
      "Iteration 209, loss = 0.00949602\n",
      "Iteration 210, loss = 0.00944149\n",
      "Iteration 211, loss = 0.00937268\n",
      "Iteration 212, loss = 0.00935187\n",
      "Iteration 213, loss = 0.00925950\n",
      "Iteration 214, loss = 0.00920438\n",
      "Iteration 215, loss = 0.00913371\n",
      "Iteration 216, loss = 0.00918324\n",
      "Iteration 217, loss = 0.00903522\n",
      "Iteration 218, loss = 0.00896000\n",
      "Iteration 219, loss = 0.00897104\n",
      "Iteration 220, loss = 0.00886799\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.50467804\n",
      "Iteration 2, loss = 0.44527974\n",
      "Iteration 3, loss = 0.38938013\n",
      "Iteration 4, loss = 0.34304221\n",
      "Iteration 5, loss = 0.30258171\n",
      "Iteration 6, loss = 0.27098466\n",
      "Iteration 7, loss = 0.24659348\n",
      "Iteration 8, loss = 0.22366588\n",
      "Iteration 9, loss = 0.20550789\n",
      "Iteration 10, loss = 0.19088796\n",
      "Iteration 11, loss = 0.17898061\n",
      "Iteration 12, loss = 0.16761214\n",
      "Iteration 13, loss = 0.15981754\n",
      "Iteration 14, loss = 0.14979675\n",
      "Iteration 15, loss = 0.14176500\n",
      "Iteration 16, loss = 0.13461794\n",
      "Iteration 17, loss = 0.12764167\n",
      "Iteration 18, loss = 0.12154637\n",
      "Iteration 19, loss = 0.11586971\n",
      "Iteration 20, loss = 0.11134555\n",
      "Iteration 21, loss = 0.10591356\n",
      "Iteration 22, loss = 0.10141388\n",
      "Iteration 23, loss = 0.09683521\n",
      "Iteration 24, loss = 0.09185671\n",
      "Iteration 25, loss = 0.08769609\n",
      "Iteration 26, loss = 0.08388884\n",
      "Iteration 27, loss = 0.08082246\n",
      "Iteration 28, loss = 0.07812952\n",
      "Iteration 29, loss = 0.07539350\n",
      "Iteration 30, loss = 0.07330925\n",
      "Iteration 31, loss = 0.07125730\n",
      "Iteration 32, loss = 0.06942806\n",
      "Iteration 33, loss = 0.06767196\n",
      "Iteration 34, loss = 0.06594720\n",
      "Iteration 35, loss = 0.06454731\n",
      "Iteration 36, loss = 0.06309678\n",
      "Iteration 37, loss = 0.06172796\n",
      "Iteration 38, loss = 0.06072243\n",
      "Iteration 39, loss = 0.05939271\n",
      "Iteration 40, loss = 0.05821568\n",
      "Iteration 41, loss = 0.05720882\n",
      "Iteration 42, loss = 0.05626213\n",
      "Iteration 43, loss = 0.05536883\n",
      "Iteration 44, loss = 0.05430171\n",
      "Iteration 45, loss = 0.05336965\n",
      "Iteration 46, loss = 0.05246419\n",
      "Iteration 47, loss = 0.05168302\n",
      "Iteration 48, loss = 0.05090088\n",
      "Iteration 49, loss = 0.05016543\n",
      "Iteration 50, loss = 0.04943856\n",
      "Iteration 51, loss = 0.04867026\n",
      "Iteration 52, loss = 0.04794822\n",
      "Iteration 53, loss = 0.04730791\n",
      "Iteration 54, loss = 0.04667276\n",
      "Iteration 55, loss = 0.04601793\n",
      "Iteration 56, loss = 0.04541146\n",
      "Iteration 57, loss = 0.04487536\n",
      "Iteration 58, loss = 0.04418485\n",
      "Iteration 59, loss = 0.04359120\n",
      "Iteration 60, loss = 0.04304607\n",
      "Iteration 61, loss = 0.04249100\n",
      "Iteration 62, loss = 0.04206799\n",
      "Iteration 63, loss = 0.04138942\n",
      "Iteration 64, loss = 0.04085790\n",
      "Iteration 65, loss = 0.04049363\n",
      "Iteration 66, loss = 0.03980656\n",
      "Iteration 67, loss = 0.03932079\n",
      "Iteration 68, loss = 0.03881866\n",
      "Iteration 69, loss = 0.03853547\n",
      "Iteration 70, loss = 0.03785575\n",
      "Iteration 71, loss = 0.03752754\n",
      "Iteration 72, loss = 0.03690948\n",
      "Iteration 73, loss = 0.03641659\n",
      "Iteration 74, loss = 0.03598370\n",
      "Iteration 75, loss = 0.03559970\n",
      "Iteration 76, loss = 0.03511711\n",
      "Iteration 77, loss = 0.03472232\n",
      "Iteration 78, loss = 0.03427386\n",
      "Iteration 79, loss = 0.03391820\n",
      "Iteration 80, loss = 0.03344686\n",
      "Iteration 81, loss = 0.03299416\n",
      "Iteration 82, loss = 0.03257343\n",
      "Iteration 83, loss = 0.03219852\n",
      "Iteration 84, loss = 0.03174093\n",
      "Iteration 85, loss = 0.03130480\n",
      "Iteration 86, loss = 0.03101305\n",
      "Iteration 87, loss = 0.03072862\n",
      "Iteration 88, loss = 0.03019314\n",
      "Iteration 89, loss = 0.02986892\n",
      "Iteration 90, loss = 0.02953226\n",
      "Iteration 91, loss = 0.02918433\n",
      "Iteration 92, loss = 0.02882139\n",
      "Iteration 93, loss = 0.02852839\n",
      "Iteration 94, loss = 0.02833734\n",
      "Iteration 95, loss = 0.02783972\n",
      "Iteration 96, loss = 0.02753461\n",
      "Iteration 97, loss = 0.02721079\n",
      "Iteration 98, loss = 0.02696409\n",
      "Iteration 99, loss = 0.02665300\n",
      "Iteration 100, loss = 0.02631018\n",
      "Iteration 101, loss = 0.02602130\n",
      "Iteration 102, loss = 0.02590725\n",
      "Iteration 103, loss = 0.02547850\n",
      "Iteration 104, loss = 0.02530535\n",
      "Iteration 105, loss = 0.02503066\n",
      "Iteration 106, loss = 0.02460091\n",
      "Iteration 107, loss = 0.02432973\n",
      "Iteration 108, loss = 0.02404586\n",
      "Iteration 109, loss = 0.02382859\n",
      "Iteration 110, loss = 0.02353941\n",
      "Iteration 111, loss = 0.02330243\n",
      "Iteration 112, loss = 0.02299304\n",
      "Iteration 113, loss = 0.02286255\n",
      "Iteration 114, loss = 0.02259941\n",
      "Iteration 115, loss = 0.02227418\n",
      "Iteration 116, loss = 0.02203463\n",
      "Iteration 117, loss = 0.02181191\n",
      "Iteration 118, loss = 0.02154019\n",
      "Iteration 119, loss = 0.02126574\n",
      "Iteration 120, loss = 0.02114297\n",
      "Iteration 121, loss = 0.02095100\n",
      "Iteration 122, loss = 0.02069563\n",
      "Iteration 123, loss = 0.02042870\n",
      "Iteration 124, loss = 0.02020785\n",
      "Iteration 125, loss = 0.01996166\n",
      "Iteration 126, loss = 0.01973499\n",
      "Iteration 127, loss = 0.01953662\n",
      "Iteration 128, loss = 0.01936526\n",
      "Iteration 129, loss = 0.01903161\n",
      "Iteration 130, loss = 0.01893182\n",
      "Iteration 131, loss = 0.01871965\n",
      "Iteration 132, loss = 0.01847769\n",
      "Iteration 133, loss = 0.01829050\n",
      "Iteration 134, loss = 0.01812029\n",
      "Iteration 135, loss = 0.01799989\n",
      "Iteration 136, loss = 0.01766174\n",
      "Iteration 137, loss = 0.01748214\n",
      "Iteration 138, loss = 0.01752231\n",
      "Iteration 139, loss = 0.01716048\n",
      "Iteration 140, loss = 0.01692472\n",
      "Iteration 141, loss = 0.01682232\n",
      "Iteration 142, loss = 0.01659175\n",
      "Iteration 143, loss = 0.01655902\n",
      "Iteration 144, loss = 0.01623707\n",
      "Iteration 145, loss = 0.01612450\n",
      "Iteration 146, loss = 0.01596948\n",
      "Iteration 147, loss = 0.01574289\n",
      "Iteration 148, loss = 0.01566418\n",
      "Iteration 149, loss = 0.01547624\n",
      "Iteration 150, loss = 0.01528831\n",
      "Iteration 151, loss = 0.01517822\n",
      "Iteration 152, loss = 0.01505331\n",
      "Iteration 153, loss = 0.01487886\n",
      "Iteration 154, loss = 0.01482794\n",
      "Iteration 155, loss = 0.01470608\n",
      "Iteration 156, loss = 0.01465688\n",
      "Iteration 157, loss = 0.01446696\n",
      "Iteration 158, loss = 0.01425508\n",
      "Iteration 159, loss = 0.01409853\n",
      "Iteration 160, loss = 0.01395053\n",
      "Iteration 161, loss = 0.01383900\n",
      "Iteration 162, loss = 0.01372264\n",
      "Iteration 163, loss = 0.01357804\n",
      "Iteration 164, loss = 0.01361213\n",
      "Iteration 165, loss = 0.01337983\n",
      "Iteration 166, loss = 0.01326477\n",
      "Iteration 167, loss = 0.01308833\n",
      "Iteration 168, loss = 0.01300637\n",
      "Iteration 169, loss = 0.01286755\n",
      "Iteration 170, loss = 0.01273256\n",
      "Iteration 171, loss = 0.01266439\n",
      "Iteration 172, loss = 0.01256939\n",
      "Iteration 173, loss = 0.01241124\n",
      "Iteration 174, loss = 0.01237013\n",
      "Iteration 175, loss = 0.01219945\n",
      "Iteration 176, loss = 0.01214531\n",
      "Iteration 177, loss = 0.01200370\n",
      "Iteration 178, loss = 0.01194647\n",
      "Iteration 179, loss = 0.01184341\n",
      "Iteration 180, loss = 0.01177691\n",
      "Iteration 181, loss = 0.01168371\n",
      "Iteration 182, loss = 0.01152357\n",
      "Iteration 183, loss = 0.01151414\n",
      "Iteration 184, loss = 0.01137420\n",
      "Iteration 185, loss = 0.01129106\n",
      "Iteration 186, loss = 0.01119152\n",
      "Iteration 187, loss = 0.01110113\n",
      "Iteration 188, loss = 0.01113444\n",
      "Iteration 189, loss = 0.01097527\n",
      "Iteration 190, loss = 0.01088639\n",
      "Iteration 191, loss = 0.01077971\n",
      "Iteration 192, loss = 0.01068184\n",
      "Iteration 193, loss = 0.01061771\n",
      "Iteration 194, loss = 0.01050827\n",
      "Iteration 195, loss = 0.01045467\n",
      "Iteration 196, loss = 0.01037063\n",
      "Iteration 197, loss = 0.01031851\n",
      "Iteration 198, loss = 0.01022013\n",
      "Iteration 199, loss = 0.01015127\n",
      "Iteration 200, loss = 0.01008982\n",
      "Iteration 201, loss = 0.01008441\n",
      "Iteration 202, loss = 0.01003011\n",
      "Iteration 203, loss = 0.00990786\n",
      "Iteration 204, loss = 0.00982987\n",
      "Iteration 205, loss = 0.00976791\n",
      "Iteration 206, loss = 0.00971666\n",
      "Iteration 207, loss = 0.00963777\n",
      "Iteration 208, loss = 0.00965244\n",
      "Iteration 209, loss = 0.00949602\n",
      "Iteration 210, loss = 0.00944149\n",
      "Iteration 211, loss = 0.00937268\n",
      "Iteration 212, loss = 0.00935187\n",
      "Iteration 213, loss = 0.00925950\n",
      "Iteration 214, loss = 0.00920438\n",
      "Iteration 215, loss = 0.00913371\n",
      "Iteration 216, loss = 0.00918324\n",
      "Iteration 217, loss = 0.00903522\n",
      "Iteration 218, loss = 0.00896000\n",
      "Iteration 219, loss = 0.00897104\n",
      "Iteration 220, loss = 0.00886799\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.50467804\n",
      "Iteration 2, loss = 0.44527974\n",
      "Iteration 3, loss = 0.38938013\n",
      "Iteration 4, loss = 0.34304221\n",
      "Iteration 5, loss = 0.30258171\n",
      "Iteration 6, loss = 0.27098466\n",
      "Iteration 7, loss = 0.24659348\n",
      "Iteration 8, loss = 0.22366588\n",
      "Iteration 9, loss = 0.20550789\n",
      "Iteration 10, loss = 0.19088796\n",
      "Iteration 11, loss = 0.17898061\n",
      "Iteration 12, loss = 0.16761214\n",
      "Iteration 13, loss = 0.15981754\n",
      "Iteration 14, loss = 0.14979675\n",
      "Iteration 15, loss = 0.14176500\n",
      "Iteration 16, loss = 0.13461794\n",
      "Iteration 17, loss = 0.12764167\n",
      "Iteration 18, loss = 0.12154637\n",
      "Iteration 19, loss = 0.11586971\n",
      "Iteration 20, loss = 0.11134555\n",
      "Iteration 21, loss = 0.10591356\n",
      "Iteration 22, loss = 0.10141388\n",
      "Iteration 23, loss = 0.09683521\n",
      "Iteration 24, loss = 0.09185671\n",
      "Iteration 25, loss = 0.08769609\n",
      "Iteration 26, loss = 0.08388884\n",
      "Iteration 27, loss = 0.08082246\n",
      "Iteration 28, loss = 0.07812952\n",
      "Iteration 29, loss = 0.07539350\n",
      "Iteration 30, loss = 0.07330925\n",
      "Iteration 31, loss = 0.07125730\n",
      "Iteration 32, loss = 0.06942806\n",
      "Iteration 33, loss = 0.06767196\n",
      "Iteration 34, loss = 0.06594720\n",
      "Iteration 35, loss = 0.06454731\n",
      "Iteration 36, loss = 0.06309678\n",
      "Iteration 37, loss = 0.06172796\n",
      "Iteration 38, loss = 0.06072243\n",
      "Iteration 39, loss = 0.05939271\n",
      "Iteration 40, loss = 0.05821568\n",
      "Iteration 41, loss = 0.05720882\n",
      "Iteration 42, loss = 0.05626213\n",
      "Iteration 43, loss = 0.05536883\n",
      "Iteration 44, loss = 0.05430171\n",
      "Iteration 45, loss = 0.05336965\n",
      "Iteration 46, loss = 0.05246419\n",
      "Iteration 47, loss = 0.05168302\n",
      "Iteration 48, loss = 0.05090088\n",
      "Iteration 49, loss = 0.05016543\n",
      "Iteration 50, loss = 0.04943856\n",
      "Iteration 51, loss = 0.04867026\n",
      "Iteration 52, loss = 0.04794822\n",
      "Iteration 53, loss = 0.04730791\n",
      "Iteration 54, loss = 0.04667276\n",
      "Iteration 55, loss = 0.04601793\n",
      "Iteration 56, loss = 0.04541146\n",
      "Iteration 57, loss = 0.04487536\n",
      "Iteration 58, loss = 0.04418485\n",
      "Iteration 59, loss = 0.04359120\n",
      "Iteration 60, loss = 0.04304607\n",
      "Iteration 61, loss = 0.04249100\n",
      "Iteration 62, loss = 0.04206799\n",
      "Iteration 63, loss = 0.04138942\n",
      "Iteration 64, loss = 0.04085790\n",
      "Iteration 65, loss = 0.04049363\n",
      "Iteration 66, loss = 0.03980656\n",
      "Iteration 67, loss = 0.03932079\n",
      "Iteration 68, loss = 0.03881866\n",
      "Iteration 69, loss = 0.03853547\n",
      "Iteration 70, loss = 0.03785575\n",
      "Iteration 71, loss = 0.03752754\n",
      "Iteration 72, loss = 0.03690948\n",
      "Iteration 73, loss = 0.03641659\n",
      "Iteration 74, loss = 0.03598370\n",
      "Iteration 75, loss = 0.03559970\n",
      "Iteration 76, loss = 0.03511711\n",
      "Iteration 77, loss = 0.03472232\n",
      "Iteration 78, loss = 0.03427386\n",
      "Iteration 79, loss = 0.03391820\n",
      "Iteration 80, loss = 0.03344686\n",
      "Iteration 81, loss = 0.03299416\n",
      "Iteration 82, loss = 0.03257343\n",
      "Iteration 83, loss = 0.03219852\n",
      "Iteration 84, loss = 0.03174093\n",
      "Iteration 85, loss = 0.03130480\n",
      "Iteration 86, loss = 0.03101305\n",
      "Iteration 87, loss = 0.03072862\n",
      "Iteration 88, loss = 0.03019314\n",
      "Iteration 89, loss = 0.02986892\n",
      "Iteration 90, loss = 0.02953226\n",
      "Iteration 91, loss = 0.02918433\n",
      "Iteration 92, loss = 0.02882139\n",
      "Iteration 93, loss = 0.02852839\n",
      "Iteration 94, loss = 0.02833734\n",
      "Iteration 95, loss = 0.02783972\n",
      "Iteration 96, loss = 0.02753461\n",
      "Iteration 97, loss = 0.02721079\n",
      "Iteration 98, loss = 0.02696409\n",
      "Iteration 99, loss = 0.02665300\n",
      "Iteration 100, loss = 0.02631018\n",
      "Iteration 101, loss = 0.02602130\n",
      "Iteration 102, loss = 0.02590725\n",
      "Iteration 103, loss = 0.02547850\n",
      "Iteration 104, loss = 0.02530535\n",
      "Iteration 105, loss = 0.02503066\n",
      "Iteration 106, loss = 0.02460091\n",
      "Iteration 107, loss = 0.02432973\n",
      "Iteration 108, loss = 0.02404586\n",
      "Iteration 109, loss = 0.02382859\n",
      "Iteration 110, loss = 0.02353941\n",
      "Iteration 111, loss = 0.02330243\n",
      "Iteration 112, loss = 0.02299304\n",
      "Iteration 113, loss = 0.02286255\n",
      "Iteration 114, loss = 0.02259941\n",
      "Iteration 115, loss = 0.02227418\n",
      "Iteration 116, loss = 0.02203463\n",
      "Iteration 117, loss = 0.02181191\n",
      "Iteration 118, loss = 0.02154019\n",
      "Iteration 119, loss = 0.02126574\n",
      "Iteration 120, loss = 0.02114297\n",
      "Iteration 121, loss = 0.02095100\n",
      "Iteration 122, loss = 0.02069563\n",
      "Iteration 123, loss = 0.02042870\n",
      "Iteration 124, loss = 0.02020785\n",
      "Iteration 125, loss = 0.01996166\n",
      "Iteration 126, loss = 0.01973499\n",
      "Iteration 127, loss = 0.01953662\n",
      "Iteration 128, loss = 0.01936526\n",
      "Iteration 129, loss = 0.01903161\n",
      "Iteration 130, loss = 0.01893182\n",
      "Iteration 131, loss = 0.01871965\n",
      "Iteration 132, loss = 0.01847769\n",
      "Iteration 133, loss = 0.01829050\n",
      "Iteration 134, loss = 0.01812029\n",
      "Iteration 135, loss = 0.01799989\n",
      "Iteration 136, loss = 0.01766174\n",
      "Iteration 137, loss = 0.01748214\n",
      "Iteration 138, loss = 0.01752231\n",
      "Iteration 139, loss = 0.01716048\n",
      "Iteration 140, loss = 0.01692472\n",
      "Iteration 141, loss = 0.01682232\n",
      "Iteration 142, loss = 0.01659175\n",
      "Iteration 143, loss = 0.01655902\n",
      "Iteration 144, loss = 0.01623707\n",
      "Iteration 145, loss = 0.01612450\n",
      "Iteration 146, loss = 0.01596948\n",
      "Iteration 147, loss = 0.01574289\n",
      "Iteration 148, loss = 0.01566418\n",
      "Iteration 149, loss = 0.01547624\n",
      "Iteration 150, loss = 0.01528831\n",
      "Iteration 151, loss = 0.01517822\n",
      "Iteration 152, loss = 0.01505331\n",
      "Iteration 153, loss = 0.01487886\n",
      "Iteration 154, loss = 0.01482794\n",
      "Iteration 155, loss = 0.01470608\n",
      "Iteration 156, loss = 0.01465688\n",
      "Iteration 157, loss = 0.01446696\n",
      "Iteration 158, loss = 0.01425508\n",
      "Iteration 159, loss = 0.01409853\n",
      "Iteration 160, loss = 0.01395053\n",
      "Iteration 161, loss = 0.01383900\n",
      "Iteration 162, loss = 0.01372264\n",
      "Iteration 163, loss = 0.01357804\n",
      "Iteration 164, loss = 0.01361213\n",
      "Iteration 165, loss = 0.01337983\n",
      "Iteration 166, loss = 0.01326477\n",
      "Iteration 167, loss = 0.01308833\n",
      "Iteration 168, loss = 0.01300637\n",
      "Iteration 169, loss = 0.01286755\n",
      "Iteration 170, loss = 0.01273256\n",
      "Iteration 171, loss = 0.01266439\n",
      "Iteration 172, loss = 0.01256939\n",
      "Iteration 173, loss = 0.01241124\n",
      "Iteration 174, loss = 0.01237013\n",
      "Iteration 175, loss = 0.01219945\n",
      "Iteration 176, loss = 0.01214531\n",
      "Iteration 177, loss = 0.01200370\n",
      "Iteration 178, loss = 0.01194647\n",
      "Iteration 179, loss = 0.01184341\n",
      "Iteration 180, loss = 0.01177691\n",
      "Iteration 181, loss = 0.01168371\n",
      "Iteration 182, loss = 0.01152357\n",
      "Iteration 183, loss = 0.01151414\n",
      "Iteration 184, loss = 0.01137420\n",
      "Iteration 185, loss = 0.01129106\n",
      "Iteration 186, loss = 0.01119152\n",
      "Iteration 187, loss = 0.01110113\n",
      "Iteration 188, loss = 0.01113444\n",
      "Iteration 189, loss = 0.01097527\n",
      "Iteration 190, loss = 0.01088639\n",
      "Iteration 191, loss = 0.01077971\n",
      "Iteration 192, loss = 0.01068184\n",
      "Iteration 193, loss = 0.01061771\n",
      "Iteration 194, loss = 0.01050827\n",
      "Iteration 195, loss = 0.01045467\n",
      "Iteration 196, loss = 0.01037063\n",
      "Iteration 197, loss = 0.01031851\n",
      "Iteration 198, loss = 0.01022013\n",
      "Iteration 199, loss = 0.01015127\n",
      "Iteration 200, loss = 0.01008982\n",
      "Iteration 201, loss = 0.01008441\n",
      "Iteration 202, loss = 0.01003011\n",
      "Iteration 203, loss = 0.00990786\n",
      "Iteration 204, loss = 0.00982987\n",
      "Iteration 205, loss = 0.00976791\n",
      "Iteration 206, loss = 0.00971666\n",
      "Iteration 207, loss = 0.00963777\n",
      "Iteration 208, loss = 0.00965244\n",
      "LinearRegression\n",
      "NeuralNetwork\n",
      "Iteration 209, loss = 0.00949602\n",
      "Iteration 210, loss = 0.00944149\n",
      "Iteration 211, loss = 0.00937268\n",
      "Iteration 212, loss = 0.00935187\n",
      "Iteration 213, loss = 0.00925950\n",
      "Iteration 214, loss = 0.00920438\n",
      "Iteration 215, loss = 0.00913371\n",
      "Iteration 216, loss = 0.00918324\n",
      "Iteration 217, loss = 0.00903522\n",
      "Iteration 218, loss = 0.00896000\n",
      "Iteration 219, loss = 0.00897104\n",
      "Iteration 220, loss = 0.00886799\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.48830824\n",
      "Iteration 2, loss = 0.40057510\n",
      "Iteration 3, loss = 0.33494967\n",
      "Iteration 4, loss = 0.28500752\n",
      "Iteration 5, loss = 0.24966634\n",
      "Iteration 6, loss = 0.22640086\n",
      "Iteration 7, loss = 0.20648977\n",
      "Iteration 8, loss = 0.19176317\n",
      "Iteration 9, loss = 0.17763848\n",
      "Iteration 10, loss = 0.16348342\n",
      "Iteration 11, loss = 0.15162213\n",
      "Iteration 12, loss = 0.14271503\n",
      "Iteration 13, loss = 0.13539349\n",
      "Iteration 14, loss = 0.12971564\n",
      "Iteration 15, loss = 0.12326821\n",
      "Iteration 16, loss = 0.11720761\n",
      "Iteration 17, loss = 0.11074465\n",
      "Iteration 18, loss = 0.10487974\n",
      "Iteration 19, loss = 0.09981126\n",
      "Iteration 20, loss = 0.09519514\n",
      "Iteration 21, loss = 0.09091210\n",
      "Iteration 22, loss = 0.08698788\n",
      "Iteration 23, loss = 0.08368202\n",
      "Iteration 24, loss = 0.08066662\n",
      "Iteration 25, loss = 0.07766202\n",
      "Iteration 26, loss = 0.07491304\n",
      "Iteration 27, loss = 0.07251094\n",
      "Iteration 28, loss = 0.07045579\n",
      "Iteration 29, loss = 0.06863675\n",
      "Iteration 30, loss = 0.06700998\n",
      "Iteration 31, loss = 0.06480954\n",
      "Iteration 32, loss = 0.06279905\n",
      "Iteration 33, loss = 0.06099715\n",
      "Iteration 34, loss = 0.05956280\n",
      "Iteration 35, loss = 0.05801912\n",
      "Iteration 36, loss = 0.05606484\n",
      "Iteration 37, loss = 0.05476412\n",
      "Iteration 38, loss = 0.05333736\n",
      "Iteration 39, loss = 0.05205429\n",
      "Iteration 40, loss = 0.05097144\n",
      "Iteration 41, loss = 0.04947635\n",
      "Iteration 42, loss = 0.04828894\n",
      "Iteration 43, loss = 0.04716657\n",
      "Iteration 44, loss = 0.04600341\n",
      "Iteration 45, loss = 0.04469641\n",
      "Iteration 46, loss = 0.04379648\n",
      "Iteration 47, loss = 0.04284616\n",
      "Iteration 48, loss = 0.04187626\n",
      "Iteration 49, loss = 0.04147095\n",
      "Iteration 50, loss = 0.04017241\n",
      "Iteration 51, loss = 0.03982244\n",
      "Iteration 52, loss = 0.03916286\n",
      "Iteration 53, loss = 0.03812286\n",
      "Iteration 54, loss = 0.03752196\n",
      "Iteration 55, loss = 0.03664060\n",
      "Iteration 56, loss = 0.03645403\n",
      "Iteration 57, loss = 0.03547456\n",
      "Iteration 58, loss = 0.03526155\n",
      "Iteration 59, loss = 0.03490138\n",
      "Iteration 60, loss = 0.03432810\n",
      "Iteration 61, loss = 0.03379947\n",
      "Iteration 62, loss = 0.03328824\n",
      "Iteration 63, loss = 0.03265151\n",
      "Iteration 64, loss = 0.03246307\n",
      "Iteration 65, loss = 0.03225596\n",
      "Iteration 66, loss = 0.03196634\n",
      "Iteration 67, loss = 0.03168032\n",
      "Iteration 68, loss = 0.03125100\n",
      "Iteration 69, loss = 0.03077785\n",
      "Iteration 70, loss = 0.03058345\n",
      "Iteration 71, loss = 0.03025725\n",
      "Iteration 72, loss = 0.03009707\n",
      "Iteration 73, loss = 0.02943113\n",
      "Iteration 74, loss = 0.02921210\n",
      "Iteration 75, loss = 0.02894657\n",
      "Iteration 76, loss = 0.02866830\n",
      "Iteration 77, loss = 0.02840940\n",
      "Iteration 78, loss = 0.02820405\n",
      "Iteration 79, loss = 0.02787159\n",
      "Iteration 80, loss = 0.02755140\n",
      "Iteration 81, loss = 0.02758756\n",
      "Iteration 82, loss = 0.02732905\n",
      "Iteration 83, loss = 0.02696393\n",
      "Iteration 84, loss = 0.02674919\n",
      "Iteration 85, loss = 0.02641195\n",
      "Iteration 86, loss = 0.02622891\n",
      "Iteration 87, loss = 0.02615250\n",
      "Iteration 88, loss = 0.02597563\n",
      "Iteration 89, loss = 0.02573856\n",
      "Iteration 90, loss = 0.02548013\n",
      "Iteration 91, loss = 0.02541652\n",
      "Iteration 92, loss = 0.02509154\n",
      "Iteration 93, loss = 0.02482010\n",
      "Iteration 94, loss = 0.02447385\n",
      "Iteration 95, loss = 0.02438672\n",
      "Iteration 96, loss = 0.02422054\n",
      "Iteration 97, loss = 0.02385795\n",
      "Iteration 98, loss = 0.02374436\n",
      "Iteration 99, loss = 0.02389001\n",
      "Iteration 100, loss = 0.02338508\n",
      "Iteration 101, loss = 0.02327271\n",
      "Iteration 102, loss = 0.02329232\n",
      "Iteration 103, loss = 0.02317331\n",
      "Iteration 104, loss = 0.02304585\n",
      "Iteration 105, loss = 0.02270236\n",
      "Iteration 106, loss = 0.02250709\n",
      "Iteration 107, loss = 0.02252719\n",
      "Iteration 108, loss = 0.02222314\n",
      "Iteration 109, loss = 0.02213557\n",
      "Iteration 110, loss = 0.02166055\n",
      "Iteration 111, loss = 0.02184928\n",
      "Iteration 112, loss = 0.02147626\n",
      "Iteration 113, loss = 0.02171416\n",
      "Iteration 114, loss = 0.02134005\n",
      "Iteration 115, loss = 0.02112043\n",
      "Iteration 116, loss = 0.02097129\n",
      "Iteration 117, loss = 0.02088553\n",
      "Iteration 118, loss = 0.02066016\n",
      "Iteration 119, loss = 0.02047221\n",
      "Iteration 120, loss = 0.02035682\n",
      "Iteration 121, loss = 0.02027375\n",
      "Iteration 122, loss = 0.02023204\n",
      "Iteration 123, loss = 0.02020205\n",
      "Iteration 124, loss = 0.01984107\n",
      "Iteration 125, loss = 0.01949729\n",
      "Iteration 126, loss = 0.01967864\n",
      "Iteration 127, loss = 0.01936016\n",
      "Iteration 128, loss = 0.01933300\n",
      "Iteration 129, loss = 0.01904226\n",
      "Iteration 130, loss = 0.01896461\n",
      "Iteration 131, loss = 0.01906814\n",
      "Iteration 132, loss = 0.01878584\n",
      "Iteration 133, loss = 0.01842560\n",
      "Iteration 134, loss = 0.01864170\n",
      "Iteration 135, loss = 0.01874646\n",
      "Iteration 136, loss = 0.01816319\n",
      "Iteration 137, loss = 0.01831980\n",
      "Iteration 138, loss = 0.01848033\n",
      "Iteration 139, loss = 0.01842639\n",
      "Iteration 140, loss = 0.01830420\n",
      "Iteration 141, loss = 0.01812236\n",
      "Iteration 142, loss = 0.01794639\n",
      "Iteration 143, loss = 0.01775552\n",
      "Iteration 144, loss = 0.01769485\n",
      "Iteration 145, loss = 0.01752517\n",
      "Iteration 146, loss = 0.01723356\n",
      "Iteration 147, loss = 0.01722334\n",
      "Iteration 148, loss = 0.01723483\n",
      "Iteration 149, loss = 0.01705412\n",
      "Iteration 150, loss = 0.01705317\n",
      "Iteration 151, loss = 0.01693413\n",
      "Iteration 152, loss = 0.01710113\n",
      "Iteration 153, loss = 0.01681444\n",
      "Iteration 154, loss = 0.01652413\n",
      "Iteration 155, loss = 0.01652069\n",
      "Iteration 156, loss = 0.01649479\n",
      "Iteration 157, loss = 0.01625232\n",
      "Iteration 158, loss = 0.01619217\n",
      "Iteration 159, loss = 0.01600107\n",
      "Iteration 160, loss = 0.01608072\n",
      "Iteration 161, loss = 0.01588143\n",
      "Iteration 162, loss = 0.01576850\n",
      "Iteration 163, loss = 0.01571394\n",
      "Iteration 164, loss = 0.01561475\n",
      "Iteration 165, loss = 0.01550876\n",
      "Iteration 166, loss = 0.01537088\n",
      "Iteration 167, loss = 0.01534118\n",
      "Iteration 168, loss = 0.01515616\n",
      "Iteration 169, loss = 0.01506593\n",
      "Iteration 170, loss = 0.01526558\n",
      "Iteration 171, loss = 0.01503523\n",
      "Iteration 172, loss = 0.01491918\n",
      "Iteration 173, loss = 0.01521400\n",
      "Iteration 174, loss = 0.01504442\n",
      "Iteration 175, loss = 0.01514507\n",
      "Iteration 176, loss = 0.01498606\n",
      "Iteration 177, loss = 0.01492111\n",
      "Iteration 178, loss = 0.01472929\n",
      "Iteration 179, loss = 0.01467613\n",
      "Iteration 180, loss = 0.01502393\n",
      "Iteration 181, loss = 0.01470302\n",
      "Iteration 182, loss = 0.01474920\n",
      "Iteration 183, loss = 0.01466221\n",
      "Iteration 184, loss = 0.01467650\n",
      "Iteration 185, loss = 0.01430898\n",
      "Iteration 186, loss = 0.01418218\n",
      "Iteration 187, loss = 0.01414402\n",
      "Iteration 188, loss = 0.01402115\n",
      "Iteration 189, loss = 0.01406983\n",
      "Iteration 190, loss = 0.01418263\n",
      "Iteration 191, loss = 0.01392260\n",
      "Iteration 192, loss = 0.01400197\n",
      "Iteration 193, loss = 0.01410409\n",
      "Iteration 194, loss = 0.01415005\n",
      "Iteration 195, loss = 0.01366629\n",
      "Iteration 196, loss = 0.01397602\n",
      "Iteration 197, loss = 0.01347743\n",
      "Iteration 198, loss = 0.01351643\n",
      "Iteration 199, loss = 0.01343659\n",
      "Iteration 200, loss = 0.01334751\n",
      "Iteration 201, loss = 0.01338652\n",
      "Iteration 202, loss = 0.01358046\n",
      "Iteration 203, loss = 0.01394248\n",
      "Iteration 204, loss = 0.01337135\n",
      "Iteration 205, loss = 0.01319447\n",
      "Iteration 206, loss = 0.01347167\n",
      "Iteration 207, loss = 0.01342578\n",
      "Iteration 208, loss = 0.01297165\n",
      "Iteration 209, loss = 0.01400467\n",
      "Iteration 210, loss = 0.01315125\n",
      "Iteration 211, loss = 0.01294448\n",
      "Iteration 212, loss = 0.01325253\n",
      "Iteration 213, loss = 0.01414335\n",
      "Iteration 214, loss = 0.01375324\n",
      "Iteration 215, loss = 0.01285884\n",
      "Iteration 216, loss = 0.01269984\n",
      "Iteration 217, loss = 0.01281675\n",
      "Iteration 218, loss = 0.01288054\n",
      "Iteration 219, loss = 0.01251692\n",
      "Iteration 220, loss = 0.01257422\n",
      "Iteration 221, loss = 0.01242787\n",
      "Iteration 222, loss = 0.01265204\n",
      "Iteration 223, loss = 0.01236711\n",
      "Iteration 224, loss = 0.01286199\n",
      "Iteration 225, loss = 0.01263901\n",
      "Iteration 226, loss = 0.01308726\n",
      "Iteration 227, loss = 0.01293631\n",
      "Iteration 228, loss = 0.01286492\n",
      "Iteration 229, loss = 0.01252004\n",
      "Iteration 230, loss = 0.01244558\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.48830824\n",
      "Iteration 2, loss = 0.40057510\n",
      "Iteration 3, loss = 0.33494967\n",
      "Iteration 4, loss = 0.28500752\n",
      "Iteration 5, loss = 0.24966634\n",
      "Iteration 6, loss = 0.22640086\n",
      "Iteration 7, loss = 0.20648977\n",
      "Iteration 8, loss = 0.19176317\n",
      "Iteration 9, loss = 0.17763848\n",
      "Iteration 10, loss = 0.16348342\n",
      "Iteration 11, loss = 0.15162213\n",
      "Iteration 12, loss = 0.14271503\n",
      "Iteration 13, loss = 0.13539349\n",
      "Iteration 14, loss = 0.12971564\n",
      "Iteration 15, loss = 0.12326821\n",
      "Iteration 16, loss = 0.11720761\n",
      "Iteration 17, loss = 0.11074465\n",
      "Iteration 18, loss = 0.10487974\n",
      "Iteration 19, loss = 0.09981126\n",
      "Iteration 20, loss = 0.09519514\n",
      "Iteration 21, loss = 0.09091210\n",
      "Iteration 22, loss = 0.08698788\n",
      "Iteration 23, loss = 0.08368202\n",
      "Iteration 24, loss = 0.08066662\n",
      "Iteration 25, loss = 0.07766202\n",
      "Iteration 26, loss = 0.07491304\n",
      "Iteration 27, loss = 0.07251094\n",
      "Iteration 28, loss = 0.07045579\n",
      "Iteration 29, loss = 0.06863675\n",
      "Iteration 30, loss = 0.06700998\n",
      "Iteration 31, loss = 0.06480954\n",
      "Iteration 32, loss = 0.06279905\n",
      "Iteration 33, loss = 0.06099715\n",
      "Iteration 34, loss = 0.05956280\n",
      "Iteration 35, loss = 0.05801912\n",
      "Iteration 36, loss = 0.05606484\n",
      "Iteration 37, loss = 0.05476412\n",
      "Iteration 38, loss = 0.05333736\n",
      "Iteration 39, loss = 0.05205429\n",
      "Iteration 40, loss = 0.05097144\n",
      "Iteration 41, loss = 0.04947635\n",
      "Iteration 42, loss = 0.04828894\n",
      "Iteration 43, loss = 0.04716657\n",
      "Iteration 44, loss = 0.04600341\n",
      "Iteration 45, loss = 0.04469641\n",
      "Iteration 46, loss = 0.04379648\n",
      "Iteration 47, loss = 0.04284616\n",
      "Iteration 48, loss = 0.04187626\n",
      "Iteration 49, loss = 0.04147095\n",
      "Iteration 50, loss = 0.04017241\n",
      "Iteration 51, loss = 0.03982244\n",
      "Iteration 52, loss = 0.03916286\n",
      "Iteration 53, loss = 0.03812286\n",
      "Iteration 54, loss = 0.03752196\n",
      "Iteration 55, loss = 0.03664060\n",
      "Iteration 56, loss = 0.03645403\n",
      "Iteration 57, loss = 0.03547456\n",
      "Iteration 58, loss = 0.03526155\n",
      "Iteration 59, loss = 0.03490138\n",
      "Iteration 60, loss = 0.03432810\n",
      "Iteration 61, loss = 0.03379947\n",
      "Iteration 62, loss = 0.03328824\n",
      "Iteration 63, loss = 0.03265151\n",
      "Iteration 64, loss = 0.03246307\n",
      "Iteration 65, loss = 0.03225596\n",
      "Iteration 66, loss = 0.03196634\n",
      "Iteration 67, loss = 0.03168032\n",
      "Iteration 68, loss = 0.03125100\n",
      "Iteration 69, loss = 0.03077785\n",
      "Iteration 70, loss = 0.03058345\n",
      "Iteration 71, loss = 0.03025725\n",
      "Iteration 72, loss = 0.03009707\n",
      "Iteration 73, loss = 0.02943113\n",
      "Iteration 74, loss = 0.02921210\n",
      "Iteration 75, loss = 0.02894657\n",
      "Iteration 76, loss = 0.02866830\n",
      "Iteration 77, loss = 0.02840940\n",
      "Iteration 78, loss = 0.02820405\n",
      "Iteration 79, loss = 0.02787159\n",
      "Iteration 80, loss = 0.02755140\n",
      "Iteration 81, loss = 0.02758756\n",
      "Iteration 82, loss = 0.02732905\n",
      "Iteration 83, loss = 0.02696393\n",
      "Iteration 84, loss = 0.02674919\n",
      "Iteration 85, loss = 0.02641195\n",
      "Iteration 86, loss = 0.02622891\n",
      "Iteration 87, loss = 0.02615250\n",
      "Iteration 88, loss = 0.02597563\n",
      "Iteration 89, loss = 0.02573856\n",
      "Iteration 90, loss = 0.02548013\n",
      "Iteration 91, loss = 0.02541652\n",
      "Iteration 92, loss = 0.02509154\n",
      "Iteration 93, loss = 0.02482010\n",
      "Iteration 94, loss = 0.02447385\n",
      "Iteration 95, loss = 0.02438672\n",
      "Iteration 96, loss = 0.02422054\n",
      "Iteration 97, loss = 0.02385795\n",
      "Iteration 98, loss = 0.02374436\n",
      "Iteration 99, loss = 0.02389001\n",
      "Iteration 100, loss = 0.02338508\n",
      "Iteration 101, loss = 0.02327271\n",
      "Iteration 102, loss = 0.02329232\n",
      "Iteration 103, loss = 0.02317331\n",
      "Iteration 104, loss = 0.02304585\n",
      "Iteration 105, loss = 0.02270236\n",
      "Iteration 106, loss = 0.02250709\n",
      "Iteration 107, loss = 0.02252719\n",
      "Iteration 108, loss = 0.02222314\n",
      "Iteration 109, loss = 0.02213557\n",
      "Iteration 110, loss = 0.02166055\n",
      "Iteration 111, loss = 0.02184928\n",
      "Iteration 112, loss = 0.02147626\n",
      "Iteration 113, loss = 0.02171416\n",
      "Iteration 114, loss = 0.02134005\n",
      "Iteration 115, loss = 0.02112043\n",
      "Iteration 116, loss = 0.02097129\n",
      "Iteration 117, loss = 0.02088553\n",
      "Iteration 118, loss = 0.02066016\n",
      "Iteration 119, loss = 0.02047221\n",
      "Iteration 120, loss = 0.02035682\n",
      "Iteration 121, loss = 0.02027375\n",
      "Iteration 122, loss = 0.02023204\n",
      "Iteration 123, loss = 0.02020205\n",
      "Iteration 124, loss = 0.01984107\n",
      "Iteration 125, loss = 0.01949729\n",
      "Iteration 126, loss = 0.01967864\n",
      "Iteration 127, loss = 0.01936016\n",
      "Iteration 128, loss = 0.01933300\n",
      "Iteration 129, loss = 0.01904226\n",
      "Iteration 130, loss = 0.01896461\n",
      "Iteration 131, loss = 0.01906814\n",
      "Iteration 132, loss = 0.01878584\n",
      "Iteration 133, loss = 0.01842560\n",
      "Iteration 134, loss = 0.01864170\n",
      "Iteration 135, loss = 0.01874646\n",
      "Iteration 136, loss = 0.01816319\n",
      "Iteration 137, loss = 0.01831980\n",
      "Iteration 138, loss = 0.01848033\n",
      "Iteration 139, loss = 0.01842639\n",
      "Iteration 140, loss = 0.01830420\n",
      "Iteration 141, loss = 0.01812236\n",
      "Iteration 142, loss = 0.01794639\n",
      "Iteration 143, loss = 0.01775552\n",
      "Iteration 144, loss = 0.01769485\n",
      "Iteration 145, loss = 0.01752517\n",
      "Iteration 146, loss = 0.01723356\n",
      "Iteration 147, loss = 0.01722334\n",
      "Iteration 148, loss = 0.01723483\n",
      "Iteration 149, loss = 0.01705412\n",
      "Iteration 150, loss = 0.01705317\n",
      "Iteration 151, loss = 0.01693413\n",
      "Iteration 152, loss = 0.01710113\n",
      "Iteration 153, loss = 0.01681444\n",
      "Iteration 154, loss = 0.01652413\n",
      "Iteration 155, loss = 0.01652069\n",
      "Iteration 156, loss = 0.01649479\n",
      "Iteration 157, loss = 0.01625232\n",
      "Iteration 158, loss = 0.01619217\n",
      "Iteration 159, loss = 0.01600107\n",
      "Iteration 160, loss = 0.01608072\n",
      "Iteration 161, loss = 0.01588143\n",
      "Iteration 162, loss = 0.01576850\n",
      "Iteration 163, loss = 0.01571394\n",
      "Iteration 164, loss = 0.01561475\n",
      "Iteration 165, loss = 0.01550876\n",
      "Iteration 166, loss = 0.01537088\n",
      "Iteration 167, loss = 0.01534118\n",
      "Iteration 168, loss = 0.01515616\n",
      "Iteration 169, loss = 0.01506593\n",
      "Iteration 170, loss = 0.01526558\n",
      "Iteration 171, loss = 0.01503523\n",
      "Iteration 172, loss = 0.01491918\n",
      "Iteration 173, loss = 0.01521400\n",
      "Iteration 174, loss = 0.01504442\n",
      "Iteration 175, loss = 0.01514507\n",
      "Iteration 176, loss = 0.01498606\n",
      "Iteration 177, loss = 0.01492111\n",
      "Iteration 178, loss = 0.01472929\n",
      "Iteration 179, loss = 0.01467613\n",
      "Iteration 180, loss = 0.01502393\n",
      "Iteration 181, loss = 0.01470302\n",
      "Iteration 182, loss = 0.01474920\n",
      "Iteration 183, loss = 0.01466221\n",
      "Iteration 184, loss = 0.01467650\n",
      "Iteration 185, loss = 0.01430898\n",
      "Iteration 186, loss = 0.01418218\n",
      "Iteration 187, loss = 0.01414402\n",
      "Iteration 188, loss = 0.01402115\n",
      "Iteration 189, loss = 0.01406983\n",
      "Iteration 190, loss = 0.01418263\n",
      "Iteration 191, loss = 0.01392260\n",
      "Iteration 192, loss = 0.01400197\n",
      "Iteration 193, loss = 0.01410409\n",
      "Iteration 194, loss = 0.01415005\n",
      "Iteration 195, loss = 0.01366629\n",
      "Iteration 196, loss = 0.01397602\n",
      "Iteration 197, loss = 0.01347743\n",
      "Iteration 198, loss = 0.01351643\n",
      "Iteration 199, loss = 0.01343659\n",
      "Iteration 200, loss = 0.01334751\n",
      "Iteration 201, loss = 0.01338652\n",
      "Iteration 202, loss = 0.01358046\n",
      "Iteration 203, loss = 0.01394248\n",
      "Iteration 204, loss = 0.01337135\n",
      "Iteration 205, loss = 0.01319447\n",
      "Iteration 206, loss = 0.01347167\n",
      "Iteration 207, loss = 0.01342578\n",
      "Iteration 208, loss = 0.01297165\n",
      "Iteration 209, loss = 0.01400467\n",
      "Iteration 210, loss = 0.01315125\n",
      "Iteration 211, loss = 0.01294448\n",
      "Iteration 212, loss = 0.01325253\n",
      "Iteration 213, loss = 0.01414335\n",
      "Iteration 214, loss = 0.01375324\n",
      "Iteration 215, loss = 0.01285884\n",
      "Iteration 216, loss = 0.01269984\n",
      "Iteration 217, loss = 0.01281675\n",
      "Iteration 218, loss = 0.01288054\n",
      "Iteration 219, loss = 0.01251692\n",
      "Iteration 220, loss = 0.01257422\n",
      "Iteration 221, loss = 0.01242787\n",
      "Iteration 222, loss = 0.01265204\n",
      "Iteration 223, loss = 0.01236711\n",
      "Iteration 224, loss = 0.01286199\n",
      "Iteration 225, loss = 0.01263901\n",
      "Iteration 226, loss = 0.01308726\n",
      "Iteration 227, loss = 0.01293631\n",
      "Iteration 228, loss = 0.01286492\n",
      "Iteration 229, loss = 0.01252004\n",
      "Iteration 230, loss = 0.01244558\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.48830824\n",
      "Iteration 2, loss = 0.40057510\n",
      "Iteration 3, loss = 0.33494967\n",
      "Iteration 4, loss = 0.28500752\n",
      "Iteration 5, loss = 0.24966634\n",
      "Iteration 6, loss = 0.22640086\n",
      "Iteration 7, loss = 0.20648977\n",
      "Iteration 8, loss = 0.19176317\n",
      "Iteration 9, loss = 0.17763848\n",
      "Iteration 10, loss = 0.16348342\n",
      "Iteration 11, loss = 0.15162213\n",
      "Iteration 12, loss = 0.14271503\n",
      "Iteration 13, loss = 0.13539349\n",
      "Iteration 14, loss = 0.12971564\n",
      "Iteration 15, loss = 0.12326821\n",
      "Iteration 16, loss = 0.11720761\n",
      "Iteration 17, loss = 0.11074465\n",
      "Iteration 18, loss = 0.10487974\n",
      "Iteration 19, loss = 0.09981126\n",
      "Iteration 20, loss = 0.09519514\n",
      "Iteration 21, loss = 0.09091210\n",
      "Iteration 22, loss = 0.08698788\n",
      "Iteration 23, loss = 0.08368202\n",
      "LinearRegression\n",
      "NeuralNetwork\n",
      "Iteration 24, loss = 0.08066662\n",
      "Iteration 25, loss = 0.07766202\n",
      "Iteration 26, loss = 0.07491304\n",
      "Iteration 27, loss = 0.07251094\n",
      "Iteration 28, loss = 0.07045579\n",
      "Iteration 29, loss = 0.06863675\n",
      "Iteration 30, loss = 0.06700998\n",
      "Iteration 31, loss = 0.06480954\n",
      "Iteration 32, loss = 0.06279905\n",
      "Iteration 33, loss = 0.06099715\n",
      "Iteration 34, loss = 0.05956280\n",
      "Iteration 35, loss = 0.05801912\n",
      "Iteration 36, loss = 0.05606484\n",
      "Iteration 37, loss = 0.05476412\n",
      "Iteration 38, loss = 0.05333736\n",
      "Iteration 39, loss = 0.05205429\n",
      "Iteration 40, loss = 0.05097144\n",
      "Iteration 41, loss = 0.04947635\n",
      "Iteration 42, loss = 0.04828894\n",
      "Iteration 43, loss = 0.04716657\n",
      "Iteration 44, loss = 0.04600341\n",
      "Iteration 45, loss = 0.04469641\n",
      "Iteration 46, loss = 0.04379648\n",
      "Iteration 47, loss = 0.04284616\n",
      "Iteration 48, loss = 0.04187626\n",
      "Iteration 49, loss = 0.04147095\n",
      "Iteration 50, loss = 0.04017241\n",
      "Iteration 51, loss = 0.03982244\n",
      "Iteration 52, loss = 0.03916286\n",
      "Iteration 53, loss = 0.03812286\n",
      "Iteration 54, loss = 0.03752196\n",
      "Iteration 55, loss = 0.03664060\n",
      "Iteration 56, loss = 0.03645403\n",
      "Iteration 57, loss = 0.03547456\n",
      "Iteration 58, loss = 0.03526155\n",
      "Iteration 59, loss = 0.03490138\n",
      "Iteration 60, loss = 0.03432810\n",
      "Iteration 61, loss = 0.03379947\n",
      "Iteration 62, loss = 0.03328824\n",
      "Iteration 63, loss = 0.03265151\n",
      "Iteration 64, loss = 0.03246307\n",
      "Iteration 65, loss = 0.03225596\n",
      "Iteration 66, loss = 0.03196634\n",
      "Iteration 67, loss = 0.03168032\n",
      "Iteration 68, loss = 0.03125100\n",
      "Iteration 69, loss = 0.03077785\n",
      "Iteration 70, loss = 0.03058345\n",
      "Iteration 71, loss = 0.03025725\n",
      "Iteration 72, loss = 0.03009707\n",
      "Iteration 73, loss = 0.02943113\n",
      "Iteration 74, loss = 0.02921210\n",
      "Iteration 75, loss = 0.02894657\n",
      "Iteration 76, loss = 0.02866830\n",
      "Iteration 77, loss = 0.02840940\n",
      "Iteration 78, loss = 0.02820405\n",
      "Iteration 79, loss = 0.02787159\n",
      "Iteration 80, loss = 0.02755140\n",
      "Iteration 81, loss = 0.02758756\n",
      "Iteration 82, loss = 0.02732905\n",
      "Iteration 83, loss = 0.02696393\n",
      "Iteration 84, loss = 0.02674919\n",
      "Iteration 85, loss = 0.02641195\n",
      "Iteration 86, loss = 0.02622891\n",
      "Iteration 87, loss = 0.02615250\n",
      "Iteration 88, loss = 0.02597563\n",
      "Iteration 89, loss = 0.02573856\n",
      "Iteration 90, loss = 0.02548013\n",
      "Iteration 91, loss = 0.02541652\n",
      "Iteration 92, loss = 0.02509154\n",
      "Iteration 93, loss = 0.02482010\n",
      "Iteration 94, loss = 0.02447385\n",
      "Iteration 95, loss = 0.02438672\n",
      "Iteration 96, loss = 0.02422054\n",
      "Iteration 97, loss = 0.02385795\n",
      "Iteration 98, loss = 0.02374436\n",
      "Iteration 99, loss = 0.02389001\n",
      "Iteration 100, loss = 0.02338508\n",
      "Iteration 101, loss = 0.02327271\n",
      "Iteration 102, loss = 0.02329232\n",
      "Iteration 103, loss = 0.02317331\n",
      "Iteration 104, loss = 0.02304585\n",
      "Iteration 105, loss = 0.02270236\n",
      "Iteration 106, loss = 0.02250709\n",
      "Iteration 107, loss = 0.02252719\n",
      "Iteration 108, loss = 0.02222314\n",
      "Iteration 109, loss = 0.02213557\n",
      "Iteration 110, loss = 0.02166055\n",
      "Iteration 111, loss = 0.02184928\n",
      "Iteration 112, loss = 0.02147626\n",
      "Iteration 113, loss = 0.02171416\n",
      "Iteration 114, loss = 0.02134005\n",
      "Iteration 115, loss = 0.02112043\n",
      "Iteration 116, loss = 0.02097129\n",
      "Iteration 117, loss = 0.02088553\n",
      "Iteration 118, loss = 0.02066016\n",
      "Iteration 119, loss = 0.02047221\n",
      "Iteration 120, loss = 0.02035682\n",
      "Iteration 121, loss = 0.02027375\n",
      "Iteration 122, loss = 0.02023204\n",
      "Iteration 123, loss = 0.02020205\n",
      "Iteration 124, loss = 0.01984107\n",
      "Iteration 125, loss = 0.01949729\n",
      "Iteration 126, loss = 0.01967864\n",
      "Iteration 127, loss = 0.01936016\n",
      "Iteration 128, loss = 0.01933300\n",
      "Iteration 129, loss = 0.01904226\n",
      "Iteration 130, loss = 0.01896461\n",
      "Iteration 131, loss = 0.01906814\n",
      "Iteration 132, loss = 0.01878584\n",
      "Iteration 133, loss = 0.01842560\n",
      "Iteration 134, loss = 0.01864170\n",
      "Iteration 135, loss = 0.01874646\n",
      "Iteration 136, loss = 0.01816319\n",
      "Iteration 137, loss = 0.01831980\n",
      "Iteration 138, loss = 0.01848033\n",
      "Iteration 139, loss = 0.01842639\n",
      "Iteration 140, loss = 0.01830420\n",
      "Iteration 141, loss = 0.01812236\n",
      "Iteration 142, loss = 0.01794639\n",
      "Iteration 143, loss = 0.01775552\n",
      "Iteration 144, loss = 0.01769485\n",
      "Iteration 145, loss = 0.01752517\n",
      "Iteration 146, loss = 0.01723356\n",
      "Iteration 147, loss = 0.01722334\n",
      "Iteration 148, loss = 0.01723483\n",
      "Iteration 149, loss = 0.01705412\n",
      "Iteration 150, loss = 0.01705317\n",
      "Iteration 151, loss = 0.01693413\n",
      "Iteration 152, loss = 0.01710113\n",
      "Iteration 153, loss = 0.01681444\n",
      "Iteration 154, loss = 0.01652413\n",
      "Iteration 155, loss = 0.01652069\n",
      "Iteration 156, loss = 0.01649479\n",
      "Iteration 157, loss = 0.01625232\n",
      "Iteration 158, loss = 0.01619217\n",
      "Iteration 159, loss = 0.01600107\n",
      "Iteration 160, loss = 0.01608072\n",
      "Iteration 161, loss = 0.01588143\n",
      "Iteration 162, loss = 0.01576850\n",
      "Iteration 163, loss = 0.01571394\n",
      "Iteration 164, loss = 0.01561475\n",
      "Iteration 165, loss = 0.01550876\n",
      "Iteration 166, loss = 0.01537088\n",
      "Iteration 167, loss = 0.01534118\n",
      "Iteration 168, loss = 0.01515616\n",
      "Iteration 169, loss = 0.01506593\n",
      "Iteration 170, loss = 0.01526558\n",
      "Iteration 171, loss = 0.01503523\n",
      "Iteration 172, loss = 0.01491918\n",
      "Iteration 173, loss = 0.01521400\n",
      "Iteration 174, loss = 0.01504442\n",
      "Iteration 175, loss = 0.01514507\n",
      "Iteration 176, loss = 0.01498606\n",
      "Iteration 177, loss = 0.01492111\n",
      "Iteration 178, loss = 0.01472929\n",
      "Iteration 179, loss = 0.01467613\n",
      "Iteration 180, loss = 0.01502393\n",
      "Iteration 181, loss = 0.01470302\n",
      "Iteration 182, loss = 0.01474920\n",
      "Iteration 183, loss = 0.01466221\n",
      "Iteration 184, loss = 0.01467650\n",
      "Iteration 185, loss = 0.01430898\n",
      "Iteration 186, loss = 0.01418218\n",
      "Iteration 187, loss = 0.01414402\n",
      "Iteration 188, loss = 0.01402115\n",
      "Iteration 189, loss = 0.01406983\n",
      "Iteration 190, loss = 0.01418263\n",
      "Iteration 191, loss = 0.01392260\n",
      "Iteration 192, loss = 0.01400197\n",
      "Iteration 193, loss = 0.01410409\n",
      "Iteration 194, loss = 0.01415005\n",
      "Iteration 195, loss = 0.01366629\n",
      "Iteration 196, loss = 0.01397602\n",
      "Iteration 197, loss = 0.01347743\n",
      "Iteration 198, loss = 0.01351643\n",
      "Iteration 199, loss = 0.01343659\n",
      "Iteration 200, loss = 0.01334751\n",
      "Iteration 201, loss = 0.01338652\n",
      "Iteration 202, loss = 0.01358046\n",
      "Iteration 203, loss = 0.01394248\n",
      "Iteration 204, loss = 0.01337135\n",
      "Iteration 205, loss = 0.01319447\n",
      "Iteration 206, loss = 0.01347167\n",
      "Iteration 207, loss = 0.01342578\n",
      "Iteration 208, loss = 0.01297165\n",
      "Iteration 209, loss = 0.01400467\n",
      "Iteration 210, loss = 0.01315125\n",
      "Iteration 211, loss = 0.01294448\n",
      "Iteration 212, loss = 0.01325253\n",
      "Iteration 213, loss = 0.01414335\n",
      "Iteration 214, loss = 0.01375324\n",
      "Iteration 215, loss = 0.01285884\n",
      "Iteration 216, loss = 0.01269984\n",
      "Iteration 217, loss = 0.01281675\n",
      "Iteration 218, loss = 0.01288054\n",
      "Iteration 219, loss = 0.01251692\n",
      "Iteration 220, loss = 0.01257422\n",
      "Iteration 221, loss = 0.01242787\n",
      "Iteration 222, loss = 0.01265204\n",
      "Iteration 223, loss = 0.01236711\n",
      "Iteration 224, loss = 0.01286199\n",
      "Iteration 225, loss = 0.01263901\n",
      "Iteration 226, loss = 0.01308726\n",
      "Iteration 227, loss = 0.01293631\n",
      "Iteration 228, loss = 0.01286492\n",
      "Iteration 229, loss = 0.01252004\n",
      "Iteration 230, loss = 0.01244558\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.49073423\n",
      "Iteration 2, loss = 0.40480324\n",
      "Iteration 3, loss = 0.34480108\n",
      "Iteration 4, loss = 0.28811931\n",
      "Iteration 5, loss = 0.24911897\n",
      "Iteration 6, loss = 0.22062988\n",
      "Iteration 7, loss = 0.20305533\n",
      "Iteration 8, loss = 0.18569173\n",
      "Iteration 9, loss = 0.17391077\n",
      "Iteration 10, loss = 0.16274574\n",
      "Iteration 11, loss = 0.15139869\n",
      "Iteration 12, loss = 0.14215346\n",
      "Iteration 13, loss = 0.13421691\n",
      "Iteration 14, loss = 0.12760181\n",
      "Iteration 15, loss = 0.12204174\n",
      "Iteration 16, loss = 0.11610021\n",
      "Iteration 17, loss = 0.10969689\n",
      "Iteration 18, loss = 0.10440009\n",
      "Iteration 19, loss = 0.09909750\n",
      "Iteration 20, loss = 0.09438167\n",
      "Iteration 21, loss = 0.09025975\n",
      "Iteration 22, loss = 0.08661297\n",
      "Iteration 23, loss = 0.08272199\n",
      "Iteration 24, loss = 0.07935221\n",
      "Iteration 25, loss = 0.07640829\n",
      "Iteration 26, loss = 0.07366975\n",
      "Iteration 27, loss = 0.07153252\n",
      "Iteration 28, loss = 0.06893869\n",
      "Iteration 29, loss = 0.06651848\n",
      "Iteration 30, loss = 0.06491587\n",
      "Iteration 31, loss = 0.06309532\n",
      "Iteration 32, loss = 0.06114802\n",
      "Iteration 33, loss = 0.05917563\n",
      "Iteration 34, loss = 0.05717903\n",
      "Iteration 35, loss = 0.05608984\n",
      "Iteration 36, loss = 0.05422675\n",
      "Iteration 37, loss = 0.05267568\n",
      "Iteration 38, loss = 0.05134282\n",
      "Iteration 39, loss = 0.04996176\n",
      "Iteration 40, loss = 0.04870357\n",
      "Iteration 41, loss = 0.04740911\n",
      "Iteration 42, loss = 0.04635658\n",
      "Iteration 43, loss = 0.04507860\n",
      "Iteration 44, loss = 0.04411708\n",
      "Iteration 45, loss = 0.04309707\n",
      "Iteration 46, loss = 0.04231795\n",
      "Iteration 47, loss = 0.04136677\n",
      "Iteration 48, loss = 0.04037865\n",
      "Iteration 49, loss = 0.03978465\n",
      "Iteration 50, loss = 0.03875261\n",
      "Iteration 51, loss = 0.03812068\n",
      "Iteration 52, loss = 0.03742888\n",
      "Iteration 53, loss = 0.03678055\n",
      "Iteration 54, loss = 0.03610751\n",
      "Iteration 55, loss = 0.03519165\n",
      "Iteration 56, loss = 0.03491132\n",
      "Iteration 57, loss = 0.03418201\n",
      "Iteration 58, loss = 0.03381542\n",
      "Iteration 59, loss = 0.03303550\n",
      "Iteration 60, loss = 0.03260025\n",
      "Iteration 61, loss = 0.03208285\n",
      "Iteration 62, loss = 0.03179187\n",
      "Iteration 63, loss = 0.03128035\n",
      "Iteration 64, loss = 0.03089101\n",
      "Iteration 65, loss = 0.03033030\n",
      "Iteration 66, loss = 0.03022126\n",
      "Iteration 67, loss = 0.02948743\n",
      "Iteration 68, loss = 0.02946079\n",
      "Iteration 69, loss = 0.02917802\n",
      "Iteration 70, loss = 0.02871550\n",
      "Iteration 71, loss = 0.02833688\n",
      "Iteration 72, loss = 0.02834797\n",
      "Iteration 73, loss = 0.02755551\n",
      "Iteration 74, loss = 0.02732317\n",
      "Iteration 75, loss = 0.02727606\n",
      "Iteration 76, loss = 0.02680634\n",
      "Iteration 77, loss = 0.02663243\n",
      "Iteration 78, loss = 0.02634317\n",
      "Iteration 79, loss = 0.02607446\n",
      "Iteration 80, loss = 0.02580545\n",
      "Iteration 81, loss = 0.02556736\n",
      "Iteration 82, loss = 0.02534145\n",
      "Iteration 83, loss = 0.02513601\n",
      "Iteration 84, loss = 0.02496376\n",
      "Iteration 85, loss = 0.02463857\n",
      "Iteration 86, loss = 0.02454056\n",
      "Iteration 87, loss = 0.02452223\n",
      "Iteration 88, loss = 0.02424935\n",
      "Iteration 89, loss = 0.02404476\n",
      "Iteration 90, loss = 0.02381486\n",
      "Iteration 91, loss = 0.02360470\n",
      "Iteration 92, loss = 0.02339361\n",
      "Iteration 93, loss = 0.02332903\n",
      "Iteration 94, loss = 0.02312631\n",
      "Iteration 95, loss = 0.02341268\n",
      "Iteration 96, loss = 0.02283557\n",
      "Iteration 97, loss = 0.02293989\n",
      "Iteration 98, loss = 0.02248810\n",
      "Iteration 99, loss = 0.02233184\n",
      "Iteration 100, loss = 0.02223968\n",
      "Iteration 101, loss = 0.02208599\n",
      "Iteration 102, loss = 0.02186982\n",
      "Iteration 103, loss = 0.02186208\n",
      "Iteration 104, loss = 0.02202742\n",
      "Iteration 105, loss = 0.02144266\n",
      "Iteration 106, loss = 0.02154089\n",
      "Iteration 107, loss = 0.02117548\n",
      "Iteration 108, loss = 0.02113450\n",
      "Iteration 109, loss = 0.02123459\n",
      "Iteration 110, loss = 0.02080049\n",
      "Iteration 111, loss = 0.02068359\n",
      "Iteration 112, loss = 0.02058934\n",
      "Iteration 113, loss = 0.02052093\n",
      "Iteration 114, loss = 0.02036437\n",
      "Iteration 115, loss = 0.02032465\n",
      "Iteration 116, loss = 0.02010060\n",
      "Iteration 117, loss = 0.02012070\n",
      "Iteration 118, loss = 0.01992884\n",
      "Iteration 119, loss = 0.01985169\n",
      "Iteration 120, loss = 0.01996694\n",
      "Iteration 121, loss = 0.01994448\n",
      "Iteration 122, loss = 0.01945421\n",
      "Iteration 123, loss = 0.01990665\n",
      "Iteration 124, loss = 0.01934231\n",
      "Iteration 125, loss = 0.01938430\n",
      "Iteration 126, loss = 0.01902058\n",
      "Iteration 127, loss = 0.01885725\n",
      "Iteration 128, loss = 0.01888930\n",
      "Iteration 129, loss = 0.01877380\n",
      "Iteration 130, loss = 0.01874015\n",
      "Iteration 131, loss = 0.01846757\n",
      "Iteration 132, loss = 0.01842299\n",
      "Iteration 133, loss = 0.01849007\n",
      "Iteration 134, loss = 0.01825939\n",
      "Iteration 135, loss = 0.01806918\n",
      "Iteration 136, loss = 0.01801421\n",
      "Iteration 137, loss = 0.01827203\n",
      "Iteration 138, loss = 0.01807935\n",
      "Iteration 139, loss = 0.01784035\n",
      "Iteration 140, loss = 0.01770022\n",
      "Iteration 141, loss = 0.01759785\n",
      "Iteration 142, loss = 0.01742484\n",
      "Iteration 143, loss = 0.01740496\n",
      "Iteration 144, loss = 0.01743529\n",
      "Iteration 145, loss = 0.01734509\n",
      "Iteration 146, loss = 0.01724250\n",
      "Iteration 147, loss = 0.01708583\n",
      "Iteration 148, loss = 0.01730551\n",
      "Iteration 149, loss = 0.01696255\n",
      "Iteration 150, loss = 0.01685098\n",
      "Iteration 151, loss = 0.01697573\n",
      "Iteration 152, loss = 0.01689221\n",
      "Iteration 153, loss = 0.01677264\n",
      "Iteration 154, loss = 0.01691755\n",
      "Iteration 155, loss = 0.01665892\n",
      "Iteration 156, loss = 0.01651856\n",
      "Iteration 157, loss = 0.01642218\n",
      "Iteration 158, loss = 0.01623793\n",
      "Iteration 159, loss = 0.01625139\n",
      "Iteration 160, loss = 0.01606320\n",
      "Iteration 161, loss = 0.01614758\n",
      "Iteration 162, loss = 0.01609330\n",
      "Iteration 163, loss = 0.01613020\n",
      "Iteration 164, loss = 0.01581236\n",
      "Iteration 165, loss = 0.01573337\n",
      "Iteration 166, loss = 0.01565519\n",
      "Iteration 167, loss = 0.01579056\n",
      "Iteration 168, loss = 0.01552443\n",
      "Iteration 169, loss = 0.01556441\n",
      "Iteration 170, loss = 0.01546181\n",
      "Iteration 171, loss = 0.01534331\n",
      "Iteration 172, loss = 0.01528764\n",
      "Iteration 173, loss = 0.01510715\n",
      "Iteration 174, loss = 0.01522187\n",
      "Iteration 175, loss = 0.01536643\n",
      "Iteration 176, loss = 0.01494787\n",
      "Iteration 177, loss = 0.01543139\n",
      "Iteration 178, loss = 0.01526918\n",
      "Iteration 179, loss = 0.01497236\n",
      "Iteration 180, loss = 0.01494162\n",
      "Iteration 181, loss = 0.01475559\n",
      "Iteration 182, loss = 0.01489745\n",
      "Iteration 183, loss = 0.01457828\n",
      "Iteration 184, loss = 0.01462315\n",
      "Iteration 185, loss = 0.01444340\n",
      "Iteration 186, loss = 0.01442521\n",
      "Iteration 187, loss = 0.01448440\n",
      "Iteration 188, loss = 0.01444649\n",
      "Iteration 189, loss = 0.01432332\n",
      "Iteration 190, loss = 0.01472325\n",
      "Iteration 191, loss = 0.01426470\n",
      "Iteration 192, loss = 0.01414754\n",
      "Iteration 193, loss = 0.01417284\n",
      "Iteration 194, loss = 0.01405875\n",
      "Iteration 195, loss = 0.01390040\n",
      "Iteration 196, loss = 0.01405090\n",
      "Iteration 197, loss = 0.01387247\n",
      "Iteration 198, loss = 0.01384551\n",
      "Iteration 199, loss = 0.01399604\n",
      "Iteration 200, loss = 0.01405964\n",
      "Iteration 201, loss = 0.01393954\n",
      "Iteration 202, loss = 0.01390180\n",
      "Iteration 203, loss = 0.01361401\n",
      "Iteration 204, loss = 0.01368320\n",
      "Iteration 205, loss = 0.01375083\n",
      "Iteration 206, loss = 0.01340381\n",
      "Iteration 207, loss = 0.01341205\n",
      "Iteration 208, loss = 0.01339394\n",
      "Iteration 209, loss = 0.01359846\n",
      "Iteration 210, loss = 0.01315188\n",
      "Iteration 211, loss = 0.01364389\n",
      "Iteration 212, loss = 0.01337164\n",
      "Iteration 213, loss = 0.01329608\n",
      "Iteration 214, loss = 0.01317253\n",
      "Iteration 215, loss = 0.01304988\n",
      "Iteration 216, loss = 0.01295883\n",
      "Iteration 217, loss = 0.01294158\n",
      "Iteration 218, loss = 0.01293744\n",
      "Iteration 219, loss = 0.01354198\n",
      "Iteration 220, loss = 0.01299275\n",
      "Iteration 221, loss = 0.01308450\n",
      "Iteration 222, loss = 0.01280999\n",
      "Iteration 223, loss = 0.01269799\n",
      "Iteration 224, loss = 0.01264568\n",
      "Iteration 225, loss = 0.01268424\n",
      "Iteration 226, loss = 0.01254752\n",
      "Iteration 227, loss = 0.01244868\n",
      "Iteration 228, loss = 0.01237807\n",
      "Iteration 229, loss = 0.01235889\n",
      "Iteration 230, loss = 0.01232580\n",
      "Iteration 231, loss = 0.01232430\n",
      "Iteration 232, loss = 0.01233004\n",
      "Iteration 233, loss = 0.01246060\n",
      "Iteration 234, loss = 0.01224435\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.49073423\n",
      "Iteration 2, loss = 0.40480324\n",
      "Iteration 3, loss = 0.34480108\n",
      "Iteration 4, loss = 0.28811931\n",
      "Iteration 5, loss = 0.24911897\n",
      "Iteration 6, loss = 0.22062988\n",
      "Iteration 7, loss = 0.20305533\n",
      "Iteration 8, loss = 0.18569173\n",
      "Iteration 9, loss = 0.17391077\n",
      "Iteration 10, loss = 0.16274574\n",
      "Iteration 11, loss = 0.15139869\n",
      "Iteration 12, loss = 0.14215346\n",
      "Iteration 13, loss = 0.13421691\n",
      "Iteration 14, loss = 0.12760181\n",
      "Iteration 15, loss = 0.12204174\n",
      "Iteration 16, loss = 0.11610021\n",
      "Iteration 17, loss = 0.10969689\n",
      "Iteration 18, loss = 0.10440009\n",
      "Iteration 19, loss = 0.09909750\n",
      "Iteration 20, loss = 0.09438167\n",
      "Iteration 21, loss = 0.09025975\n",
      "Iteration 22, loss = 0.08661297\n",
      "Iteration 23, loss = 0.08272199\n",
      "Iteration 24, loss = 0.07935221\n",
      "Iteration 25, loss = 0.07640829\n",
      "Iteration 26, loss = 0.07366975\n",
      "Iteration 27, loss = 0.07153252\n",
      "Iteration 28, loss = 0.06893869\n",
      "Iteration 29, loss = 0.06651848\n",
      "Iteration 30, loss = 0.06491587\n",
      "Iteration 31, loss = 0.06309532\n",
      "Iteration 32, loss = 0.06114802\n",
      "Iteration 33, loss = 0.05917563\n",
      "Iteration 34, loss = 0.05717903\n",
      "Iteration 35, loss = 0.05608984\n",
      "Iteration 36, loss = 0.05422675\n",
      "Iteration 37, loss = 0.05267568\n",
      "Iteration 38, loss = 0.05134282\n",
      "Iteration 39, loss = 0.04996176\n",
      "Iteration 40, loss = 0.04870357\n",
      "Iteration 41, loss = 0.04740911\n",
      "Iteration 42, loss = 0.04635658\n",
      "Iteration 43, loss = 0.04507860\n",
      "Iteration 44, loss = 0.04411708\n",
      "Iteration 45, loss = 0.04309707\n",
      "Iteration 46, loss = 0.04231795\n",
      "Iteration 47, loss = 0.04136677\n",
      "Iteration 48, loss = 0.04037865\n",
      "Iteration 49, loss = 0.03978465\n",
      "Iteration 50, loss = 0.03875261\n",
      "Iteration 51, loss = 0.03812068\n",
      "Iteration 52, loss = 0.03742888\n",
      "Iteration 53, loss = 0.03678055\n",
      "Iteration 54, loss = 0.03610751\n",
      "Iteration 55, loss = 0.03519165\n",
      "Iteration 56, loss = 0.03491132\n",
      "Iteration 57, loss = 0.03418201\n",
      "Iteration 58, loss = 0.03381542\n",
      "Iteration 59, loss = 0.03303550\n",
      "Iteration 60, loss = 0.03260025\n",
      "Iteration 61, loss = 0.03208285\n",
      "Iteration 62, loss = 0.03179187\n",
      "Iteration 63, loss = 0.03128035\n",
      "Iteration 64, loss = 0.03089101\n",
      "Iteration 65, loss = 0.03033030\n",
      "Iteration 66, loss = 0.03022126\n",
      "Iteration 67, loss = 0.02948743\n",
      "Iteration 68, loss = 0.02946079\n",
      "Iteration 69, loss = 0.02917802\n",
      "Iteration 70, loss = 0.02871550\n",
      "Iteration 71, loss = 0.02833688\n",
      "Iteration 72, loss = 0.02834797\n",
      "Iteration 73, loss = 0.02755551\n",
      "Iteration 74, loss = 0.02732317\n",
      "Iteration 75, loss = 0.02727606\n",
      "Iteration 76, loss = 0.02680634\n",
      "Iteration 77, loss = 0.02663243\n",
      "Iteration 78, loss = 0.02634317\n",
      "Iteration 79, loss = 0.02607446\n",
      "Iteration 80, loss = 0.02580545\n",
      "Iteration 81, loss = 0.02556736\n",
      "Iteration 82, loss = 0.02534145\n",
      "Iteration 83, loss = 0.02513601\n",
      "Iteration 84, loss = 0.02496376\n",
      "Iteration 85, loss = 0.02463857\n",
      "Iteration 86, loss = 0.02454056\n",
      "Iteration 87, loss = 0.02452223\n",
      "Iteration 88, loss = 0.02424935\n",
      "Iteration 89, loss = 0.02404476\n",
      "Iteration 90, loss = 0.02381486\n",
      "Iteration 91, loss = 0.02360470\n",
      "Iteration 92, loss = 0.02339361\n",
      "Iteration 93, loss = 0.02332903\n",
      "Iteration 94, loss = 0.02312631\n",
      "Iteration 95, loss = 0.02341268\n",
      "Iteration 96, loss = 0.02283557\n",
      "Iteration 97, loss = 0.02293989\n",
      "Iteration 98, loss = 0.02248810\n",
      "Iteration 99, loss = 0.02233184\n",
      "Iteration 100, loss = 0.02223968\n",
      "Iteration 101, loss = 0.02208599\n",
      "Iteration 102, loss = 0.02186982\n",
      "Iteration 103, loss = 0.02186208\n",
      "Iteration 104, loss = 0.02202742\n",
      "Iteration 105, loss = 0.02144266\n",
      "Iteration 106, loss = 0.02154089\n",
      "Iteration 107, loss = 0.02117548\n",
      "Iteration 108, loss = 0.02113450\n",
      "Iteration 109, loss = 0.02123459\n",
      "Iteration 110, loss = 0.02080049\n",
      "Iteration 111, loss = 0.02068359\n",
      "Iteration 112, loss = 0.02058934\n",
      "Iteration 113, loss = 0.02052093\n",
      "Iteration 114, loss = 0.02036437\n",
      "Iteration 115, loss = 0.02032465\n",
      "Iteration 116, loss = 0.02010060\n",
      "Iteration 117, loss = 0.02012070\n",
      "Iteration 118, loss = 0.01992884\n",
      "Iteration 119, loss = 0.01985169\n",
      "Iteration 120, loss = 0.01996694\n",
      "Iteration 121, loss = 0.01994448\n",
      "Iteration 122, loss = 0.01945421\n",
      "Iteration 123, loss = 0.01990665\n",
      "Iteration 124, loss = 0.01934231\n",
      "Iteration 125, loss = 0.01938430\n",
      "Iteration 126, loss = 0.01902058\n",
      "Iteration 127, loss = 0.01885725\n",
      "Iteration 128, loss = 0.01888930\n",
      "Iteration 129, loss = 0.01877380\n",
      "Iteration 130, loss = 0.01874015\n",
      "Iteration 131, loss = 0.01846757\n",
      "Iteration 132, loss = 0.01842299\n",
      "Iteration 133, loss = 0.01849007\n",
      "Iteration 134, loss = 0.01825939\n",
      "Iteration 135, loss = 0.01806918\n",
      "Iteration 136, loss = 0.01801421\n",
      "Iteration 137, loss = 0.01827203\n",
      "Iteration 138, loss = 0.01807935\n",
      "Iteration 139, loss = 0.01784035\n",
      "Iteration 140, loss = 0.01770022\n",
      "Iteration 141, loss = 0.01759785\n",
      "Iteration 142, loss = 0.01742484\n",
      "Iteration 143, loss = 0.01740496\n",
      "Iteration 144, loss = 0.01743529\n",
      "Iteration 145, loss = 0.01734509\n",
      "Iteration 146, loss = 0.01724250\n",
      "Iteration 147, loss = 0.01708583\n",
      "Iteration 148, loss = 0.01730551\n",
      "Iteration 149, loss = 0.01696255\n",
      "Iteration 150, loss = 0.01685098\n",
      "Iteration 151, loss = 0.01697573\n",
      "Iteration 152, loss = 0.01689221\n",
      "Iteration 153, loss = 0.01677264\n",
      "Iteration 154, loss = 0.01691755\n",
      "Iteration 155, loss = 0.01665892\n",
      "Iteration 156, loss = 0.01651856\n",
      "Iteration 157, loss = 0.01642218\n",
      "Iteration 158, loss = 0.01623793\n",
      "Iteration 159, loss = 0.01625139\n",
      "Iteration 160, loss = 0.01606320\n",
      "Iteration 161, loss = 0.01614758\n",
      "Iteration 162, loss = 0.01609330\n",
      "Iteration 163, loss = 0.01613020\n",
      "Iteration 164, loss = 0.01581236\n",
      "Iteration 165, loss = 0.01573337\n",
      "Iteration 166, loss = 0.01565519\n",
      "Iteration 167, loss = 0.01579056\n",
      "Iteration 168, loss = 0.01552443\n",
      "Iteration 169, loss = 0.01556441\n",
      "Iteration 170, loss = 0.01546181\n",
      "Iteration 171, loss = 0.01534331\n",
      "Iteration 172, loss = 0.01528764\n",
      "Iteration 173, loss = 0.01510715\n",
      "Iteration 174, loss = 0.01522187\n",
      "Iteration 175, loss = 0.01536643\n",
      "Iteration 176, loss = 0.01494787\n",
      "Iteration 177, loss = 0.01543139\n",
      "Iteration 178, loss = 0.01526918\n",
      "Iteration 179, loss = 0.01497236\n",
      "Iteration 180, loss = 0.01494162\n",
      "Iteration 181, loss = 0.01475559\n",
      "Iteration 182, loss = 0.01489745\n",
      "Iteration 183, loss = 0.01457828\n",
      "Iteration 184, loss = 0.01462315\n",
      "Iteration 185, loss = 0.01444340\n",
      "Iteration 186, loss = 0.01442521\n",
      "Iteration 187, loss = 0.01448440\n",
      "Iteration 188, loss = 0.01444649\n",
      "Iteration 189, loss = 0.01432332\n",
      "Iteration 190, loss = 0.01472325\n",
      "Iteration 191, loss = 0.01426470\n",
      "Iteration 192, loss = 0.01414754\n",
      "Iteration 193, loss = 0.01417284\n",
      "Iteration 194, loss = 0.01405875\n",
      "Iteration 195, loss = 0.01390040\n",
      "Iteration 196, loss = 0.01405090\n",
      "Iteration 197, loss = 0.01387247\n",
      "Iteration 198, loss = 0.01384551\n",
      "Iteration 199, loss = 0.01399604\n",
      "Iteration 200, loss = 0.01405964\n",
      "Iteration 201, loss = 0.01393954\n",
      "Iteration 202, loss = 0.01390180\n",
      "Iteration 203, loss = 0.01361401\n",
      "Iteration 204, loss = 0.01368320\n",
      "Iteration 205, loss = 0.01375083\n",
      "Iteration 206, loss = 0.01340381\n",
      "Iteration 207, loss = 0.01341205\n",
      "Iteration 208, loss = 0.01339394\n",
      "Iteration 209, loss = 0.01359846\n",
      "Iteration 210, loss = 0.01315188\n",
      "Iteration 211, loss = 0.01364389\n",
      "Iteration 212, loss = 0.01337164\n",
      "Iteration 213, loss = 0.01329608\n",
      "Iteration 214, loss = 0.01317253\n",
      "Iteration 215, loss = 0.01304988\n",
      "Iteration 216, loss = 0.01295883\n",
      "Iteration 217, loss = 0.01294158\n",
      "Iteration 218, loss = 0.01293744\n",
      "Iteration 219, loss = 0.01354198\n",
      "Iteration 220, loss = 0.01299275\n",
      "Iteration 221, loss = 0.01308450\n",
      "Iteration 222, loss = 0.01280999\n",
      "Iteration 223, loss = 0.01269799\n",
      "Iteration 224, loss = 0.01264568\n",
      "Iteration 225, loss = 0.01268424\n",
      "Iteration 226, loss = 0.01254752\n",
      "Iteration 227, loss = 0.01244868\n",
      "Iteration 228, loss = 0.01237807\n",
      "Iteration 229, loss = 0.01235889\n",
      "Iteration 230, loss = 0.01232580\n",
      "Iteration 231, loss = 0.01232430\n",
      "Iteration 232, loss = 0.01233004\n",
      "Iteration 233, loss = 0.01246060\n",
      "Iteration 234, loss = 0.01224435\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.49073423\n",
      "Iteration 2, loss = 0.40480324\n",
      "Iteration 3, loss = 0.34480108\n",
      "Iteration 4, loss = 0.28811931\n",
      "Iteration 5, loss = 0.24911897\n",
      "Iteration 6, loss = 0.22062988\n",
      "Iteration 7, loss = 0.20305533\n",
      "Iteration 8, loss = 0.18569173\n",
      "Iteration 9, loss = 0.17391077\n",
      "Iteration 10, loss = 0.16274574\n",
      "Iteration 11, loss = 0.15139869\n",
      "Iteration 12, loss = 0.14215346\n",
      "Iteration 13, loss = 0.13421691\n",
      "Iteration 14, loss = 0.12760181\n",
      "Iteration 15, loss = 0.12204174\n",
      "Iteration 16, loss = 0.11610021\n",
      "Iteration 17, loss = 0.10969689\n",
      "Iteration 18, loss = 0.10440009\n",
      "Iteration 19, loss = 0.09909750\n",
      "Iteration 20, loss = 0.09438167\n",
      "Iteration 21, loss = 0.09025975\n",
      "Iteration 22, loss = 0.08661297\n",
      "Iteration 23, loss = 0.08272199\n",
      "Iteration 24, loss = 0.07935221\n",
      "Iteration 25, loss = 0.07640829\n",
      "Iteration 26, loss = 0.07366975\n",
      "Iteration 27, loss = 0.07153252\n",
      "Iteration 28, loss = 0.06893869\n",
      "Iteration 29, loss = 0.06651848\n",
      "Iteration 30, loss = 0.06491587\n",
      "Iteration 31, loss = 0.06309532\n",
      "Iteration 32, loss = 0.06114802\n",
      "Iteration 33, loss = 0.05917563\n",
      "Iteration 34, loss = 0.05717903\n",
      "Iteration 35, loss = 0.05608984\n",
      "Iteration 36, loss = 0.05422675\n",
      "Iteration 37, loss = 0.05267568\n",
      "Iteration 38, loss = 0.05134282\n",
      "Iteration 39, loss = 0.04996176\n",
      "Iteration 40, loss = 0.04870357\n",
      "Iteration 41, loss = 0.04740911\n",
      "Iteration 42, loss = 0.04635658\n",
      "Iteration 43, loss = 0.04507860\n",
      "Iteration 44, loss = 0.04411708\n",
      "Iteration 45, loss = 0.04309707\n",
      "Iteration 46, loss = 0.04231795\n",
      "Iteration 47, loss = 0.04136677\n",
      "Iteration 48, loss = 0.04037865\n",
      "Iteration 49, loss = 0.03978465\n",
      "Iteration 50, loss = 0.03875261\n",
      "Iteration 51, loss = 0.03812068\n",
      "Iteration 52, loss = 0.03742888\n",
      "Iteration 53, loss = 0.03678055\n",
      "Iteration 54, loss = 0.03610751\n",
      "Iteration 55, loss = 0.03519165\n",
      "Iteration 56, loss = 0.03491132\n",
      "Iteration 57, loss = 0.03418201\n",
      "Iteration 58, loss = 0.03381542\n",
      "Iteration 59, loss = 0.03303550\n",
      "Iteration 60, loss = 0.03260025\n",
      "Iteration 61, loss = 0.03208285\n",
      "Iteration 62, loss = 0.03179187\n",
      "Iteration 63, loss = 0.03128035\n",
      "Iteration 64, loss = 0.03089101\n",
      "Iteration 65, loss = 0.03033030\n",
      "Iteration 66, loss = 0.03022126\n",
      "Iteration 67, loss = 0.02948743\n",
      "Iteration 68, loss = 0.02946079\n",
      "Iteration 69, loss = 0.02917802\n",
      "Iteration 70, loss = 0.02871550\n",
      "Iteration 71, loss = 0.02833688\n",
      "Iteration 72, loss = 0.02834797\n",
      "LinearRegression\n",
      "NeuralNetwork\n",
      "Iteration 73, loss = 0.02755551\n",
      "Iteration 74, loss = 0.02732317\n",
      "Iteration 75, loss = 0.02727606\n",
      "Iteration 76, loss = 0.02680634\n",
      "Iteration 77, loss = 0.02663243\n",
      "Iteration 78, loss = 0.02634317\n",
      "Iteration 79, loss = 0.02607446\n",
      "Iteration 80, loss = 0.02580545\n",
      "Iteration 81, loss = 0.02556736\n",
      "Iteration 82, loss = 0.02534145\n",
      "Iteration 83, loss = 0.02513601\n",
      "Iteration 84, loss = 0.02496376\n",
      "Iteration 85, loss = 0.02463857\n",
      "Iteration 86, loss = 0.02454056\n",
      "Iteration 87, loss = 0.02452223\n",
      "Iteration 88, loss = 0.02424935\n",
      "Iteration 89, loss = 0.02404476\n",
      "Iteration 90, loss = 0.02381486\n",
      "Iteration 91, loss = 0.02360470\n",
      "Iteration 92, loss = 0.02339361\n",
      "Iteration 93, loss = 0.02332903\n",
      "Iteration 94, loss = 0.02312631\n",
      "Iteration 95, loss = 0.02341268\n",
      "Iteration 96, loss = 0.02283557\n",
      "Iteration 97, loss = 0.02293989\n",
      "Iteration 98, loss = 0.02248810\n",
      "Iteration 99, loss = 0.02233184\n",
      "Iteration 100, loss = 0.02223968\n",
      "Iteration 101, loss = 0.02208599\n",
      "Iteration 102, loss = 0.02186982\n",
      "Iteration 103, loss = 0.02186208\n",
      "Iteration 104, loss = 0.02202742\n",
      "Iteration 105, loss = 0.02144266\n",
      "Iteration 106, loss = 0.02154089\n",
      "Iteration 107, loss = 0.02117548\n",
      "Iteration 108, loss = 0.02113450\n",
      "Iteration 109, loss = 0.02123459\n",
      "Iteration 110, loss = 0.02080049\n",
      "Iteration 111, loss = 0.02068359\n",
      "Iteration 112, loss = 0.02058934\n",
      "Iteration 113, loss = 0.02052093\n",
      "Iteration 114, loss = 0.02036437\n",
      "Iteration 115, loss = 0.02032465\n",
      "Iteration 116, loss = 0.02010060\n",
      "Iteration 117, loss = 0.02012070\n",
      "Iteration 118, loss = 0.01992884\n",
      "Iteration 119, loss = 0.01985169\n",
      "Iteration 120, loss = 0.01996694\n",
      "Iteration 121, loss = 0.01994448\n",
      "Iteration 122, loss = 0.01945421\n",
      "Iteration 123, loss = 0.01990665\n",
      "Iteration 124, loss = 0.01934231\n",
      "Iteration 125, loss = 0.01938430\n",
      "Iteration 126, loss = 0.01902058\n",
      "Iteration 127, loss = 0.01885725\n",
      "Iteration 128, loss = 0.01888930\n",
      "Iteration 129, loss = 0.01877380\n",
      "Iteration 130, loss = 0.01874015\n",
      "Iteration 131, loss = 0.01846757\n",
      "Iteration 132, loss = 0.01842299\n",
      "Iteration 133, loss = 0.01849007\n",
      "Iteration 134, loss = 0.01825939\n",
      "Iteration 135, loss = 0.01806918\n",
      "Iteration 136, loss = 0.01801421\n",
      "Iteration 137, loss = 0.01827203\n",
      "Iteration 138, loss = 0.01807935\n",
      "Iteration 139, loss = 0.01784035\n",
      "Iteration 140, loss = 0.01770022\n",
      "Iteration 141, loss = 0.01759785\n",
      "Iteration 142, loss = 0.01742484\n",
      "Iteration 143, loss = 0.01740496\n",
      "Iteration 144, loss = 0.01743529\n",
      "Iteration 145, loss = 0.01734509\n",
      "Iteration 146, loss = 0.01724250\n",
      "Iteration 147, loss = 0.01708583\n",
      "Iteration 148, loss = 0.01730551\n",
      "Iteration 149, loss = 0.01696255\n",
      "Iteration 150, loss = 0.01685098\n",
      "Iteration 151, loss = 0.01697573\n",
      "Iteration 152, loss = 0.01689221\n",
      "Iteration 153, loss = 0.01677264\n",
      "Iteration 154, loss = 0.01691755\n",
      "Iteration 155, loss = 0.01665892\n",
      "Iteration 156, loss = 0.01651856\n",
      "Iteration 157, loss = 0.01642218\n",
      "Iteration 158, loss = 0.01623793\n",
      "Iteration 159, loss = 0.01625139\n",
      "Iteration 160, loss = 0.01606320\n",
      "Iteration 161, loss = 0.01614758\n",
      "Iteration 162, loss = 0.01609330\n",
      "Iteration 163, loss = 0.01613020\n",
      "Iteration 164, loss = 0.01581236\n",
      "Iteration 165, loss = 0.01573337\n",
      "Iteration 166, loss = 0.01565519\n",
      "Iteration 167, loss = 0.01579056\n",
      "Iteration 168, loss = 0.01552443\n",
      "Iteration 169, loss = 0.01556441\n",
      "Iteration 170, loss = 0.01546181\n",
      "Iteration 171, loss = 0.01534331\n",
      "Iteration 172, loss = 0.01528764\n",
      "Iteration 173, loss = 0.01510715\n",
      "Iteration 174, loss = 0.01522187\n",
      "Iteration 175, loss = 0.01536643\n",
      "Iteration 176, loss = 0.01494787\n",
      "Iteration 177, loss = 0.01543139\n",
      "Iteration 178, loss = 0.01526918\n",
      "Iteration 179, loss = 0.01497236\n",
      "Iteration 180, loss = 0.01494162\n",
      "Iteration 181, loss = 0.01475559\n",
      "Iteration 182, loss = 0.01489745\n",
      "Iteration 183, loss = 0.01457828\n",
      "Iteration 184, loss = 0.01462315\n",
      "Iteration 185, loss = 0.01444340\n",
      "Iteration 186, loss = 0.01442521\n",
      "Iteration 187, loss = 0.01448440\n",
      "Iteration 188, loss = 0.01444649\n",
      "Iteration 189, loss = 0.01432332\n",
      "Iteration 190, loss = 0.01472325\n",
      "Iteration 191, loss = 0.01426470\n",
      "Iteration 192, loss = 0.01414754\n",
      "Iteration 193, loss = 0.01417284\n",
      "Iteration 194, loss = 0.01405875\n",
      "Iteration 195, loss = 0.01390040\n",
      "Iteration 196, loss = 0.01405090\n",
      "Iteration 197, loss = 0.01387247\n",
      "Iteration 198, loss = 0.01384551\n",
      "Iteration 199, loss = 0.01399604\n",
      "Iteration 200, loss = 0.01405964\n",
      "Iteration 201, loss = 0.01393954\n",
      "Iteration 202, loss = 0.01390180\n",
      "Iteration 203, loss = 0.01361401\n",
      "Iteration 204, loss = 0.01368320\n",
      "Iteration 205, loss = 0.01375083\n",
      "Iteration 206, loss = 0.01340381\n",
      "Iteration 207, loss = 0.01341205\n",
      "Iteration 208, loss = 0.01339394\n",
      "Iteration 209, loss = 0.01359846\n",
      "Iteration 210, loss = 0.01315188\n",
      "Iteration 211, loss = 0.01364389\n",
      "Iteration 212, loss = 0.01337164\n",
      "Iteration 213, loss = 0.01329608\n",
      "Iteration 214, loss = 0.01317253\n",
      "Iteration 215, loss = 0.01304988\n",
      "Iteration 216, loss = 0.01295883\n",
      "Iteration 217, loss = 0.01294158\n",
      "Iteration 218, loss = 0.01293744\n",
      "Iteration 219, loss = 0.01354198\n",
      "Iteration 220, loss = 0.01299275\n",
      "Iteration 221, loss = 0.01308450\n",
      "Iteration 222, loss = 0.01280999\n",
      "Iteration 223, loss = 0.01269799\n",
      "Iteration 224, loss = 0.01264568\n",
      "Iteration 225, loss = 0.01268424\n",
      "Iteration 226, loss = 0.01254752\n",
      "Iteration 227, loss = 0.01244868\n",
      "Iteration 228, loss = 0.01237807\n",
      "Iteration 229, loss = 0.01235889\n",
      "Iteration 230, loss = 0.01232580\n",
      "Iteration 231, loss = 0.01232430\n",
      "Iteration 232, loss = 0.01233004\n",
      "Iteration 233, loss = 0.01246060\n",
      "Iteration 234, loss = 0.01224435\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.47446727\n",
      "Iteration 2, loss = 0.37435558\n",
      "Iteration 3, loss = 0.29631671\n",
      "Iteration 4, loss = 0.24295393\n",
      "Iteration 5, loss = 0.21421973\n",
      "Iteration 6, loss = 0.19055714\n",
      "Iteration 7, loss = 0.17227457\n",
      "Iteration 8, loss = 0.15557553\n",
      "Iteration 9, loss = 0.14337059\n",
      "Iteration 10, loss = 0.13245082\n",
      "Iteration 11, loss = 0.12472523\n",
      "Iteration 12, loss = 0.11660901\n",
      "Iteration 13, loss = 0.10878904\n",
      "Iteration 14, loss = 0.10162794\n",
      "Iteration 15, loss = 0.09520607\n",
      "Iteration 16, loss = 0.09087796\n",
      "Iteration 17, loss = 0.08570818\n",
      "Iteration 18, loss = 0.08148735\n",
      "Iteration 19, loss = 0.07720048\n",
      "Iteration 20, loss = 0.07375678\n",
      "Iteration 21, loss = 0.07049780\n",
      "Iteration 22, loss = 0.06741165\n",
      "Iteration 23, loss = 0.06489313\n",
      "Iteration 24, loss = 0.06223404\n",
      "Iteration 25, loss = 0.06013247\n",
      "Iteration 26, loss = 0.05811742\n",
      "Iteration 27, loss = 0.05607552\n",
      "Iteration 28, loss = 0.05439610\n",
      "Iteration 29, loss = 0.05244848\n",
      "Iteration 30, loss = 0.05152880\n",
      "Iteration 31, loss = 0.04994284\n",
      "Iteration 32, loss = 0.04823985\n",
      "Iteration 33, loss = 0.04637679\n",
      "Iteration 34, loss = 0.04551665\n",
      "Iteration 35, loss = 0.04449314\n",
      "Iteration 36, loss = 0.04389562\n",
      "Iteration 37, loss = 0.04277400\n",
      "Iteration 38, loss = 0.04113344\n",
      "Iteration 39, loss = 0.04039223\n",
      "Iteration 40, loss = 0.03967492\n",
      "Iteration 41, loss = 0.03850868\n",
      "Iteration 42, loss = 0.03759476\n",
      "Iteration 43, loss = 0.03686487\n",
      "Iteration 44, loss = 0.03606532\n",
      "Iteration 45, loss = 0.03594424\n",
      "Iteration 46, loss = 0.03522576\n",
      "Iteration 47, loss = 0.03449495\n",
      "Iteration 48, loss = 0.03375902\n",
      "Iteration 49, loss = 0.03394998\n",
      "Iteration 50, loss = 0.03329889\n",
      "Iteration 51, loss = 0.03240766\n",
      "Iteration 52, loss = 0.03229150\n",
      "Iteration 53, loss = 0.03186236\n",
      "Iteration 54, loss = 0.03115168\n",
      "Iteration 55, loss = 0.03065764\n",
      "Iteration 56, loss = 0.03061344\n",
      "Iteration 57, loss = 0.03030161\n",
      "Iteration 58, loss = 0.03073513\n",
      "Iteration 59, loss = 0.03045981\n",
      "Iteration 60, loss = 0.02956791\n",
      "Iteration 61, loss = 0.02896326\n",
      "Iteration 62, loss = 0.02866101\n",
      "Iteration 63, loss = 0.02860665\n",
      "Iteration 64, loss = 0.02838653\n",
      "Iteration 65, loss = 0.02796439\n",
      "Iteration 66, loss = 0.02822784\n",
      "Iteration 67, loss = 0.02772281\n",
      "Iteration 68, loss = 0.02755794\n",
      "Iteration 69, loss = 0.02725205\n",
      "Iteration 70, loss = 0.02681547\n",
      "Iteration 71, loss = 0.02683125\n",
      "Iteration 72, loss = 0.02651627\n",
      "Iteration 73, loss = 0.02623941\n",
      "Iteration 74, loss = 0.02578825\n",
      "Iteration 75, loss = 0.02587519\n",
      "Iteration 76, loss = 0.02573370\n",
      "Iteration 77, loss = 0.02526914\n",
      "Iteration 78, loss = 0.02498186\n",
      "Iteration 79, loss = 0.02501324\n",
      "Iteration 80, loss = 0.02492902\n",
      "Iteration 81, loss = 0.02461509\n",
      "Iteration 82, loss = 0.02449843\n",
      "Iteration 83, loss = 0.02484854\n",
      "Iteration 84, loss = 0.02576957\n",
      "Iteration 85, loss = 0.02591998\n",
      "Iteration 86, loss = 0.02485539\n",
      "Iteration 87, loss = 0.02377444\n",
      "Iteration 88, loss = 0.02421343\n",
      "Iteration 89, loss = 0.02354812\n",
      "Iteration 90, loss = 0.02359724\n",
      "Iteration 91, loss = 0.02339709\n",
      "Iteration 92, loss = 0.02286086\n",
      "Iteration 93, loss = 0.02342959\n",
      "Iteration 94, loss = 0.02360096\n",
      "Iteration 95, loss = 0.02314523\n",
      "Iteration 96, loss = 0.02256391\n",
      "Iteration 97, loss = 0.02252134\n",
      "Iteration 98, loss = 0.02199506\n",
      "Iteration 99, loss = 0.02230127\n",
      "Iteration 100, loss = 0.02197545\n",
      "Iteration 101, loss = 0.02180440\n",
      "Iteration 102, loss = 0.02197418\n",
      "Iteration 103, loss = 0.02137973\n",
      "Iteration 104, loss = 0.02113748\n",
      "Iteration 105, loss = 0.02146597\n",
      "Iteration 106, loss = 0.02097467\n",
      "Iteration 107, loss = 0.02089985\n",
      "Iteration 108, loss = 0.02075692\n",
      "Iteration 109, loss = 0.02108004\n",
      "Iteration 110, loss = 0.02080872\n",
      "Iteration 111, loss = 0.02038010\n",
      "Iteration 112, loss = 0.02055246\n",
      "Iteration 113, loss = 0.02020600\n",
      "Iteration 114, loss = 0.01972529\n",
      "Iteration 115, loss = 0.01959375\n",
      "Iteration 116, loss = 0.01933689\n",
      "Iteration 117, loss = 0.01948596\n",
      "Iteration 118, loss = 0.01905066\n",
      "Iteration 119, loss = 0.01901591\n",
      "Iteration 120, loss = 0.01882898\n",
      "Iteration 121, loss = 0.01991159\n",
      "Iteration 122, loss = 0.02057188\n",
      "Iteration 123, loss = 0.01994699\n",
      "Iteration 124, loss = 0.01913017\n",
      "Iteration 125, loss = 0.01820517\n",
      "Iteration 126, loss = 0.01832485\n",
      "Iteration 127, loss = 0.01794360\n",
      "Iteration 128, loss = 0.01765693\n",
      "Iteration 129, loss = 0.01766512\n",
      "Iteration 130, loss = 0.01785218\n",
      "Iteration 131, loss = 0.01914633\n",
      "Iteration 132, loss = 0.01840152\n",
      "Iteration 133, loss = 0.01771331\n",
      "Iteration 134, loss = 0.01738337\n",
      "Iteration 135, loss = 0.01711465\n",
      "Iteration 136, loss = 0.01687866\n",
      "Iteration 137, loss = 0.01688591\n",
      "Iteration 138, loss = 0.01656693\n",
      "Iteration 139, loss = 0.01757578\n",
      "Iteration 140, loss = 0.01758274\n",
      "Iteration 141, loss = 0.01694980\n",
      "Iteration 142, loss = 0.01709520\n",
      "Iteration 143, loss = 0.01647523\n",
      "Iteration 144, loss = 0.01615176\n",
      "Iteration 145, loss = 0.01694302\n",
      "Iteration 146, loss = 0.01630880\n",
      "Iteration 147, loss = 0.01606700\n",
      "Iteration 148, loss = 0.01588063\n",
      "Iteration 149, loss = 0.01609941\n",
      "Iteration 150, loss = 0.01608656\n",
      "Iteration 151, loss = 0.01600372\n",
      "Iteration 152, loss = 0.01580480\n",
      "Iteration 153, loss = 0.01540321\n",
      "Iteration 154, loss = 0.01531420\n",
      "Iteration 155, loss = 0.01534909\n",
      "Iteration 156, loss = 0.01521145\n",
      "Iteration 157, loss = 0.01504296\n",
      "Iteration 158, loss = 0.01513589\n",
      "Iteration 159, loss = 0.01561437\n",
      "Iteration 160, loss = 0.01544780\n",
      "Iteration 161, loss = 0.01589561\n",
      "Iteration 162, loss = 0.01541610\n",
      "Iteration 163, loss = 0.01492473\n",
      "Iteration 164, loss = 0.01477861\n",
      "Iteration 165, loss = 0.01476306\n",
      "Iteration 166, loss = 0.01454121\n",
      "Iteration 167, loss = 0.01445592\n",
      "Iteration 168, loss = 0.01443014\n",
      "Iteration 169, loss = 0.01414910\n",
      "Iteration 170, loss = 0.01448647\n",
      "Iteration 171, loss = 0.01425367\n",
      "Iteration 172, loss = 0.01422977\n",
      "Iteration 173, loss = 0.01395830\n",
      "Iteration 174, loss = 0.01382927\n",
      "Iteration 175, loss = 0.01360166\n",
      "Iteration 176, loss = 0.01354271\n",
      "Iteration 177, loss = 0.01373609\n",
      "Iteration 178, loss = 0.01357143\n",
      "Iteration 179, loss = 0.01381501\n",
      "Iteration 180, loss = 0.01373661\n",
      "Iteration 181, loss = 0.01352463\n",
      "Iteration 182, loss = 0.01365330\n",
      "Iteration 183, loss = 0.01377621\n",
      "Iteration 184, loss = 0.01349980\n",
      "Iteration 185, loss = 0.01345215\n",
      "Iteration 186, loss = 0.01331227\n",
      "Iteration 187, loss = 0.01358957\n",
      "Iteration 188, loss = 0.01334020\n",
      "Iteration 189, loss = 0.01298285\n",
      "Iteration 190, loss = 0.01309081\n",
      "Iteration 191, loss = 0.01285112\n",
      "Iteration 192, loss = 0.01300082\n",
      "Iteration 193, loss = 0.01326608\n",
      "Iteration 194, loss = 0.01349514\n",
      "Iteration 195, loss = 0.01337464\n",
      "Iteration 196, loss = 0.01432680\n",
      "Iteration 197, loss = 0.01388818\n",
      "Iteration 198, loss = 0.01317107\n",
      "Iteration 199, loss = 0.01278927\n",
      "Iteration 200, loss = 0.01266773\n",
      "Iteration 201, loss = 0.01268050\n",
      "Iteration 202, loss = 0.01274768\n",
      "Iteration 203, loss = 0.01467279\n",
      "Iteration 204, loss = 0.01403298\n",
      "Iteration 205, loss = 0.01303324\n",
      "Iteration 206, loss = 0.01370949\n",
      "Iteration 207, loss = 0.01309230\n",
      "Iteration 208, loss = 0.01271547\n",
      "Iteration 209, loss = 0.01273814\n",
      "Iteration 210, loss = 0.01222465\n",
      "Iteration 211, loss = 0.01274183\n",
      "Iteration 212, loss = 0.01242379\n",
      "Iteration 213, loss = 0.01246789\n",
      "Iteration 214, loss = 0.01249415\n",
      "Iteration 215, loss = 0.01229471\n",
      "Iteration 216, loss = 0.01228078\n",
      "Iteration 217, loss = 0.01208495\n",
      "Iteration 218, loss = 0.01215202\n",
      "Iteration 219, loss = 0.01187977\n",
      "Iteration 220, loss = 0.01199287\n",
      "Iteration 221, loss = 0.01254821\n",
      "Iteration 222, loss = 0.01301345\n",
      "Iteration 223, loss = 0.01394560\n",
      "Iteration 224, loss = 0.01326085\n",
      "Iteration 225, loss = 0.01249456\n",
      "Iteration 226, loss = 0.01219831\n",
      "Iteration 227, loss = 0.01186083\n",
      "Iteration 228, loss = 0.01186178\n",
      "Iteration 229, loss = 0.01160055\n",
      "Iteration 230, loss = 0.01164527\n",
      "Iteration 231, loss = 0.01234105\n",
      "Iteration 232, loss = 0.01200852\n",
      "Iteration 233, loss = 0.01263726\n",
      "Iteration 234, loss = 0.01223854\n",
      "Iteration 235, loss = 0.01204780\n",
      "Iteration 236, loss = 0.01153478\n",
      "Iteration 237, loss = 0.01163199\n",
      "Iteration 238, loss = 0.01202715\n",
      "Iteration 239, loss = 0.01234271\n",
      "Iteration 240, loss = 0.01196290\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.47446727\n",
      "Iteration 2, loss = 0.37435558\n",
      "Iteration 3, loss = 0.29631671\n",
      "Iteration 4, loss = 0.24295393\n",
      "Iteration 5, loss = 0.21421973\n",
      "Iteration 6, loss = 0.19055714\n",
      "Iteration 7, loss = 0.17227457\n",
      "Iteration 8, loss = 0.15557553\n",
      "Iteration 9, loss = 0.14337059\n",
      "Iteration 10, loss = 0.13245082\n",
      "Iteration 11, loss = 0.12472523\n",
      "Iteration 12, loss = 0.11660901\n",
      "Iteration 13, loss = 0.10878904\n",
      "Iteration 14, loss = 0.10162794\n",
      "Iteration 15, loss = 0.09520607\n",
      "Iteration 16, loss = 0.09087796\n",
      "Iteration 17, loss = 0.08570818\n",
      "Iteration 18, loss = 0.08148735\n",
      "Iteration 19, loss = 0.07720048\n",
      "Iteration 20, loss = 0.07375678\n",
      "Iteration 21, loss = 0.07049780\n",
      "Iteration 22, loss = 0.06741165\n",
      "Iteration 23, loss = 0.06489313\n",
      "Iteration 24, loss = 0.06223404\n",
      "Iteration 25, loss = 0.06013247\n",
      "Iteration 26, loss = 0.05811742\n",
      "Iteration 27, loss = 0.05607552\n",
      "Iteration 28, loss = 0.05439610\n",
      "Iteration 29, loss = 0.05244848\n",
      "Iteration 30, loss = 0.05152880\n",
      "Iteration 31, loss = 0.04994284\n",
      "Iteration 32, loss = 0.04823985\n",
      "Iteration 33, loss = 0.04637679\n",
      "Iteration 34, loss = 0.04551665\n",
      "Iteration 35, loss = 0.04449314\n",
      "Iteration 36, loss = 0.04389562\n",
      "Iteration 37, loss = 0.04277400\n",
      "Iteration 38, loss = 0.04113344\n",
      "Iteration 39, loss = 0.04039223\n",
      "Iteration 40, loss = 0.03967492\n",
      "Iteration 41, loss = 0.03850868\n",
      "Iteration 42, loss = 0.03759476\n",
      "Iteration 43, loss = 0.03686487\n",
      "Iteration 44, loss = 0.03606532\n",
      "Iteration 45, loss = 0.03594424\n",
      "Iteration 46, loss = 0.03522576\n",
      "Iteration 47, loss = 0.03449495\n",
      "Iteration 48, loss = 0.03375902\n",
      "Iteration 49, loss = 0.03394998\n",
      "Iteration 50, loss = 0.03329889\n",
      "Iteration 51, loss = 0.03240766\n",
      "Iteration 52, loss = 0.03229150\n",
      "Iteration 53, loss = 0.03186236\n",
      "Iteration 54, loss = 0.03115168\n",
      "Iteration 55, loss = 0.03065764\n",
      "Iteration 56, loss = 0.03061344\n",
      "Iteration 57, loss = 0.03030161\n",
      "Iteration 58, loss = 0.03073513\n",
      "Iteration 59, loss = 0.03045981\n",
      "Iteration 60, loss = 0.02956791\n",
      "Iteration 61, loss = 0.02896326\n",
      "Iteration 62, loss = 0.02866101\n",
      "Iteration 63, loss = 0.02860665\n",
      "Iteration 64, loss = 0.02838653\n",
      "Iteration 65, loss = 0.02796439\n",
      "Iteration 66, loss = 0.02822784\n",
      "Iteration 67, loss = 0.02772281\n",
      "Iteration 68, loss = 0.02755794\n",
      "Iteration 69, loss = 0.02725205\n",
      "Iteration 70, loss = 0.02681547\n",
      "Iteration 71, loss = 0.02683125\n",
      "Iteration 72, loss = 0.02651627\n",
      "Iteration 73, loss = 0.02623941\n",
      "Iteration 74, loss = 0.02578825\n",
      "Iteration 75, loss = 0.02587519\n",
      "Iteration 76, loss = 0.02573370\n",
      "Iteration 77, loss = 0.02526914\n",
      "Iteration 78, loss = 0.02498186\n",
      "Iteration 79, loss = 0.02501324\n",
      "Iteration 80, loss = 0.02492902\n",
      "Iteration 81, loss = 0.02461509\n",
      "Iteration 82, loss = 0.02449843\n",
      "Iteration 83, loss = 0.02484854\n",
      "Iteration 84, loss = 0.02576957\n",
      "Iteration 85, loss = 0.02591998\n",
      "Iteration 86, loss = 0.02485539\n",
      "Iteration 87, loss = 0.02377444\n",
      "Iteration 88, loss = 0.02421343\n",
      "Iteration 89, loss = 0.02354812\n",
      "Iteration 90, loss = 0.02359724\n",
      "Iteration 91, loss = 0.02339709\n",
      "Iteration 92, loss = 0.02286086\n",
      "Iteration 93, loss = 0.02342959\n",
      "Iteration 94, loss = 0.02360096\n",
      "Iteration 95, loss = 0.02314523\n",
      "Iteration 96, loss = 0.02256391\n",
      "Iteration 97, loss = 0.02252134\n",
      "Iteration 98, loss = 0.02199506\n",
      "Iteration 99, loss = 0.02230127\n",
      "Iteration 100, loss = 0.02197545\n",
      "Iteration 101, loss = 0.02180440\n",
      "Iteration 102, loss = 0.02197418\n",
      "Iteration 103, loss = 0.02137973\n",
      "Iteration 104, loss = 0.02113748\n",
      "Iteration 105, loss = 0.02146597\n",
      "Iteration 106, loss = 0.02097467\n",
      "Iteration 107, loss = 0.02089985\n",
      "Iteration 108, loss = 0.02075692\n",
      "Iteration 109, loss = 0.02108004\n",
      "Iteration 110, loss = 0.02080872\n",
      "Iteration 111, loss = 0.02038010\n",
      "Iteration 112, loss = 0.02055246\n",
      "Iteration 113, loss = 0.02020600\n",
      "Iteration 114, loss = 0.01972529\n",
      "Iteration 115, loss = 0.01959375\n",
      "Iteration 116, loss = 0.01933689\n",
      "Iteration 117, loss = 0.01948596\n",
      "Iteration 118, loss = 0.01905066\n",
      "Iteration 119, loss = 0.01901591\n",
      "Iteration 120, loss = 0.01882898\n",
      "Iteration 121, loss = 0.01991159\n",
      "Iteration 122, loss = 0.02057188\n",
      "Iteration 123, loss = 0.01994699\n",
      "Iteration 124, loss = 0.01913017\n",
      "Iteration 125, loss = 0.01820517\n",
      "Iteration 126, loss = 0.01832485\n",
      "Iteration 127, loss = 0.01794360\n",
      "Iteration 128, loss = 0.01765693\n",
      "Iteration 129, loss = 0.01766512\n",
      "Iteration 130, loss = 0.01785218\n",
      "Iteration 131, loss = 0.01914633\n",
      "Iteration 132, loss = 0.01840152\n",
      "Iteration 133, loss = 0.01771331\n",
      "Iteration 134, loss = 0.01738337\n",
      "Iteration 135, loss = 0.01711465\n",
      "Iteration 136, loss = 0.01687866\n",
      "Iteration 137, loss = 0.01688591\n",
      "Iteration 138, loss = 0.01656693\n",
      "Iteration 139, loss = 0.01757578\n",
      "Iteration 140, loss = 0.01758274\n",
      "Iteration 141, loss = 0.01694980\n",
      "Iteration 142, loss = 0.01709520\n",
      "Iteration 143, loss = 0.01647523\n",
      "Iteration 144, loss = 0.01615176\n",
      "Iteration 145, loss = 0.01694302\n",
      "Iteration 146, loss = 0.01630880\n",
      "Iteration 147, loss = 0.01606700\n",
      "Iteration 148, loss = 0.01588063\n",
      "Iteration 149, loss = 0.01609941\n",
      "Iteration 150, loss = 0.01608656\n",
      "Iteration 151, loss = 0.01600372\n",
      "Iteration 152, loss = 0.01580480\n",
      "Iteration 153, loss = 0.01540321\n",
      "Iteration 154, loss = 0.01531420\n",
      "Iteration 155, loss = 0.01534909\n",
      "Iteration 156, loss = 0.01521145\n",
      "Iteration 157, loss = 0.01504296\n",
      "Iteration 158, loss = 0.01513589\n",
      "Iteration 159, loss = 0.01561437\n",
      "Iteration 160, loss = 0.01544780\n",
      "Iteration 161, loss = 0.01589561\n",
      "Iteration 162, loss = 0.01541610\n",
      "Iteration 163, loss = 0.01492473\n",
      "Iteration 164, loss = 0.01477861\n",
      "Iteration 165, loss = 0.01476306\n",
      "Iteration 166, loss = 0.01454121\n",
      "Iteration 167, loss = 0.01445592\n",
      "Iteration 168, loss = 0.01443014\n",
      "Iteration 169, loss = 0.01414910\n",
      "Iteration 170, loss = 0.01448647\n",
      "Iteration 171, loss = 0.01425367\n",
      "Iteration 172, loss = 0.01422977\n",
      "Iteration 173, loss = 0.01395830\n",
      "Iteration 174, loss = 0.01382927\n",
      "Iteration 175, loss = 0.01360166\n",
      "Iteration 176, loss = 0.01354271\n",
      "Iteration 177, loss = 0.01373609\n",
      "Iteration 178, loss = 0.01357143\n",
      "Iteration 179, loss = 0.01381501\n",
      "Iteration 180, loss = 0.01373661\n",
      "Iteration 181, loss = 0.01352463\n",
      "Iteration 182, loss = 0.01365330\n",
      "Iteration 183, loss = 0.01377621\n",
      "Iteration 184, loss = 0.01349980\n",
      "Iteration 185, loss = 0.01345215\n",
      "Iteration 186, loss = 0.01331227\n",
      "Iteration 187, loss = 0.01358957\n",
      "Iteration 188, loss = 0.01334020\n",
      "Iteration 189, loss = 0.01298285\n",
      "Iteration 190, loss = 0.01309081\n",
      "Iteration 191, loss = 0.01285112\n",
      "Iteration 192, loss = 0.01300082\n",
      "Iteration 193, loss = 0.01326608\n",
      "Iteration 194, loss = 0.01349514\n",
      "Iteration 195, loss = 0.01337464\n",
      "Iteration 196, loss = 0.01432680\n",
      "Iteration 197, loss = 0.01388818\n",
      "Iteration 198, loss = 0.01317107\n",
      "Iteration 199, loss = 0.01278927\n",
      "Iteration 200, loss = 0.01266773\n",
      "Iteration 201, loss = 0.01268050\n",
      "Iteration 202, loss = 0.01274768\n",
      "Iteration 203, loss = 0.01467279\n",
      "Iteration 204, loss = 0.01403298\n",
      "Iteration 205, loss = 0.01303324\n",
      "Iteration 206, loss = 0.01370949\n",
      "Iteration 207, loss = 0.01309230\n",
      "Iteration 208, loss = 0.01271547\n",
      "Iteration 209, loss = 0.01273814\n",
      "Iteration 210, loss = 0.01222465\n",
      "Iteration 211, loss = 0.01274183\n",
      "Iteration 212, loss = 0.01242379\n",
      "Iteration 213, loss = 0.01246789\n",
      "Iteration 214, loss = 0.01249415\n",
      "Iteration 215, loss = 0.01229471\n",
      "Iteration 216, loss = 0.01228078\n",
      "Iteration 217, loss = 0.01208495\n",
      "Iteration 218, loss = 0.01215202\n",
      "Iteration 219, loss = 0.01187977\n",
      "Iteration 220, loss = 0.01199287\n",
      "Iteration 221, loss = 0.01254821\n",
      "Iteration 222, loss = 0.01301345\n",
      "Iteration 223, loss = 0.01394560\n",
      "Iteration 224, loss = 0.01326085\n",
      "Iteration 225, loss = 0.01249456\n",
      "Iteration 226, loss = 0.01219831\n",
      "Iteration 227, loss = 0.01186083\n",
      "Iteration 228, loss = 0.01186178\n",
      "Iteration 229, loss = 0.01160055\n",
      "Iteration 230, loss = 0.01164527\n",
      "Iteration 231, loss = 0.01234105\n",
      "Iteration 232, loss = 0.01200852\n",
      "Iteration 233, loss = 0.01263726\n",
      "Iteration 234, loss = 0.01223854\n",
      "Iteration 235, loss = 0.01204780\n",
      "Iteration 236, loss = 0.01153478\n",
      "Iteration 237, loss = 0.01163199\n",
      "Iteration 238, loss = 0.01202715\n",
      "Iteration 239, loss = 0.01234271\n",
      "Iteration 240, loss = 0.01196290\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.47446727\n",
      "Iteration 2, loss = 0.37435558\n",
      "Iteration 3, loss = 0.29631671\n",
      "Iteration 4, loss = 0.24295393\n",
      "Iteration 5, loss = 0.21421973\n",
      "Iteration 6, loss = 0.19055714\n",
      "Iteration 7, loss = 0.17227457\n",
      "Iteration 8, loss = 0.15557553\n",
      "Iteration 9, loss = 0.14337059\n",
      "Iteration 10, loss = 0.13245082\n",
      "Iteration 11, loss = 0.12472523\n",
      "Iteration 12, loss = 0.11660901\n",
      "Iteration 13, loss = 0.10878904\n",
      "Iteration 14, loss = 0.10162794\n",
      "Iteration 15, loss = 0.09520607\n",
      "Iteration 16, loss = 0.09087796\n",
      "Iteration 17, loss = 0.08570818\n",
      "Iteration 18, loss = 0.08148735\n",
      "Iteration 19, loss = 0.07720048\n",
      "Iteration 20, loss = 0.07375678\n",
      "Iteration 21, loss = 0.07049780\n",
      "Iteration 22, loss = 0.06741165\n",
      "Iteration 23, loss = 0.06489313\n",
      "Iteration 24, loss = 0.06223404\n",
      "Iteration 25, loss = 0.06013247\n",
      "Iteration 26, loss = 0.05811742\n",
      "Iteration 27, loss = 0.05607552\n",
      "Iteration 28, loss = 0.05439610\n",
      "Iteration 29, loss = 0.05244848\n",
      "Iteration 30, loss = 0.05152880\n",
      "Iteration 31, loss = 0.04994284\n",
      "Iteration 32, loss = 0.04823985\n",
      "Iteration 33, loss = 0.04637679\n",
      "Iteration 34, loss = 0.04551665\n",
      "Iteration 35, loss = 0.04449314\n",
      "Iteration 36, loss = 0.04389562\n",
      "Iteration 37, loss = 0.04277400\n",
      "Iteration 38, loss = 0.04113344\n",
      "Iteration 39, loss = 0.04039223\n",
      "Iteration 40, loss = 0.03967492\n",
      "Iteration 41, loss = 0.03850868\n",
      "Iteration 42, loss = 0.03759476\n",
      "Iteration 43, loss = 0.03686487\n",
      "Iteration 44, loss = 0.03606532\n",
      "Iteration 45, loss = 0.03594424\n",
      "Iteration 46, loss = 0.03522576\n",
      "Iteration 47, loss = 0.03449495\n",
      "Iteration 48, loss = 0.03375902\n",
      "Iteration 49, loss = 0.03394998\n",
      "Iteration 50, loss = 0.03329889\n",
      "Iteration 51, loss = 0.03240766\n",
      "Iteration 52, loss = 0.03229150\n",
      "Iteration 53, loss = 0.03186236\n",
      "Iteration 54, loss = 0.03115168\n",
      "Iteration 55, loss = 0.03065764\n",
      "Iteration 56, loss = 0.03061344\n",
      "Iteration 57, loss = 0.03030161\n",
      "Iteration 58, loss = 0.03073513\n",
      "Iteration 59, loss = 0.03045981\n",
      "Iteration 60, loss = 0.02956791\n",
      "Iteration 61, loss = 0.02896326\n",
      "Iteration 62, loss = 0.02866101\n",
      "Iteration 63, loss = 0.02860665\n",
      "Iteration 64, loss = 0.02838653\n",
      "Iteration 65, loss = 0.02796439\n",
      "Iteration 66, loss = 0.02822784\n",
      "Iteration 67, loss = 0.02772281\n",
      "Iteration 68, loss = 0.02755794\n",
      "Iteration 69, loss = 0.02725205\n",
      "Iteration 70, loss = 0.02681547\n",
      "Iteration 71, loss = 0.02683125\n",
      "Iteration 72, loss = 0.02651627\n",
      "Iteration 73, loss = 0.02623941\n",
      "Iteration 74, loss = 0.02578825\n",
      "Iteration 75, loss = 0.02587519\n",
      "Iteration 76, loss = 0.02573370\n",
      "Iteration 77, loss = 0.02526914\n",
      "Iteration 78, loss = 0.02498186\n",
      "Iteration 79, loss = 0.02501324\n",
      "Iteration 80, loss = 0.02492902\n",
      "Iteration 81, loss = 0.02461509\n",
      "Iteration 82, loss = 0.02449843\n",
      "Iteration 83, loss = 0.02484854\n",
      "Iteration 84, loss = 0.02576957\n",
      "Iteration 85, loss = 0.02591998\n",
      "Iteration 86, loss = 0.02485539\n",
      "Iteration 87, loss = 0.02377444\n",
      "Iteration 88, loss = 0.02421343\n",
      "Iteration 89, loss = 0.02354812\n",
      "Iteration 90, loss = 0.02359724\n",
      "Iteration 91, loss = 0.02339709\n",
      "Iteration 92, loss = 0.02286086\n",
      "Iteration 93, loss = 0.02342959\n",
      "Iteration 94, loss = 0.02360096\n",
      "Iteration 95, loss = 0.02314523\n",
      "Iteration 96, loss = 0.02256391\n",
      "Iteration 97, loss = 0.02252134\n",
      "Iteration 98, loss = 0.02199506\n",
      "Iteration 99, loss = 0.02230127\n",
      "Iteration 100, loss = 0.02197545\n",
      "Iteration 101, loss = 0.02180440\n",
      "Iteration 102, loss = 0.02197418\n",
      "Iteration 103, loss = 0.02137973\n",
      "Iteration 104, loss = 0.02113748\n",
      "LinearRegression\n",
      "NeuralNetwork\n",
      "Iteration 105, loss = 0.02146597\n",
      "Iteration 106, loss = 0.02097467\n",
      "Iteration 107, loss = 0.02089985\n",
      "Iteration 108, loss = 0.02075692\n",
      "Iteration 109, loss = 0.02108004\n",
      "Iteration 110, loss = 0.02080872\n",
      "Iteration 111, loss = 0.02038010\n",
      "Iteration 112, loss = 0.02055246\n",
      "Iteration 113, loss = 0.02020600\n",
      "Iteration 114, loss = 0.01972529\n",
      "Iteration 115, loss = 0.01959375\n",
      "Iteration 116, loss = 0.01933689\n",
      "Iteration 117, loss = 0.01948596\n",
      "Iteration 118, loss = 0.01905066\n",
      "Iteration 119, loss = 0.01901591\n",
      "Iteration 120, loss = 0.01882898\n",
      "Iteration 121, loss = 0.01991159\n",
      "Iteration 122, loss = 0.02057188\n",
      "Iteration 123, loss = 0.01994699\n",
      "Iteration 124, loss = 0.01913017\n",
      "Iteration 125, loss = 0.01820517\n",
      "Iteration 126, loss = 0.01832485\n",
      "Iteration 127, loss = 0.01794360\n",
      "Iteration 128, loss = 0.01765693\n",
      "Iteration 129, loss = 0.01766512\n",
      "Iteration 130, loss = 0.01785218\n",
      "Iteration 131, loss = 0.01914633\n",
      "Iteration 132, loss = 0.01840152\n",
      "Iteration 133, loss = 0.01771331\n",
      "Iteration 134, loss = 0.01738337\n",
      "Iteration 135, loss = 0.01711465\n",
      "Iteration 136, loss = 0.01687866\n",
      "Iteration 137, loss = 0.01688591\n",
      "Iteration 138, loss = 0.01656693\n",
      "Iteration 139, loss = 0.01757578\n",
      "Iteration 140, loss = 0.01758274\n",
      "Iteration 141, loss = 0.01694980\n",
      "Iteration 142, loss = 0.01709520\n",
      "Iteration 143, loss = 0.01647523\n",
      "Iteration 144, loss = 0.01615176\n",
      "Iteration 145, loss = 0.01694302\n",
      "Iteration 146, loss = 0.01630880\n",
      "Iteration 147, loss = 0.01606700\n",
      "Iteration 148, loss = 0.01588063\n",
      "Iteration 149, loss = 0.01609941\n",
      "Iteration 150, loss = 0.01608656\n",
      "Iteration 151, loss = 0.01600372\n",
      "Iteration 152, loss = 0.01580480\n",
      "Iteration 153, loss = 0.01540321\n",
      "Iteration 154, loss = 0.01531420\n",
      "Iteration 155, loss = 0.01534909\n",
      "Iteration 156, loss = 0.01521145\n",
      "Iteration 157, loss = 0.01504296\n",
      "Iteration 158, loss = 0.01513589\n",
      "Iteration 159, loss = 0.01561437\n",
      "Iteration 160, loss = 0.01544780\n",
      "Iteration 161, loss = 0.01589561\n",
      "Iteration 162, loss = 0.01541610\n",
      "Iteration 163, loss = 0.01492473\n",
      "Iteration 164, loss = 0.01477861\n",
      "Iteration 165, loss = 0.01476306\n",
      "Iteration 166, loss = 0.01454121\n",
      "Iteration 167, loss = 0.01445592\n",
      "Iteration 168, loss = 0.01443014\n",
      "Iteration 169, loss = 0.01414910\n",
      "Iteration 170, loss = 0.01448647\n",
      "Iteration 171, loss = 0.01425367\n",
      "Iteration 172, loss = 0.01422977\n",
      "Iteration 173, loss = 0.01395830\n",
      "Iteration 174, loss = 0.01382927\n",
      "Iteration 175, loss = 0.01360166\n",
      "Iteration 176, loss = 0.01354271\n",
      "Iteration 177, loss = 0.01373609\n",
      "Iteration 178, loss = 0.01357143\n",
      "Iteration 179, loss = 0.01381501\n",
      "Iteration 180, loss = 0.01373661\n",
      "Iteration 181, loss = 0.01352463\n",
      "Iteration 182, loss = 0.01365330\n",
      "Iteration 183, loss = 0.01377621\n",
      "Iteration 184, loss = 0.01349980\n",
      "Iteration 185, loss = 0.01345215\n",
      "Iteration 186, loss = 0.01331227\n",
      "Iteration 187, loss = 0.01358957\n",
      "Iteration 188, loss = 0.01334020\n",
      "Iteration 189, loss = 0.01298285\n",
      "Iteration 190, loss = 0.01309081\n",
      "Iteration 191, loss = 0.01285112\n",
      "Iteration 192, loss = 0.01300082\n",
      "Iteration 193, loss = 0.01326608\n",
      "Iteration 194, loss = 0.01349514\n",
      "Iteration 195, loss = 0.01337464\n",
      "Iteration 196, loss = 0.01432680\n",
      "Iteration 197, loss = 0.01388818\n",
      "Iteration 198, loss = 0.01317107\n",
      "Iteration 199, loss = 0.01278927\n",
      "Iteration 200, loss = 0.01266773\n",
      "Iteration 201, loss = 0.01268050\n",
      "Iteration 202, loss = 0.01274768\n",
      "Iteration 203, loss = 0.01467279\n",
      "Iteration 204, loss = 0.01403298\n",
      "Iteration 205, loss = 0.01303324\n",
      "Iteration 206, loss = 0.01370949\n",
      "Iteration 207, loss = 0.01309230\n",
      "Iteration 208, loss = 0.01271547\n",
      "Iteration 209, loss = 0.01273814\n",
      "Iteration 210, loss = 0.01222465\n",
      "Iteration 211, loss = 0.01274183\n",
      "Iteration 212, loss = 0.01242379\n",
      "Iteration 213, loss = 0.01246789\n",
      "Iteration 214, loss = 0.01249415\n",
      "Iteration 215, loss = 0.01229471\n",
      "Iteration 216, loss = 0.01228078\n",
      "Iteration 217, loss = 0.01208495\n",
      "Iteration 218, loss = 0.01215202\n",
      "Iteration 219, loss = 0.01187977\n",
      "Iteration 220, loss = 0.01199287\n",
      "Iteration 221, loss = 0.01254821\n",
      "Iteration 222, loss = 0.01301345\n",
      "Iteration 223, loss = 0.01394560\n",
      "Iteration 224, loss = 0.01326085\n",
      "Iteration 225, loss = 0.01249456\n",
      "Iteration 226, loss = 0.01219831\n",
      "Iteration 227, loss = 0.01186083\n",
      "Iteration 228, loss = 0.01186178\n",
      "Iteration 229, loss = 0.01160055\n",
      "Iteration 230, loss = 0.01164527\n",
      "Iteration 231, loss = 0.01234105\n",
      "Iteration 232, loss = 0.01200852\n",
      "Iteration 233, loss = 0.01263726\n",
      "Iteration 234, loss = 0.01223854\n",
      "Iteration 235, loss = 0.01204780\n",
      "Iteration 236, loss = 0.01153478\n",
      "Iteration 237, loss = 0.01163199\n",
      "Iteration 238, loss = 0.01202715\n",
      "Iteration 239, loss = 0.01234271\n",
      "Iteration 240, loss = 0.01196290\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46324851\n",
      "Iteration 2, loss = 0.35941492\n",
      "Iteration 3, loss = 0.28550739\n",
      "Iteration 4, loss = 0.23944089\n",
      "Iteration 5, loss = 0.20635895\n",
      "Iteration 6, loss = 0.18216356\n",
      "Iteration 7, loss = 0.16549971\n",
      "Iteration 8, loss = 0.15166451\n",
      "Iteration 9, loss = 0.14053606\n",
      "Iteration 10, loss = 0.13243902\n",
      "Iteration 11, loss = 0.12371684\n",
      "Iteration 12, loss = 0.11471210\n",
      "Iteration 13, loss = 0.10628999\n",
      "Iteration 14, loss = 0.09906424\n",
      "Iteration 15, loss = 0.09262482\n",
      "Iteration 16, loss = 0.08591889\n",
      "Iteration 17, loss = 0.08094528\n",
      "Iteration 18, loss = 0.07659169\n",
      "Iteration 19, loss = 0.07258639\n",
      "Iteration 20, loss = 0.06950486\n",
      "Iteration 21, loss = 0.06635924\n",
      "Iteration 22, loss = 0.06367644\n",
      "Iteration 23, loss = 0.06232014\n",
      "Iteration 24, loss = 0.05970747\n",
      "Iteration 25, loss = 0.05798394\n",
      "Iteration 26, loss = 0.05688184\n",
      "Iteration 27, loss = 0.05514188\n",
      "Iteration 28, loss = 0.05330126\n",
      "Iteration 29, loss = 0.05188316\n",
      "Iteration 30, loss = 0.05072541\n",
      "Iteration 31, loss = 0.04942744\n",
      "Iteration 32, loss = 0.04789565\n",
      "Iteration 33, loss = 0.04673036\n",
      "Iteration 34, loss = 0.04624678\n",
      "Iteration 35, loss = 0.04547219\n",
      "Iteration 36, loss = 0.04395954\n",
      "Iteration 37, loss = 0.04305856\n",
      "Iteration 38, loss = 0.04223179\n",
      "Iteration 39, loss = 0.04154218\n",
      "Iteration 40, loss = 0.04030411\n",
      "Iteration 41, loss = 0.03955255\n",
      "Iteration 42, loss = 0.03934948\n",
      "Iteration 43, loss = 0.03838548\n",
      "Iteration 44, loss = 0.03770689\n",
      "Iteration 45, loss = 0.03745365\n",
      "Iteration 46, loss = 0.03609782\n",
      "Iteration 47, loss = 0.03579298\n",
      "Iteration 48, loss = 0.03468762\n",
      "Iteration 49, loss = 0.03409048\n",
      "Iteration 50, loss = 0.03331079\n",
      "Iteration 51, loss = 0.03297208\n",
      "Iteration 52, loss = 0.03214776\n",
      "Iteration 53, loss = 0.03173763\n",
      "Iteration 54, loss = 0.03107871\n",
      "Iteration 55, loss = 0.03062330\n",
      "Iteration 56, loss = 0.03031322\n",
      "Iteration 57, loss = 0.02974215\n",
      "Iteration 58, loss = 0.02909181\n",
      "Iteration 59, loss = 0.02965139\n",
      "Iteration 60, loss = 0.02865485\n",
      "Iteration 61, loss = 0.02821170\n",
      "Iteration 62, loss = 0.02750707\n",
      "Iteration 63, loss = 0.02779641\n",
      "Iteration 64, loss = 0.02651792\n",
      "Iteration 65, loss = 0.02662533\n",
      "Iteration 66, loss = 0.02605479\n",
      "Iteration 67, loss = 0.02547792\n",
      "Iteration 68, loss = 0.02540315\n",
      "Iteration 69, loss = 0.02496851\n",
      "Iteration 70, loss = 0.02434972\n",
      "Iteration 71, loss = 0.02407528\n",
      "Iteration 72, loss = 0.02364984\n",
      "Iteration 73, loss = 0.02338867\n",
      "Iteration 74, loss = 0.02318713\n",
      "Iteration 75, loss = 0.02345367\n",
      "Iteration 76, loss = 0.02266810\n",
      "Iteration 77, loss = 0.02264495\n",
      "Iteration 78, loss = 0.02222104\n",
      "Iteration 79, loss = 0.02211856\n",
      "Iteration 80, loss = 0.02159582\n",
      "Iteration 81, loss = 0.02133303\n",
      "Iteration 82, loss = 0.02095061\n",
      "Iteration 83, loss = 0.02101477\n",
      "Iteration 84, loss = 0.02063753\n",
      "Iteration 85, loss = 0.02040359\n",
      "Iteration 86, loss = 0.02020878\n",
      "Iteration 87, loss = 0.01989889\n",
      "Iteration 88, loss = 0.01976419\n",
      "Iteration 89, loss = 0.01937072\n",
      "Iteration 90, loss = 0.01898773\n",
      "Iteration 91, loss = 0.01899472\n",
      "Iteration 92, loss = 0.01896070\n",
      "Iteration 93, loss = 0.01861137\n",
      "Iteration 94, loss = 0.01832371\n",
      "Iteration 95, loss = 0.01812335\n",
      "Iteration 96, loss = 0.01799307\n",
      "Iteration 97, loss = 0.01773480\n",
      "Iteration 98, loss = 0.01791535\n",
      "Iteration 99, loss = 0.01730416\n",
      "Iteration 100, loss = 0.01730285\n",
      "Iteration 101, loss = 0.01711971\n",
      "Iteration 102, loss = 0.01721377\n",
      "Iteration 103, loss = 0.01666462\n",
      "Iteration 104, loss = 0.01687832\n",
      "Iteration 105, loss = 0.01624310\n",
      "Iteration 106, loss = 0.01602981\n",
      "Iteration 107, loss = 0.01629150\n",
      "Iteration 108, loss = 0.01613517\n",
      "Iteration 109, loss = 0.01571558\n",
      "Iteration 110, loss = 0.01596724\n",
      "Iteration 111, loss = 0.01544355\n",
      "Iteration 112, loss = 0.01529804\n",
      "Iteration 113, loss = 0.01523076\n",
      "Iteration 114, loss = 0.01531225\n",
      "Iteration 115, loss = 0.01480754\n",
      "Iteration 116, loss = 0.01489574\n",
      "Iteration 117, loss = 0.01460659\n",
      "Iteration 118, loss = 0.01452254\n",
      "Iteration 119, loss = 0.01427482\n",
      "Iteration 120, loss = 0.01416487\n",
      "Iteration 121, loss = 0.01441214\n",
      "Iteration 122, loss = 0.01403859\n",
      "Iteration 123, loss = 0.01437750\n",
      "Iteration 124, loss = 0.01394925\n",
      "Iteration 125, loss = 0.01399405\n",
      "Iteration 126, loss = 0.01365323\n",
      "Iteration 127, loss = 0.01337614\n",
      "Iteration 128, loss = 0.01335382\n",
      "Iteration 129, loss = 0.01345683\n",
      "Iteration 130, loss = 0.01313389\n",
      "Iteration 131, loss = 0.01300321\n",
      "Iteration 132, loss = 0.01290836\n",
      "Iteration 133, loss = 0.01272115\n",
      "Iteration 134, loss = 0.01257667\n",
      "Iteration 135, loss = 0.01281937\n",
      "Iteration 136, loss = 0.01229669\n",
      "Iteration 137, loss = 0.01276218\n",
      "Iteration 138, loss = 0.01232440\n",
      "Iteration 139, loss = 0.01218109\n",
      "Iteration 140, loss = 0.01255270\n",
      "Iteration 141, loss = 0.01197703\n",
      "Iteration 142, loss = 0.01223553\n",
      "Iteration 143, loss = 0.01201204\n",
      "Iteration 144, loss = 0.01182596\n",
      "Iteration 145, loss = 0.01172901\n",
      "Iteration 146, loss = 0.01166659\n",
      "Iteration 147, loss = 0.01167208\n",
      "Iteration 148, loss = 0.01131282\n",
      "Iteration 149, loss = 0.01123023\n",
      "Iteration 150, loss = 0.01116755\n",
      "Iteration 151, loss = 0.01103655\n",
      "Iteration 152, loss = 0.01107167\n",
      "Iteration 153, loss = 0.01104340\n",
      "Iteration 154, loss = 0.01086975\n",
      "Iteration 155, loss = 0.01083048\n",
      "Iteration 156, loss = 0.01050163\n",
      "Iteration 157, loss = 0.01076481\n",
      "Iteration 158, loss = 0.01083297\n",
      "Iteration 159, loss = 0.01069089\n",
      "Iteration 160, loss = 0.01069992\n",
      "Iteration 161, loss = 0.01054010\n",
      "Iteration 162, loss = 0.01046286\n",
      "Iteration 163, loss = 0.01049858\n",
      "Iteration 164, loss = 0.01026943\n",
      "Iteration 165, loss = 0.01026419\n",
      "Iteration 166, loss = 0.01039088\n",
      "Iteration 167, loss = 0.01019210\n",
      "Iteration 168, loss = 0.01012090\n",
      "Iteration 169, loss = 0.01011464\n",
      "Iteration 170, loss = 0.01010529\n",
      "Iteration 171, loss = 0.00991702\n",
      "Iteration 172, loss = 0.00991114\n",
      "Iteration 173, loss = 0.00971123\n",
      "Iteration 174, loss = 0.00962961\n",
      "Iteration 175, loss = 0.00951485\n",
      "Iteration 176, loss = 0.00938014\n",
      "Iteration 177, loss = 0.00939856\n",
      "Iteration 178, loss = 0.00934752\n",
      "Iteration 179, loss = 0.01005445\n",
      "Iteration 180, loss = 0.00973870\n",
      "Iteration 181, loss = 0.00940602\n",
      "Iteration 182, loss = 0.00922381\n",
      "Iteration 183, loss = 0.00923191\n",
      "Iteration 184, loss = 0.00907525\n",
      "Iteration 185, loss = 0.00912091\n",
      "Iteration 186, loss = 0.00916828\n",
      "Iteration 187, loss = 0.00912379\n",
      "Iteration 188, loss = 0.00903835\n",
      "Iteration 189, loss = 0.00900714\n",
      "Iteration 190, loss = 0.00913554\n",
      "Iteration 191, loss = 0.00882246\n",
      "Iteration 192, loss = 0.00873558\n",
      "Iteration 193, loss = 0.00879456\n",
      "Iteration 194, loss = 0.00874303\n",
      "Iteration 195, loss = 0.00870059\n",
      "Iteration 196, loss = 0.00859822\n",
      "Iteration 197, loss = 0.00858453\n",
      "Iteration 198, loss = 0.00888269\n",
      "Iteration 199, loss = 0.00834502\n",
      "Iteration 200, loss = 0.00856841\n",
      "Iteration 201, loss = 0.00883310\n",
      "Iteration 202, loss = 0.00830071\n",
      "Iteration 203, loss = 0.00830659\n",
      "Iteration 204, loss = 0.00842621\n",
      "Iteration 205, loss = 0.00812210\n",
      "Iteration 206, loss = 0.00852137\n",
      "Iteration 207, loss = 0.00813987\n",
      "Iteration 208, loss = 0.00849905\n",
      "Iteration 209, loss = 0.00803194\n",
      "Iteration 210, loss = 0.00865098\n",
      "Iteration 211, loss = 0.00814135\n",
      "Iteration 212, loss = 0.00889258\n",
      "Iteration 213, loss = 0.00846438\n",
      "Iteration 214, loss = 0.00882789\n",
      "Iteration 215, loss = 0.00824361\n",
      "Iteration 216, loss = 0.00824831\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46324851\n",
      "Iteration 2, loss = 0.35941492\n",
      "Iteration 3, loss = 0.28550739\n",
      "Iteration 4, loss = 0.23944089\n",
      "Iteration 5, loss = 0.20635895\n",
      "Iteration 6, loss = 0.18216356\n",
      "Iteration 7, loss = 0.16549971\n",
      "Iteration 8, loss = 0.15166451\n",
      "Iteration 9, loss = 0.14053606\n",
      "Iteration 10, loss = 0.13243902\n",
      "Iteration 11, loss = 0.12371684\n",
      "Iteration 12, loss = 0.11471210\n",
      "Iteration 13, loss = 0.10628999\n",
      "Iteration 14, loss = 0.09906424\n",
      "Iteration 15, loss = 0.09262482\n",
      "Iteration 16, loss = 0.08591889\n",
      "Iteration 17, loss = 0.08094528\n",
      "Iteration 18, loss = 0.07659169\n",
      "Iteration 19, loss = 0.07258639\n",
      "Iteration 20, loss = 0.06950486\n",
      "Iteration 21, loss = 0.06635924\n",
      "Iteration 22, loss = 0.06367644\n",
      "Iteration 23, loss = 0.06232014\n",
      "Iteration 24, loss = 0.05970747\n",
      "Iteration 25, loss = 0.05798394\n",
      "Iteration 26, loss = 0.05688184\n",
      "Iteration 27, loss = 0.05514188\n",
      "Iteration 28, loss = 0.05330126\n",
      "Iteration 29, loss = 0.05188316\n",
      "Iteration 30, loss = 0.05072541\n",
      "Iteration 31, loss = 0.04942744\n",
      "Iteration 32, loss = 0.04789565\n",
      "Iteration 33, loss = 0.04673036\n",
      "Iteration 34, loss = 0.04624678\n",
      "Iteration 35, loss = 0.04547219\n",
      "Iteration 36, loss = 0.04395954\n",
      "Iteration 37, loss = 0.04305856\n",
      "Iteration 38, loss = 0.04223179\n",
      "Iteration 39, loss = 0.04154218\n",
      "Iteration 40, loss = 0.04030411\n",
      "Iteration 41, loss = 0.03955255\n",
      "Iteration 42, loss = 0.03934948\n",
      "Iteration 43, loss = 0.03838548\n",
      "Iteration 44, loss = 0.03770689\n",
      "Iteration 45, loss = 0.03745365\n",
      "Iteration 46, loss = 0.03609782\n",
      "Iteration 47, loss = 0.03579298\n",
      "Iteration 48, loss = 0.03468762\n",
      "Iteration 49, loss = 0.03409048\n",
      "Iteration 50, loss = 0.03331079\n",
      "Iteration 51, loss = 0.03297208\n",
      "Iteration 52, loss = 0.03214776\n",
      "Iteration 53, loss = 0.03173763\n",
      "Iteration 54, loss = 0.03107871\n",
      "Iteration 55, loss = 0.03062330\n",
      "Iteration 56, loss = 0.03031322\n",
      "Iteration 57, loss = 0.02974215\n",
      "Iteration 58, loss = 0.02909181\n",
      "Iteration 59, loss = 0.02965139\n",
      "Iteration 60, loss = 0.02865485\n",
      "Iteration 61, loss = 0.02821170\n",
      "Iteration 62, loss = 0.02750707\n",
      "Iteration 63, loss = 0.02779641\n",
      "Iteration 64, loss = 0.02651792\n",
      "Iteration 65, loss = 0.02662533\n",
      "Iteration 66, loss = 0.02605479\n",
      "Iteration 67, loss = 0.02547792\n",
      "Iteration 68, loss = 0.02540315\n",
      "Iteration 69, loss = 0.02496851\n",
      "Iteration 70, loss = 0.02434972\n",
      "Iteration 71, loss = 0.02407528\n",
      "Iteration 72, loss = 0.02364984\n",
      "Iteration 73, loss = 0.02338867\n",
      "Iteration 74, loss = 0.02318713\n",
      "Iteration 75, loss = 0.02345367\n",
      "Iteration 76, loss = 0.02266810\n",
      "Iteration 77, loss = 0.02264495\n",
      "Iteration 78, loss = 0.02222104\n",
      "Iteration 79, loss = 0.02211856\n",
      "Iteration 80, loss = 0.02159582\n",
      "Iteration 81, loss = 0.02133303\n",
      "Iteration 82, loss = 0.02095061\n",
      "Iteration 83, loss = 0.02101477\n",
      "Iteration 84, loss = 0.02063753\n",
      "Iteration 85, loss = 0.02040359\n",
      "Iteration 86, loss = 0.02020878\n",
      "Iteration 87, loss = 0.01989889\n",
      "Iteration 88, loss = 0.01976419\n",
      "Iteration 89, loss = 0.01937072\n",
      "Iteration 90, loss = 0.01898773\n",
      "Iteration 91, loss = 0.01899472\n",
      "Iteration 92, loss = 0.01896070\n",
      "Iteration 93, loss = 0.01861137\n",
      "Iteration 94, loss = 0.01832371\n",
      "Iteration 95, loss = 0.01812335\n",
      "Iteration 96, loss = 0.01799307\n",
      "Iteration 97, loss = 0.01773480\n",
      "Iteration 98, loss = 0.01791535\n",
      "Iteration 99, loss = 0.01730416\n",
      "Iteration 100, loss = 0.01730285\n",
      "Iteration 101, loss = 0.01711971\n",
      "Iteration 102, loss = 0.01721377\n",
      "Iteration 103, loss = 0.01666462\n",
      "Iteration 104, loss = 0.01687832\n",
      "Iteration 105, loss = 0.01624310\n",
      "Iteration 106, loss = 0.01602981\n",
      "Iteration 107, loss = 0.01629150\n",
      "Iteration 108, loss = 0.01613517\n",
      "Iteration 109, loss = 0.01571558\n",
      "Iteration 110, loss = 0.01596724\n",
      "Iteration 111, loss = 0.01544355\n",
      "Iteration 112, loss = 0.01529804\n",
      "Iteration 113, loss = 0.01523076\n",
      "Iteration 114, loss = 0.01531225\n",
      "Iteration 115, loss = 0.01480754\n",
      "Iteration 116, loss = 0.01489574\n",
      "Iteration 117, loss = 0.01460659\n",
      "Iteration 118, loss = 0.01452254\n",
      "Iteration 119, loss = 0.01427482\n",
      "Iteration 120, loss = 0.01416487\n",
      "Iteration 121, loss = 0.01441214\n",
      "Iteration 122, loss = 0.01403859\n",
      "Iteration 123, loss = 0.01437750\n",
      "Iteration 124, loss = 0.01394925\n",
      "Iteration 125, loss = 0.01399405\n",
      "Iteration 126, loss = 0.01365323\n",
      "Iteration 127, loss = 0.01337614\n",
      "Iteration 128, loss = 0.01335382\n",
      "Iteration 129, loss = 0.01345683\n",
      "Iteration 130, loss = 0.01313389\n",
      "Iteration 131, loss = 0.01300321\n",
      "Iteration 132, loss = 0.01290836\n",
      "Iteration 133, loss = 0.01272115\n",
      "Iteration 134, loss = 0.01257667\n",
      "Iteration 135, loss = 0.01281937\n",
      "Iteration 136, loss = 0.01229669\n",
      "Iteration 137, loss = 0.01276218\n",
      "Iteration 138, loss = 0.01232440\n",
      "Iteration 139, loss = 0.01218109\n",
      "Iteration 140, loss = 0.01255270\n",
      "Iteration 141, loss = 0.01197703\n",
      "Iteration 142, loss = 0.01223553\n",
      "Iteration 143, loss = 0.01201204\n",
      "Iteration 144, loss = 0.01182596\n",
      "Iteration 145, loss = 0.01172901\n",
      "Iteration 146, loss = 0.01166659\n",
      "Iteration 147, loss = 0.01167208\n",
      "Iteration 148, loss = 0.01131282\n",
      "Iteration 149, loss = 0.01123023\n",
      "Iteration 150, loss = 0.01116755\n",
      "Iteration 151, loss = 0.01103655\n",
      "Iteration 152, loss = 0.01107167\n",
      "Iteration 153, loss = 0.01104340\n",
      "Iteration 154, loss = 0.01086975\n",
      "Iteration 155, loss = 0.01083048\n",
      "Iteration 156, loss = 0.01050163\n",
      "Iteration 157, loss = 0.01076481\n",
      "Iteration 158, loss = 0.01083297\n",
      "Iteration 159, loss = 0.01069089\n",
      "Iteration 160, loss = 0.01069992\n",
      "Iteration 161, loss = 0.01054010\n",
      "Iteration 162, loss = 0.01046286\n",
      "Iteration 163, loss = 0.01049858\n",
      "Iteration 164, loss = 0.01026943\n",
      "Iteration 165, loss = 0.01026419\n",
      "Iteration 166, loss = 0.01039088\n",
      "Iteration 167, loss = 0.01019210\n",
      "Iteration 168, loss = 0.01012090\n",
      "Iteration 169, loss = 0.01011464\n",
      "Iteration 170, loss = 0.01010529\n",
      "Iteration 171, loss = 0.00991702\n",
      "Iteration 172, loss = 0.00991114\n",
      "Iteration 173, loss = 0.00971123\n",
      "Iteration 174, loss = 0.00962961\n",
      "Iteration 175, loss = 0.00951485\n",
      "Iteration 176, loss = 0.00938014\n",
      "Iteration 177, loss = 0.00939856\n",
      "Iteration 178, loss = 0.00934752\n",
      "Iteration 179, loss = 0.01005445\n",
      "Iteration 180, loss = 0.00973870\n",
      "Iteration 181, loss = 0.00940602\n",
      "Iteration 182, loss = 0.00922381\n",
      "Iteration 183, loss = 0.00923191\n",
      "Iteration 184, loss = 0.00907525\n",
      "Iteration 185, loss = 0.00912091\n",
      "Iteration 186, loss = 0.00916828\n",
      "Iteration 187, loss = 0.00912379\n",
      "Iteration 188, loss = 0.00903835\n",
      "Iteration 189, loss = 0.00900714\n",
      "Iteration 190, loss = 0.00913554\n",
      "Iteration 191, loss = 0.00882246\n",
      "Iteration 192, loss = 0.00873558\n",
      "Iteration 193, loss = 0.00879456\n",
      "Iteration 194, loss = 0.00874303\n",
      "Iteration 195, loss = 0.00870059\n",
      "Iteration 196, loss = 0.00859822\n",
      "Iteration 197, loss = 0.00858453\n",
      "Iteration 198, loss = 0.00888269\n",
      "Iteration 199, loss = 0.00834502\n",
      "Iteration 200, loss = 0.00856841\n",
      "Iteration 201, loss = 0.00883310\n",
      "Iteration 202, loss = 0.00830071\n",
      "Iteration 203, loss = 0.00830659\n",
      "Iteration 204, loss = 0.00842621\n",
      "Iteration 205, loss = 0.00812210\n",
      "Iteration 206, loss = 0.00852137\n",
      "Iteration 207, loss = 0.00813987\n",
      "Iteration 208, loss = 0.00849905\n",
      "Iteration 209, loss = 0.00803194\n",
      "Iteration 210, loss = 0.00865098\n",
      "Iteration 211, loss = 0.00814135\n",
      "Iteration 212, loss = 0.00889258\n",
      "Iteration 213, loss = 0.00846438\n",
      "Iteration 214, loss = 0.00882789\n",
      "Iteration 215, loss = 0.00824361\n",
      "Iteration 216, loss = 0.00824831\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46324851\n",
      "Iteration 2, loss = 0.35941492\n",
      "Iteration 3, loss = 0.28550739\n",
      "Iteration 4, loss = 0.23944089\n",
      "Iteration 5, loss = 0.20635895\n",
      "Iteration 6, loss = 0.18216356\n",
      "Iteration 7, loss = 0.16549971\n",
      "Iteration 8, loss = 0.15166451\n",
      "Iteration 9, loss = 0.14053606\n",
      "Iteration 10, loss = 0.13243902\n",
      "Iteration 11, loss = 0.12371684\n",
      "Iteration 12, loss = 0.11471210\n",
      "Iteration 13, loss = 0.10628999\n",
      "Iteration 14, loss = 0.09906424\n",
      "Iteration 15, loss = 0.09262482\n",
      "Iteration 16, loss = 0.08591889\n",
      "Iteration 17, loss = 0.08094528\n",
      "Iteration 18, loss = 0.07659169\n",
      "Iteration 19, loss = 0.07258639\n",
      "Iteration 20, loss = 0.06950486\n",
      "Iteration 21, loss = 0.06635924\n",
      "Iteration 22, loss = 0.06367644\n",
      "Iteration 23, loss = 0.06232014\n",
      "Iteration 24, loss = 0.05970747\n",
      "Iteration 25, loss = 0.05798394\n",
      "Iteration 26, loss = 0.05688184\n",
      "Iteration 27, loss = 0.05514188\n",
      "Iteration 28, loss = 0.05330126\n",
      "Iteration 29, loss = 0.05188316\n",
      "Iteration 30, loss = 0.05072541\n",
      "Iteration 31, loss = 0.04942744\n",
      "Iteration 32, loss = 0.04789565\n",
      "Iteration 33, loss = 0.04673036\n",
      "Iteration 34, loss = 0.04624678\n",
      "Iteration 35, loss = 0.04547219\n",
      "Iteration 36, loss = 0.04395954\n",
      "Iteration 37, loss = 0.04305856\n",
      "Iteration 38, loss = 0.04223179\n",
      "Iteration 39, loss = 0.04154218\n",
      "Iteration 40, loss = 0.04030411\n",
      "Iteration 41, loss = 0.03955255\n",
      "Iteration 42, loss = 0.03934948\n",
      "Iteration 43, loss = 0.03838548\n",
      "Iteration 44, loss = 0.03770689\n",
      "Iteration 45, loss = 0.03745365\n",
      "Iteration 46, loss = 0.03609782\n",
      "Iteration 47, loss = 0.03579298\n",
      "Iteration 48, loss = 0.03468762\n",
      "Iteration 49, loss = 0.03409048\n",
      "Iteration 50, loss = 0.03331079\n",
      "Iteration 51, loss = 0.03297208\n",
      "Iteration 52, loss = 0.03214776\n",
      "Iteration 53, loss = 0.03173763\n",
      "Iteration 54, loss = 0.03107871\n",
      "Iteration 55, loss = 0.03062330\n",
      "Iteration 56, loss = 0.03031322\n",
      "Iteration 57, loss = 0.02974215\n",
      "Iteration 58, loss = 0.02909181\n",
      "Iteration 59, loss = 0.02965139\n",
      "Iteration 60, loss = 0.02865485\n",
      "Iteration 61, loss = 0.02821170\n",
      "Iteration 62, loss = 0.02750707\n",
      "Iteration 63, loss = 0.02779641\n",
      "Iteration 64, loss = 0.02651792\n",
      "Iteration 65, loss = 0.02662533\n",
      "Iteration 66, loss = 0.02605479\n",
      "Iteration 67, loss = 0.02547792\n",
      "Iteration 68, loss = 0.02540315\n",
      "Iteration 69, loss = 0.02496851\n",
      "Iteration 70, loss = 0.02434972\n",
      "Iteration 71, loss = 0.02407528\n",
      "Iteration 72, loss = 0.02364984\n",
      "Iteration 73, loss = 0.02338867\n",
      "Iteration 74, loss = 0.02318713\n",
      "Iteration 75, loss = 0.02345367\n",
      "Iteration 76, loss = 0.02266810\n",
      "Iteration 77, loss = 0.02264495\n",
      "Iteration 78, loss = 0.02222104\n",
      "Iteration 79, loss = 0.02211856\n",
      "Iteration 80, loss = 0.02159582\n",
      "Iteration 81, loss = 0.02133303\n",
      "Iteration 82, loss = 0.02095061\n",
      "Iteration 83, loss = 0.02101477\n",
      "Iteration 84, loss = 0.02063753\n",
      "Iteration 85, loss = 0.02040359\n",
      "Iteration 86, loss = 0.02020878\n",
      "Iteration 87, loss = 0.01989889\n",
      "Iteration 88, loss = 0.01976419\n",
      "Iteration 89, loss = 0.01937072\n",
      "Iteration 90, loss = 0.01898773\n",
      "Iteration 91, loss = 0.01899472\n",
      "Iteration 92, loss = 0.01896070\n",
      "Iteration 93, loss = 0.01861137\n",
      "Iteration 94, loss = 0.01832371\n",
      "Iteration 95, loss = 0.01812335\n",
      "Iteration 96, loss = 0.01799307\n",
      "Iteration 97, loss = 0.01773480\n",
      "Iteration 98, loss = 0.01791535\n",
      "Iteration 99, loss = 0.01730416\n",
      "Iteration 100, loss = 0.01730285\n",
      "Iteration 101, loss = 0.01711971\n",
      "Iteration 102, loss = 0.01721377\n",
      "Iteration 103, loss = 0.01666462\n",
      "Iteration 104, loss = 0.01687832\n",
      "Iteration 105, loss = 0.01624310\n",
      "Iteration 106, loss = 0.01602981\n",
      "Iteration 107, loss = 0.01629150\n",
      "Iteration 108, loss = 0.01613517\n",
      "Iteration 109, loss = 0.01571558\n",
      "Iteration 110, loss = 0.01596724\n",
      "Iteration 111, loss = 0.01544355\n",
      "Iteration 112, loss = 0.01529804\n",
      "Iteration 113, loss = 0.01523076\n",
      "Iteration 114, loss = 0.01531225\n",
      "Iteration 115, loss = 0.01480754\n",
      "Iteration 116, loss = 0.01489574\n",
      "Iteration 117, loss = 0.01460659\n",
      "Iteration 118, loss = 0.01452254\n",
      "Iteration 119, loss = 0.01427482\n",
      "Iteration 120, loss = 0.01416487\n",
      "Iteration 121, loss = 0.01441214\n",
      "Iteration 122, loss = 0.01403859\n",
      "Iteration 123, loss = 0.01437750\n",
      "Iteration 124, loss = 0.01394925\n",
      "Iteration 125, loss = 0.01399405\n",
      "Iteration 126, loss = 0.01365323\n",
      "Iteration 127, loss = 0.01337614\n",
      "Iteration 128, loss = 0.01335382\n",
      "Iteration 129, loss = 0.01345683\n",
      "Iteration 130, loss = 0.01313389\n",
      "Iteration 131, loss = 0.01300321\n",
      "Iteration 132, loss = 0.01290836\n",
      "Iteration 133, loss = 0.01272115\n",
      "Iteration 134, loss = 0.01257667\n",
      "Iteration 135, loss = 0.01281937\n",
      "Iteration 136, loss = 0.01229669\n",
      "Iteration 137, loss = 0.01276218\n",
      "Iteration 138, loss = 0.01232440\n",
      "Iteration 139, loss = 0.01218109\n",
      "Iteration 140, loss = 0.01255270\n",
      "Iteration 141, loss = 0.01197703\n",
      "Iteration 142, loss = 0.01223553\n",
      "Iteration 143, loss = 0.01201204\n",
      "Iteration 144, loss = 0.01182596\n",
      "Iteration 145, loss = 0.01172901\n",
      "Iteration 146, loss = 0.01166659\n",
      "Iteration 147, loss = 0.01167208\n",
      "Iteration 148, loss = 0.01131282\n",
      "Iteration 149, loss = 0.01123023\n",
      "Iteration 150, loss = 0.01116755\n",
      "Iteration 151, loss = 0.01103655\n",
      "Iteration 152, loss = 0.01107167\n",
      "Iteration 153, loss = 0.01104340\n",
      "Iteration 154, loss = 0.01086975\n",
      "Iteration 155, loss = 0.01083048\n",
      "Iteration 156, loss = 0.01050163\n",
      "Iteration 157, loss = 0.01076481\n",
      "Iteration 158, loss = 0.01083297\n",
      "Iteration 159, loss = 0.01069089\n",
      "Iteration 160, loss = 0.01069992\n",
      "Iteration 161, loss = 0.01054010\n",
      "Iteration 162, loss = 0.01046286\n",
      "Iteration 163, loss = 0.01049858\n",
      "Iteration 164, loss = 0.01026943\n",
      "Iteration 165, loss = 0.01026419\n",
      "Iteration 166, loss = 0.01039088\n",
      "Iteration 167, loss = 0.01019210\n",
      "Iteration 168, loss = 0.01012090\n",
      "Iteration 169, loss = 0.01011464\n",
      "Iteration 170, loss = 0.01010529\n",
      "Iteration 171, loss = 0.00991702\n",
      "Iteration 172, loss = 0.00991114\n",
      "Iteration 173, loss = 0.00971123\n",
      "Iteration 174, loss = 0.00962961\n",
      "Iteration 175, loss = 0.00951485\n",
      "Iteration 176, loss = 0.00938014\n",
      "LinearRegression\n",
      "NeuralNetwork\n",
      "Iteration 177, loss = 0.00939856\n",
      "Iteration 178, loss = 0.00934752\n",
      "Iteration 179, loss = 0.01005445\n",
      "Iteration 180, loss = 0.00973870\n",
      "Iteration 181, loss = 0.00940602\n",
      "Iteration 182, loss = 0.00922381\n",
      "Iteration 183, loss = 0.00923191\n",
      "Iteration 184, loss = 0.00907525\n",
      "Iteration 185, loss = 0.00912091\n",
      "Iteration 186, loss = 0.00916828\n",
      "Iteration 187, loss = 0.00912379\n",
      "Iteration 188, loss = 0.00903835\n",
      "Iteration 189, loss = 0.00900714\n",
      "Iteration 190, loss = 0.00913554\n",
      "Iteration 191, loss = 0.00882246\n",
      "Iteration 192, loss = 0.00873558\n",
      "Iteration 193, loss = 0.00879456\n",
      "Iteration 194, loss = 0.00874303\n",
      "Iteration 195, loss = 0.00870059\n",
      "Iteration 196, loss = 0.00859822\n",
      "Iteration 197, loss = 0.00858453\n",
      "Iteration 198, loss = 0.00888269\n",
      "Iteration 199, loss = 0.00834502\n",
      "Iteration 200, loss = 0.00856841\n",
      "Iteration 201, loss = 0.00883310\n",
      "Iteration 202, loss = 0.00830071\n",
      "Iteration 203, loss = 0.00830659\n",
      "Iteration 204, loss = 0.00842621\n",
      "Iteration 205, loss = 0.00812210\n",
      "Iteration 206, loss = 0.00852137\n",
      "Iteration 207, loss = 0.00813987\n",
      "Iteration 208, loss = 0.00849905\n",
      "Iteration 209, loss = 0.00803194\n",
      "Iteration 210, loss = 0.00865098\n",
      "Iteration 211, loss = 0.00814135\n",
      "Iteration 212, loss = 0.00889258\n",
      "Iteration 213, loss = 0.00846438\n",
      "Iteration 214, loss = 0.00882789\n",
      "Iteration 215, loss = 0.00824361\n",
      "Iteration 216, loss = 0.00824831\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46342797\n",
      "Iteration 2, loss = 0.36451523\n",
      "Iteration 3, loss = 0.29998361\n",
      "Iteration 4, loss = 0.25968108\n",
      "Iteration 5, loss = 0.22834674\n",
      "Iteration 6, loss = 0.20217996\n",
      "Iteration 7, loss = 0.18105129\n",
      "Iteration 8, loss = 0.16493397\n",
      "Iteration 9, loss = 0.15146614\n",
      "Iteration 10, loss = 0.14066096\n",
      "Iteration 11, loss = 0.13091522\n",
      "Iteration 12, loss = 0.11985559\n",
      "Iteration 13, loss = 0.10996710\n",
      "Iteration 14, loss = 0.10024770\n",
      "Iteration 15, loss = 0.09268555\n",
      "Iteration 16, loss = 0.08446371\n",
      "Iteration 17, loss = 0.07777713\n",
      "Iteration 18, loss = 0.07199076\n",
      "Iteration 19, loss = 0.06787513\n",
      "Iteration 20, loss = 0.06353704\n",
      "Iteration 21, loss = 0.06049633\n",
      "Iteration 22, loss = 0.05732817\n",
      "Iteration 23, loss = 0.05456730\n",
      "Iteration 24, loss = 0.05222412\n",
      "Iteration 25, loss = 0.05012153\n",
      "Iteration 26, loss = 0.04823142\n",
      "Iteration 27, loss = 0.04635019\n",
      "Iteration 28, loss = 0.04484730\n",
      "Iteration 29, loss = 0.04308214\n",
      "Iteration 30, loss = 0.04177354\n",
      "Iteration 31, loss = 0.04040022\n",
      "Iteration 32, loss = 0.03967207\n",
      "Iteration 33, loss = 0.03833892\n",
      "Iteration 34, loss = 0.03755048\n",
      "Iteration 35, loss = 0.03676250\n",
      "Iteration 36, loss = 0.03580538\n",
      "Iteration 37, loss = 0.03493546\n",
      "Iteration 38, loss = 0.03428676\n",
      "Iteration 39, loss = 0.03393516\n",
      "Iteration 40, loss = 0.03287699\n",
      "Iteration 41, loss = 0.03221297\n",
      "Iteration 42, loss = 0.03196821\n",
      "Iteration 43, loss = 0.03121267\n",
      "Iteration 44, loss = 0.03057138\n",
      "Iteration 45, loss = 0.03016142\n",
      "Iteration 46, loss = 0.02947047\n",
      "Iteration 47, loss = 0.02924807\n",
      "Iteration 48, loss = 0.02872257\n",
      "Iteration 49, loss = 0.02828715\n",
      "Iteration 50, loss = 0.02793971\n",
      "Iteration 51, loss = 0.02797827\n",
      "Iteration 52, loss = 0.02705268\n",
      "Iteration 53, loss = 0.02660194\n",
      "Iteration 54, loss = 0.02630790\n",
      "Iteration 55, loss = 0.02613570\n",
      "Iteration 56, loss = 0.02572870\n",
      "Iteration 57, loss = 0.02600728\n",
      "Iteration 58, loss = 0.02513268\n",
      "Iteration 59, loss = 0.02498415\n",
      "Iteration 60, loss = 0.02470448\n",
      "Iteration 61, loss = 0.02443019\n",
      "Iteration 62, loss = 0.02425093\n",
      "Iteration 63, loss = 0.02383238\n",
      "Iteration 64, loss = 0.02372451\n",
      "Iteration 65, loss = 0.02352938\n",
      "Iteration 66, loss = 0.02338483\n",
      "Iteration 67, loss = 0.02305728\n",
      "Iteration 68, loss = 0.02291985\n",
      "Iteration 69, loss = 0.02263814\n",
      "Iteration 70, loss = 0.02305234\n",
      "Iteration 71, loss = 0.02257164\n",
      "Iteration 72, loss = 0.02249678\n",
      "Iteration 73, loss = 0.02264919\n",
      "Iteration 74, loss = 0.02185865\n",
      "Iteration 75, loss = 0.02169943\n",
      "Iteration 76, loss = 0.02147328\n",
      "Iteration 77, loss = 0.02131338\n",
      "Iteration 78, loss = 0.02150912\n",
      "Iteration 79, loss = 0.02133149\n",
      "Iteration 80, loss = 0.02084279\n",
      "Iteration 81, loss = 0.02091675\n",
      "Iteration 82, loss = 0.02063752\n",
      "Iteration 83, loss = 0.02038964\n",
      "Iteration 84, loss = 0.02070027\n",
      "Iteration 85, loss = 0.02019454\n",
      "Iteration 86, loss = 0.02006457\n",
      "Iteration 87, loss = 0.01970792\n",
      "Iteration 88, loss = 0.01973819\n",
      "Iteration 89, loss = 0.01968204\n",
      "Iteration 90, loss = 0.01965270\n",
      "Iteration 91, loss = 0.01941793\n",
      "Iteration 92, loss = 0.01918709\n",
      "Iteration 93, loss = 0.01916810\n",
      "Iteration 94, loss = 0.01933818\n",
      "Iteration 95, loss = 0.01907545\n",
      "Iteration 96, loss = 0.01929597\n",
      "Iteration 97, loss = 0.01855722\n",
      "Iteration 98, loss = 0.01856724\n",
      "Iteration 99, loss = 0.01878375\n",
      "Iteration 100, loss = 0.01868946\n",
      "Iteration 101, loss = 0.01829354\n",
      "Iteration 102, loss = 0.01858973\n",
      "Iteration 103, loss = 0.01831339\n",
      "Iteration 104, loss = 0.01811262\n",
      "Iteration 105, loss = 0.01814914\n",
      "Iteration 106, loss = 0.01795271\n",
      "Iteration 107, loss = 0.01774360\n",
      "Iteration 108, loss = 0.01775692\n",
      "Iteration 109, loss = 0.01765533\n",
      "Iteration 110, loss = 0.01788381\n",
      "Iteration 111, loss = 0.01743161\n",
      "Iteration 112, loss = 0.01747299\n",
      "Iteration 113, loss = 0.01746941\n",
      "Iteration 114, loss = 0.01716748\n",
      "Iteration 115, loss = 0.01749553\n",
      "Iteration 116, loss = 0.01716706\n",
      "Iteration 117, loss = 0.01701998\n",
      "Iteration 118, loss = 0.01722833\n",
      "Iteration 119, loss = 0.01713855\n",
      "Iteration 120, loss = 0.01665505\n",
      "Iteration 121, loss = 0.01680052\n",
      "Iteration 122, loss = 0.01690046\n",
      "Iteration 123, loss = 0.01644100\n",
      "Iteration 124, loss = 0.01624896\n",
      "Iteration 125, loss = 0.01622181\n",
      "Iteration 126, loss = 0.01614399\n",
      "Iteration 127, loss = 0.01608109\n",
      "Iteration 128, loss = 0.01605919\n",
      "Iteration 129, loss = 0.01658300\n",
      "Iteration 130, loss = 0.01621517\n",
      "Iteration 131, loss = 0.01590823\n",
      "Iteration 132, loss = 0.01579208\n",
      "Iteration 133, loss = 0.01583206\n",
      "Iteration 134, loss = 0.01558209\n",
      "Iteration 135, loss = 0.01541842\n",
      "Iteration 136, loss = 0.01542622\n",
      "Iteration 137, loss = 0.01533775\n",
      "Iteration 138, loss = 0.01543547\n",
      "Iteration 139, loss = 0.01541465\n",
      "Iteration 140, loss = 0.01500375\n",
      "Iteration 141, loss = 0.01508854\n",
      "Iteration 142, loss = 0.01557684\n",
      "Iteration 143, loss = 0.01500384\n",
      "Iteration 144, loss = 0.01479864\n",
      "Iteration 145, loss = 0.01471221\n",
      "Iteration 146, loss = 0.01472428\n",
      "Iteration 147, loss = 0.01461271\n",
      "Iteration 148, loss = 0.01452820\n",
      "Iteration 149, loss = 0.01471559\n",
      "Iteration 150, loss = 0.01446383\n",
      "Iteration 151, loss = 0.01453375\n",
      "Iteration 152, loss = 0.01445096\n",
      "Iteration 153, loss = 0.01441521\n",
      "Iteration 154, loss = 0.01441430\n",
      "Iteration 155, loss = 0.01427542\n",
      "Iteration 156, loss = 0.01408833\n",
      "Iteration 157, loss = 0.01427963\n",
      "Iteration 158, loss = 0.01413269\n",
      "Iteration 159, loss = 0.01425962\n",
      "Iteration 160, loss = 0.01401092\n",
      "Iteration 161, loss = 0.01391371\n",
      "Iteration 162, loss = 0.01388427\n",
      "Iteration 163, loss = 0.01392043\n",
      "Iteration 164, loss = 0.01366868\n",
      "Iteration 165, loss = 0.01377227\n",
      "Iteration 166, loss = 0.01345854\n",
      "Iteration 167, loss = 0.01360338\n",
      "Iteration 168, loss = 0.01341556\n",
      "Iteration 169, loss = 0.01343081\n",
      "Iteration 170, loss = 0.01355994\n",
      "Iteration 171, loss = 0.01379305\n",
      "Iteration 172, loss = 0.01358095\n",
      "Iteration 173, loss = 0.01349383\n",
      "Iteration 174, loss = 0.01349465\n",
      "Iteration 175, loss = 0.01330960\n",
      "Iteration 176, loss = 0.01325675\n",
      "Iteration 177, loss = 0.01352630\n",
      "Iteration 178, loss = 0.01303875\n",
      "Iteration 179, loss = 0.01342345\n",
      "Iteration 180, loss = 0.01314771\n",
      "Iteration 181, loss = 0.01324785\n",
      "Iteration 182, loss = 0.01295473\n",
      "Iteration 183, loss = 0.01284612\n",
      "Iteration 184, loss = 0.01293614\n",
      "Iteration 185, loss = 0.01283357\n",
      "Iteration 186, loss = 0.01334360\n",
      "Iteration 187, loss = 0.01290791\n",
      "Iteration 188, loss = 0.01294142\n",
      "Iteration 189, loss = 0.01286590\n",
      "Iteration 190, loss = 0.01275219\n",
      "Iteration 191, loss = 0.01253397\n",
      "Iteration 192, loss = 0.01253889\n",
      "Iteration 193, loss = 0.01255391\n",
      "Iteration 194, loss = 0.01235683\n",
      "Iteration 195, loss = 0.01233935\n",
      "Iteration 196, loss = 0.01231582\n",
      "Iteration 197, loss = 0.01229042\n",
      "Iteration 198, loss = 0.01220901\n",
      "Iteration 199, loss = 0.01221975\n",
      "Iteration 200, loss = 0.01232473\n",
      "Iteration 201, loss = 0.01214878\n",
      "Iteration 202, loss = 0.01210891\n",
      "Iteration 203, loss = 0.01205168\n",
      "Iteration 204, loss = 0.01205162\n",
      "Iteration 205, loss = 0.01208692\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46342797\n",
      "Iteration 2, loss = 0.36451523\n",
      "Iteration 3, loss = 0.29998361\n",
      "Iteration 4, loss = 0.25968108\n",
      "Iteration 5, loss = 0.22834674\n",
      "Iteration 6, loss = 0.20217996\n",
      "Iteration 7, loss = 0.18105129\n",
      "Iteration 8, loss = 0.16493397\n",
      "Iteration 9, loss = 0.15146614\n",
      "Iteration 10, loss = 0.14066096\n",
      "Iteration 11, loss = 0.13091522\n",
      "Iteration 12, loss = 0.11985559\n",
      "Iteration 13, loss = 0.10996710\n",
      "Iteration 14, loss = 0.10024770\n",
      "Iteration 15, loss = 0.09268555\n",
      "Iteration 16, loss = 0.08446371\n",
      "Iteration 17, loss = 0.07777713\n",
      "Iteration 18, loss = 0.07199076\n",
      "Iteration 19, loss = 0.06787513\n",
      "Iteration 20, loss = 0.06353704\n",
      "Iteration 21, loss = 0.06049633\n",
      "Iteration 22, loss = 0.05732817\n",
      "Iteration 23, loss = 0.05456730\n",
      "Iteration 24, loss = 0.05222412\n",
      "Iteration 25, loss = 0.05012153\n",
      "Iteration 26, loss = 0.04823142\n",
      "Iteration 27, loss = 0.04635019\n",
      "Iteration 28, loss = 0.04484730\n",
      "Iteration 29, loss = 0.04308214\n",
      "Iteration 30, loss = 0.04177354\n",
      "Iteration 31, loss = 0.04040022\n",
      "Iteration 32, loss = 0.03967207\n",
      "Iteration 33, loss = 0.03833892\n",
      "Iteration 34, loss = 0.03755048\n",
      "Iteration 35, loss = 0.03676250\n",
      "Iteration 36, loss = 0.03580538\n",
      "Iteration 37, loss = 0.03493546\n",
      "Iteration 38, loss = 0.03428676\n",
      "Iteration 39, loss = 0.03393516\n",
      "Iteration 40, loss = 0.03287699\n",
      "Iteration 41, loss = 0.03221297\n",
      "Iteration 42, loss = 0.03196821\n",
      "Iteration 43, loss = 0.03121267\n",
      "Iteration 44, loss = 0.03057138\n",
      "Iteration 45, loss = 0.03016142\n",
      "Iteration 46, loss = 0.02947047\n",
      "Iteration 47, loss = 0.02924807\n",
      "Iteration 48, loss = 0.02872257\n",
      "Iteration 49, loss = 0.02828715\n",
      "Iteration 50, loss = 0.02793971\n",
      "Iteration 51, loss = 0.02797827\n",
      "Iteration 52, loss = 0.02705268\n",
      "Iteration 53, loss = 0.02660194\n",
      "Iteration 54, loss = 0.02630790\n",
      "Iteration 55, loss = 0.02613570\n",
      "Iteration 56, loss = 0.02572870\n",
      "Iteration 57, loss = 0.02600728\n",
      "Iteration 58, loss = 0.02513268\n",
      "Iteration 59, loss = 0.02498415\n",
      "Iteration 60, loss = 0.02470448\n",
      "Iteration 61, loss = 0.02443019\n",
      "Iteration 62, loss = 0.02425093\n",
      "Iteration 63, loss = 0.02383238\n",
      "Iteration 64, loss = 0.02372451\n",
      "Iteration 65, loss = 0.02352938\n",
      "Iteration 66, loss = 0.02338483\n",
      "Iteration 67, loss = 0.02305728\n",
      "Iteration 68, loss = 0.02291985\n",
      "Iteration 69, loss = 0.02263814\n",
      "Iteration 70, loss = 0.02305234\n",
      "Iteration 71, loss = 0.02257164\n",
      "Iteration 72, loss = 0.02249678\n",
      "Iteration 73, loss = 0.02264919\n",
      "Iteration 74, loss = 0.02185865\n",
      "Iteration 75, loss = 0.02169943\n",
      "Iteration 76, loss = 0.02147328\n",
      "Iteration 77, loss = 0.02131338\n",
      "Iteration 78, loss = 0.02150912\n",
      "Iteration 79, loss = 0.02133149\n",
      "Iteration 80, loss = 0.02084279\n",
      "Iteration 81, loss = 0.02091675\n",
      "Iteration 82, loss = 0.02063752\n",
      "Iteration 83, loss = 0.02038964\n",
      "Iteration 84, loss = 0.02070027\n",
      "Iteration 85, loss = 0.02019454\n",
      "Iteration 86, loss = 0.02006457\n",
      "Iteration 87, loss = 0.01970792\n",
      "Iteration 88, loss = 0.01973819\n",
      "Iteration 89, loss = 0.01968204\n",
      "Iteration 90, loss = 0.01965270\n",
      "Iteration 91, loss = 0.01941793\n",
      "Iteration 92, loss = 0.01918709\n",
      "Iteration 93, loss = 0.01916810\n",
      "Iteration 94, loss = 0.01933818\n",
      "Iteration 95, loss = 0.01907545\n",
      "Iteration 96, loss = 0.01929597\n",
      "Iteration 97, loss = 0.01855722\n",
      "Iteration 98, loss = 0.01856724\n",
      "Iteration 99, loss = 0.01878375\n",
      "Iteration 100, loss = 0.01868946\n",
      "Iteration 101, loss = 0.01829354\n",
      "Iteration 102, loss = 0.01858973\n",
      "Iteration 103, loss = 0.01831339\n",
      "Iteration 104, loss = 0.01811262\n",
      "Iteration 105, loss = 0.01814914\n",
      "Iteration 106, loss = 0.01795271\n",
      "Iteration 107, loss = 0.01774360\n",
      "Iteration 108, loss = 0.01775692\n",
      "Iteration 109, loss = 0.01765533\n",
      "Iteration 110, loss = 0.01788381\n",
      "Iteration 111, loss = 0.01743161\n",
      "Iteration 112, loss = 0.01747299\n",
      "Iteration 113, loss = 0.01746941\n",
      "Iteration 114, loss = 0.01716748\n",
      "Iteration 115, loss = 0.01749553\n",
      "Iteration 116, loss = 0.01716706\n",
      "Iteration 117, loss = 0.01701998\n",
      "Iteration 118, loss = 0.01722833\n",
      "Iteration 119, loss = 0.01713855\n",
      "Iteration 120, loss = 0.01665505\n",
      "Iteration 121, loss = 0.01680052\n",
      "Iteration 122, loss = 0.01690046\n",
      "Iteration 123, loss = 0.01644100\n",
      "Iteration 124, loss = 0.01624896\n",
      "Iteration 125, loss = 0.01622181\n",
      "Iteration 126, loss = 0.01614399\n",
      "Iteration 127, loss = 0.01608109\n",
      "Iteration 128, loss = 0.01605919\n",
      "Iteration 129, loss = 0.01658300\n",
      "Iteration 130, loss = 0.01621517\n",
      "Iteration 131, loss = 0.01590823\n",
      "Iteration 132, loss = 0.01579208\n",
      "Iteration 133, loss = 0.01583206\n",
      "Iteration 134, loss = 0.01558209\n",
      "Iteration 135, loss = 0.01541842\n",
      "Iteration 136, loss = 0.01542622\n",
      "Iteration 137, loss = 0.01533775\n",
      "Iteration 138, loss = 0.01543547\n",
      "Iteration 139, loss = 0.01541465\n",
      "Iteration 140, loss = 0.01500375\n",
      "Iteration 141, loss = 0.01508854\n",
      "Iteration 142, loss = 0.01557684\n",
      "Iteration 143, loss = 0.01500384\n",
      "Iteration 144, loss = 0.01479864\n",
      "Iteration 145, loss = 0.01471221\n",
      "Iteration 146, loss = 0.01472428\n",
      "Iteration 147, loss = 0.01461271\n",
      "Iteration 148, loss = 0.01452820\n",
      "Iteration 149, loss = 0.01471559\n",
      "Iteration 150, loss = 0.01446383\n",
      "Iteration 151, loss = 0.01453375\n",
      "Iteration 152, loss = 0.01445096\n",
      "Iteration 153, loss = 0.01441521\n",
      "Iteration 154, loss = 0.01441430\n",
      "Iteration 155, loss = 0.01427542\n",
      "Iteration 156, loss = 0.01408833\n",
      "Iteration 157, loss = 0.01427963\n",
      "Iteration 158, loss = 0.01413269\n",
      "Iteration 159, loss = 0.01425962\n",
      "Iteration 160, loss = 0.01401092\n",
      "Iteration 161, loss = 0.01391371\n",
      "Iteration 162, loss = 0.01388427\n",
      "Iteration 163, loss = 0.01392043\n",
      "Iteration 164, loss = 0.01366868\n",
      "Iteration 165, loss = 0.01377227\n",
      "Iteration 166, loss = 0.01345854\n",
      "Iteration 167, loss = 0.01360338\n",
      "Iteration 168, loss = 0.01341556\n",
      "Iteration 169, loss = 0.01343081\n",
      "Iteration 170, loss = 0.01355994\n",
      "Iteration 171, loss = 0.01379305\n",
      "Iteration 172, loss = 0.01358095\n",
      "Iteration 173, loss = 0.01349383\n",
      "Iteration 174, loss = 0.01349465\n",
      "Iteration 175, loss = 0.01330960\n",
      "Iteration 176, loss = 0.01325675\n",
      "Iteration 177, loss = 0.01352630\n",
      "Iteration 178, loss = 0.01303875\n",
      "Iteration 179, loss = 0.01342345\n",
      "Iteration 180, loss = 0.01314771\n",
      "Iteration 181, loss = 0.01324785\n",
      "Iteration 182, loss = 0.01295473\n",
      "Iteration 183, loss = 0.01284612\n",
      "Iteration 184, loss = 0.01293614\n",
      "Iteration 185, loss = 0.01283357\n",
      "Iteration 186, loss = 0.01334360\n",
      "Iteration 187, loss = 0.01290791\n",
      "Iteration 188, loss = 0.01294142\n",
      "Iteration 189, loss = 0.01286590\n",
      "Iteration 190, loss = 0.01275219\n",
      "Iteration 191, loss = 0.01253397\n",
      "Iteration 192, loss = 0.01253889\n",
      "Iteration 193, loss = 0.01255391\n",
      "Iteration 194, loss = 0.01235683\n",
      "Iteration 195, loss = 0.01233935\n",
      "Iteration 196, loss = 0.01231582\n",
      "Iteration 197, loss = 0.01229042\n",
      "Iteration 198, loss = 0.01220901\n",
      "Iteration 199, loss = 0.01221975\n",
      "Iteration 200, loss = 0.01232473\n",
      "Iteration 201, loss = 0.01214878\n",
      "Iteration 202, loss = 0.01210891\n",
      "Iteration 203, loss = 0.01205168\n",
      "Iteration 204, loss = 0.01205162\n",
      "Iteration 205, loss = 0.01208692\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46342797\n",
      "Iteration 2, loss = 0.36451523\n",
      "Iteration 3, loss = 0.29998361\n",
      "Iteration 4, loss = 0.25968108\n",
      "Iteration 5, loss = 0.22834674\n",
      "Iteration 6, loss = 0.20217996\n",
      "Iteration 7, loss = 0.18105129\n",
      "Iteration 8, loss = 0.16493397\n",
      "Iteration 9, loss = 0.15146614\n",
      "Iteration 10, loss = 0.14066096\n",
      "Iteration 11, loss = 0.13091522\n",
      "Iteration 12, loss = 0.11985559\n",
      "Iteration 13, loss = 0.10996710\n",
      "Iteration 14, loss = 0.10024770\n",
      "Iteration 15, loss = 0.09268555\n",
      "Iteration 16, loss = 0.08446371\n",
      "Iteration 17, loss = 0.07777713\n",
      "Iteration 18, loss = 0.07199076\n",
      "Iteration 19, loss = 0.06787513\n",
      "Iteration 20, loss = 0.06353704\n",
      "Iteration 21, loss = 0.06049633\n",
      "Iteration 22, loss = 0.05732817\n",
      "Iteration 23, loss = 0.05456730\n",
      "Iteration 24, loss = 0.05222412\n",
      "Iteration 25, loss = 0.05012153\n",
      "Iteration 26, loss = 0.04823142\n",
      "Iteration 27, loss = 0.04635019\n",
      "Iteration 28, loss = 0.04484730\n",
      "Iteration 29, loss = 0.04308214\n",
      "Iteration 30, loss = 0.04177354\n",
      "Iteration 31, loss = 0.04040022\n",
      "Iteration 32, loss = 0.03967207\n",
      "Iteration 33, loss = 0.03833892\n",
      "Iteration 34, loss = 0.03755048\n",
      "Iteration 35, loss = 0.03676250\n",
      "Iteration 36, loss = 0.03580538\n",
      "Iteration 37, loss = 0.03493546\n",
      "Iteration 38, loss = 0.03428676\n",
      "Iteration 39, loss = 0.03393516\n",
      "Iteration 40, loss = 0.03287699\n",
      "Iteration 41, loss = 0.03221297\n",
      "Iteration 42, loss = 0.03196821\n",
      "Iteration 43, loss = 0.03121267\n",
      "Iteration 44, loss = 0.03057138\n",
      "Iteration 45, loss = 0.03016142\n",
      "Iteration 46, loss = 0.02947047\n",
      "LinearRegression\n",
      "NeuralNetwork\n",
      "Iteration 47, loss = 0.02924807\n",
      "Iteration 48, loss = 0.02872257\n",
      "Iteration 49, loss = 0.02828715\n",
      "Iteration 50, loss = 0.02793971\n",
      "Iteration 51, loss = 0.02797827\n",
      "Iteration 52, loss = 0.02705268\n",
      "Iteration 53, loss = 0.02660194\n",
      "Iteration 54, loss = 0.02630790\n",
      "Iteration 55, loss = 0.02613570\n",
      "Iteration 56, loss = 0.02572870\n",
      "Iteration 57, loss = 0.02600728\n",
      "Iteration 58, loss = 0.02513268\n",
      "Iteration 59, loss = 0.02498415\n",
      "Iteration 60, loss = 0.02470448\n",
      "Iteration 61, loss = 0.02443019\n",
      "Iteration 62, loss = 0.02425093\n",
      "Iteration 63, loss = 0.02383238\n",
      "Iteration 64, loss = 0.02372451\n",
      "Iteration 65, loss = 0.02352938\n",
      "Iteration 66, loss = 0.02338483\n",
      "Iteration 67, loss = 0.02305728\n",
      "Iteration 68, loss = 0.02291985\n",
      "Iteration 69, loss = 0.02263814\n",
      "Iteration 70, loss = 0.02305234\n",
      "Iteration 71, loss = 0.02257164\n",
      "Iteration 72, loss = 0.02249678\n",
      "Iteration 73, loss = 0.02264919\n",
      "Iteration 74, loss = 0.02185865\n",
      "Iteration 75, loss = 0.02169943\n",
      "Iteration 76, loss = 0.02147328\n",
      "Iteration 77, loss = 0.02131338\n",
      "Iteration 78, loss = 0.02150912\n",
      "Iteration 79, loss = 0.02133149\n",
      "Iteration 80, loss = 0.02084279\n",
      "Iteration 81, loss = 0.02091675\n",
      "Iteration 82, loss = 0.02063752\n",
      "Iteration 83, loss = 0.02038964\n",
      "Iteration 84, loss = 0.02070027\n",
      "Iteration 85, loss = 0.02019454\n",
      "Iteration 86, loss = 0.02006457\n",
      "Iteration 87, loss = 0.01970792\n",
      "Iteration 88, loss = 0.01973819\n",
      "Iteration 89, loss = 0.01968204\n",
      "Iteration 90, loss = 0.01965270\n",
      "Iteration 91, loss = 0.01941793\n",
      "Iteration 92, loss = 0.01918709\n",
      "Iteration 93, loss = 0.01916810\n",
      "Iteration 94, loss = 0.01933818\n",
      "Iteration 95, loss = 0.01907545\n",
      "Iteration 96, loss = 0.01929597\n",
      "Iteration 97, loss = 0.01855722\n",
      "Iteration 98, loss = 0.01856724\n",
      "Iteration 99, loss = 0.01878375\n",
      "Iteration 100, loss = 0.01868946\n",
      "Iteration 101, loss = 0.01829354\n",
      "Iteration 102, loss = 0.01858973\n",
      "Iteration 103, loss = 0.01831339\n",
      "Iteration 104, loss = 0.01811262\n",
      "Iteration 105, loss = 0.01814914\n",
      "Iteration 106, loss = 0.01795271\n",
      "Iteration 107, loss = 0.01774360\n",
      "Iteration 108, loss = 0.01775692\n",
      "Iteration 109, loss = 0.01765533\n",
      "Iteration 110, loss = 0.01788381\n",
      "Iteration 111, loss = 0.01743161\n",
      "Iteration 112, loss = 0.01747299\n",
      "Iteration 113, loss = 0.01746941\n",
      "Iteration 114, loss = 0.01716748\n",
      "Iteration 115, loss = 0.01749553\n",
      "Iteration 116, loss = 0.01716706\n",
      "Iteration 117, loss = 0.01701998\n",
      "Iteration 118, loss = 0.01722833\n",
      "Iteration 119, loss = 0.01713855\n",
      "Iteration 120, loss = 0.01665505\n",
      "Iteration 121, loss = 0.01680052\n",
      "Iteration 122, loss = 0.01690046\n",
      "Iteration 123, loss = 0.01644100\n",
      "Iteration 124, loss = 0.01624896\n",
      "Iteration 125, loss = 0.01622181\n",
      "Iteration 126, loss = 0.01614399\n",
      "Iteration 127, loss = 0.01608109\n",
      "Iteration 128, loss = 0.01605919\n",
      "Iteration 129, loss = 0.01658300\n",
      "Iteration 130, loss = 0.01621517\n",
      "Iteration 131, loss = 0.01590823\n",
      "Iteration 132, loss = 0.01579208\n",
      "Iteration 133, loss = 0.01583206\n",
      "Iteration 134, loss = 0.01558209\n",
      "Iteration 135, loss = 0.01541842\n",
      "Iteration 136, loss = 0.01542622\n",
      "Iteration 137, loss = 0.01533775\n",
      "Iteration 138, loss = 0.01543547\n",
      "Iteration 139, loss = 0.01541465\n",
      "Iteration 140, loss = 0.01500375\n",
      "Iteration 141, loss = 0.01508854\n",
      "Iteration 142, loss = 0.01557684\n",
      "Iteration 143, loss = 0.01500384\n",
      "Iteration 144, loss = 0.01479864\n",
      "Iteration 145, loss = 0.01471221\n",
      "Iteration 146, loss = 0.01472428\n",
      "Iteration 147, loss = 0.01461271\n",
      "Iteration 148, loss = 0.01452820\n",
      "Iteration 149, loss = 0.01471559\n",
      "Iteration 150, loss = 0.01446383\n",
      "Iteration 151, loss = 0.01453375\n",
      "Iteration 152, loss = 0.01445096\n",
      "Iteration 153, loss = 0.01441521\n",
      "Iteration 154, loss = 0.01441430\n",
      "Iteration 155, loss = 0.01427542\n",
      "Iteration 156, loss = 0.01408833\n",
      "Iteration 157, loss = 0.01427963\n",
      "Iteration 158, loss = 0.01413269\n",
      "Iteration 159, loss = 0.01425962\n",
      "Iteration 160, loss = 0.01401092\n",
      "Iteration 161, loss = 0.01391371\n",
      "Iteration 162, loss = 0.01388427\n",
      "Iteration 163, loss = 0.01392043\n",
      "Iteration 164, loss = 0.01366868\n",
      "Iteration 165, loss = 0.01377227\n",
      "Iteration 166, loss = 0.01345854\n",
      "Iteration 167, loss = 0.01360338\n",
      "Iteration 168, loss = 0.01341556\n",
      "Iteration 169, loss = 0.01343081\n",
      "Iteration 170, loss = 0.01355994\n",
      "Iteration 171, loss = 0.01379305\n",
      "Iteration 172, loss = 0.01358095\n",
      "Iteration 173, loss = 0.01349383\n",
      "Iteration 174, loss = 0.01349465\n",
      "Iteration 175, loss = 0.01330960\n",
      "Iteration 176, loss = 0.01325675\n",
      "Iteration 177, loss = 0.01352630\n",
      "Iteration 178, loss = 0.01303875\n",
      "Iteration 179, loss = 0.01342345\n",
      "Iteration 180, loss = 0.01314771\n",
      "Iteration 181, loss = 0.01324785\n",
      "Iteration 182, loss = 0.01295473\n",
      "Iteration 183, loss = 0.01284612\n",
      "Iteration 184, loss = 0.01293614\n",
      "Iteration 185, loss = 0.01283357\n",
      "Iteration 186, loss = 0.01334360\n",
      "Iteration 187, loss = 0.01290791\n",
      "Iteration 188, loss = 0.01294142\n",
      "Iteration 189, loss = 0.01286590\n",
      "Iteration 190, loss = 0.01275219\n",
      "Iteration 191, loss = 0.01253397\n",
      "Iteration 192, loss = 0.01253889\n",
      "Iteration 193, loss = 0.01255391\n",
      "Iteration 194, loss = 0.01235683\n",
      "Iteration 195, loss = 0.01233935\n",
      "Iteration 196, loss = 0.01231582\n",
      "Iteration 197, loss = 0.01229042\n",
      "Iteration 198, loss = 0.01220901\n",
      "Iteration 199, loss = 0.01221975\n",
      "Iteration 200, loss = 0.01232473\n",
      "Iteration 201, loss = 0.01214878\n",
      "Iteration 202, loss = 0.01210891\n",
      "Iteration 203, loss = 0.01205168\n",
      "Iteration 204, loss = 0.01205162\n",
      "Iteration 205, loss = 0.01208692\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.44976404\n",
      "Iteration 2, loss = 0.33482426\n",
      "Iteration 3, loss = 0.26733383\n",
      "Iteration 4, loss = 0.23176380\n",
      "Iteration 5, loss = 0.20436286\n",
      "Iteration 6, loss = 0.18034207\n",
      "Iteration 7, loss = 0.16248878\n",
      "Iteration 8, loss = 0.15113515\n",
      "Iteration 9, loss = 0.13946712\n",
      "Iteration 10, loss = 0.12778798\n",
      "Iteration 11, loss = 0.11654424\n",
      "Iteration 12, loss = 0.10608287\n",
      "Iteration 13, loss = 0.09628932\n",
      "Iteration 14, loss = 0.08755476\n",
      "Iteration 15, loss = 0.07948183\n",
      "Iteration 16, loss = 0.07356740\n",
      "Iteration 17, loss = 0.06777461\n",
      "Iteration 18, loss = 0.06356523\n",
      "Iteration 19, loss = 0.05950453\n",
      "Iteration 20, loss = 0.05623670\n",
      "Iteration 21, loss = 0.05267694\n",
      "Iteration 22, loss = 0.05016734\n",
      "Iteration 23, loss = 0.04787708\n",
      "Iteration 24, loss = 0.04577378\n",
      "Iteration 25, loss = 0.04407467\n",
      "Iteration 26, loss = 0.04217512\n",
      "Iteration 27, loss = 0.04046984\n",
      "Iteration 28, loss = 0.03891518\n",
      "Iteration 29, loss = 0.03774295\n",
      "Iteration 30, loss = 0.03675201\n",
      "Iteration 31, loss = 0.03542609\n",
      "Iteration 32, loss = 0.03424462\n",
      "Iteration 33, loss = 0.03302100\n",
      "Iteration 34, loss = 0.03284876\n",
      "Iteration 35, loss = 0.03173191\n",
      "Iteration 36, loss = 0.03066143\n",
      "Iteration 37, loss = 0.03003940\n",
      "Iteration 38, loss = 0.02973333\n",
      "Iteration 39, loss = 0.02925488\n",
      "Iteration 40, loss = 0.02928075\n",
      "Iteration 41, loss = 0.02773442\n",
      "Iteration 42, loss = 0.02763992\n",
      "Iteration 43, loss = 0.02687330\n",
      "Iteration 44, loss = 0.02635128\n",
      "Iteration 45, loss = 0.02562589\n",
      "Iteration 46, loss = 0.02533201\n",
      "Iteration 47, loss = 0.02544407\n",
      "Iteration 48, loss = 0.02469035\n",
      "Iteration 49, loss = 0.02413283\n",
      "Iteration 50, loss = 0.02359529\n",
      "Iteration 51, loss = 0.02345047\n",
      "Iteration 52, loss = 0.02282818\n",
      "Iteration 53, loss = 0.02260707\n",
      "Iteration 54, loss = 0.02224630\n",
      "Iteration 55, loss = 0.02195748\n",
      "Iteration 56, loss = 0.02151720\n",
      "Iteration 57, loss = 0.02172084\n",
      "Iteration 58, loss = 0.02105637\n",
      "Iteration 59, loss = 0.02084096\n",
      "Iteration 60, loss = 0.02052028\n",
      "Iteration 61, loss = 0.02020429\n",
      "Iteration 62, loss = 0.01979576\n",
      "Iteration 63, loss = 0.01975591\n",
      "Iteration 64, loss = 0.01951380\n",
      "Iteration 65, loss = 0.01914476\n",
      "Iteration 66, loss = 0.01895892\n",
      "Iteration 67, loss = 0.01895410\n",
      "Iteration 68, loss = 0.01917371\n",
      "Iteration 69, loss = 0.01878703\n",
      "Iteration 70, loss = 0.01852387\n",
      "Iteration 71, loss = 0.01822860\n",
      "Iteration 72, loss = 0.01776454\n",
      "Iteration 73, loss = 0.01745904\n",
      "Iteration 74, loss = 0.01722683\n",
      "Iteration 75, loss = 0.01706344\n",
      "Iteration 76, loss = 0.01684063\n",
      "Iteration 77, loss = 0.01685768\n",
      "Iteration 78, loss = 0.01695124\n",
      "Iteration 79, loss = 0.01655928\n",
      "Iteration 80, loss = 0.01656697\n",
      "Iteration 81, loss = 0.01627732\n",
      "Iteration 82, loss = 0.01609291\n",
      "Iteration 83, loss = 0.01617915\n",
      "Iteration 84, loss = 0.01590880\n",
      "Iteration 85, loss = 0.01569004\n",
      "Iteration 86, loss = 0.01562575\n",
      "Iteration 87, loss = 0.01539445\n",
      "Iteration 88, loss = 0.01521280\n",
      "Iteration 89, loss = 0.01546349\n",
      "Iteration 90, loss = 0.01513890\n",
      "Iteration 91, loss = 0.01496977\n",
      "Iteration 92, loss = 0.01551720\n",
      "Iteration 93, loss = 0.01488573\n",
      "Iteration 94, loss = 0.01462656\n",
      "Iteration 95, loss = 0.01520092\n",
      "Iteration 96, loss = 0.01543813\n",
      "Iteration 97, loss = 0.01469314\n",
      "Iteration 98, loss = 0.01419661\n",
      "Iteration 99, loss = 0.01388192\n",
      "Iteration 100, loss = 0.01387782\n",
      "Iteration 101, loss = 0.01376789\n",
      "Iteration 102, loss = 0.01353593\n",
      "Iteration 103, loss = 0.01363429\n",
      "Iteration 104, loss = 0.01322630\n",
      "Iteration 105, loss = 0.01336222\n",
      "Iteration 106, loss = 0.01326168\n",
      "Iteration 107, loss = 0.01333570\n",
      "Iteration 108, loss = 0.01365772\n",
      "Iteration 109, loss = 0.01317677\n",
      "Iteration 110, loss = 0.01297419\n",
      "Iteration 111, loss = 0.01267972\n",
      "Iteration 112, loss = 0.01253727\n",
      "Iteration 113, loss = 0.01251977\n",
      "Iteration 114, loss = 0.01261667\n",
      "Iteration 115, loss = 0.01220583\n",
      "Iteration 116, loss = 0.01202273\n",
      "Iteration 117, loss = 0.01230006\n",
      "Iteration 118, loss = 0.01226329\n",
      "Iteration 119, loss = 0.01245947\n",
      "Iteration 120, loss = 0.01212347\n",
      "Iteration 121, loss = 0.01164067\n",
      "Iteration 122, loss = 0.01149002\n",
      "Iteration 123, loss = 0.01133188\n",
      "Iteration 124, loss = 0.01130140\n",
      "Iteration 125, loss = 0.01104233\n",
      "Iteration 126, loss = 0.01170554\n",
      "Iteration 127, loss = 0.01143565\n",
      "Iteration 128, loss = 0.01131895\n",
      "Iteration 129, loss = 0.01125235\n",
      "Iteration 130, loss = 0.01082342\n",
      "Iteration 131, loss = 0.01045975\n",
      "Iteration 132, loss = 0.01069396\n",
      "Iteration 133, loss = 0.01050375\n",
      "Iteration 134, loss = 0.01039457\n",
      "Iteration 135, loss = 0.01064748\n",
      "Iteration 136, loss = 0.01015074\n",
      "Iteration 137, loss = 0.01017414\n",
      "Iteration 138, loss = 0.01039727\n",
      "Iteration 139, loss = 0.01002281\n",
      "Iteration 140, loss = 0.01016260\n",
      "Iteration 141, loss = 0.01006184\n",
      "Iteration 142, loss = 0.01009563\n",
      "Iteration 143, loss = 0.01020452\n",
      "Iteration 144, loss = 0.00980627\n",
      "Iteration 145, loss = 0.00987166\n",
      "Iteration 146, loss = 0.00963412\n",
      "Iteration 147, loss = 0.00950870\n",
      "Iteration 148, loss = 0.00963527\n",
      "Iteration 149, loss = 0.00965096\n",
      "Iteration 150, loss = 0.00954543\n",
      "Iteration 151, loss = 0.00944540\n",
      "Iteration 152, loss = 0.00928845\n",
      "Iteration 153, loss = 0.00921892\n",
      "Iteration 154, loss = 0.00938038\n",
      "Iteration 155, loss = 0.00900965\n",
      "Iteration 156, loss = 0.00899896\n",
      "Iteration 157, loss = 0.00891256\n",
      "Iteration 158, loss = 0.00892786\n",
      "Iteration 159, loss = 0.00880334\n",
      "Iteration 160, loss = 0.00884157\n",
      "Iteration 161, loss = 0.00867874\n",
      "Iteration 162, loss = 0.00856431\n",
      "Iteration 163, loss = 0.00872763\n",
      "Iteration 164, loss = 0.00860442\n",
      "Iteration 165, loss = 0.00839349\n",
      "Iteration 166, loss = 0.00835375\n",
      "Iteration 167, loss = 0.00836846\n",
      "Iteration 168, loss = 0.00834732\n",
      "Iteration 169, loss = 0.00848865\n",
      "Iteration 170, loss = 0.00819075\n",
      "Iteration 171, loss = 0.00804802\n",
      "Iteration 172, loss = 0.00843357\n",
      "Iteration 173, loss = 0.00843584\n",
      "Iteration 174, loss = 0.00806372\n",
      "Iteration 175, loss = 0.00831861\n",
      "Iteration 176, loss = 0.00855082\n",
      "Iteration 177, loss = 0.00995355\n",
      "Iteration 178, loss = 0.00937096\n",
      "Iteration 179, loss = 0.00913077\n",
      "Iteration 180, loss = 0.00862903\n",
      "Iteration 181, loss = 0.00829257\n",
      "Iteration 182, loss = 0.00795383\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.44976404\n",
      "Iteration 2, loss = 0.33482426\n",
      "Iteration 3, loss = 0.26733383\n",
      "Iteration 4, loss = 0.23176380\n",
      "Iteration 5, loss = 0.20436286\n",
      "Iteration 6, loss = 0.18034207\n",
      "Iteration 7, loss = 0.16248878\n",
      "Iteration 8, loss = 0.15113515\n",
      "Iteration 9, loss = 0.13946712\n",
      "Iteration 10, loss = 0.12778798\n",
      "Iteration 11, loss = 0.11654424\n",
      "Iteration 12, loss = 0.10608287\n",
      "Iteration 13, loss = 0.09628932\n",
      "Iteration 14, loss = 0.08755476\n",
      "Iteration 15, loss = 0.07948183\n",
      "Iteration 16, loss = 0.07356740\n",
      "Iteration 17, loss = 0.06777461\n",
      "Iteration 18, loss = 0.06356523\n",
      "Iteration 19, loss = 0.05950453\n",
      "Iteration 20, loss = 0.05623670\n",
      "Iteration 21, loss = 0.05267694\n",
      "Iteration 22, loss = 0.05016734\n",
      "Iteration 23, loss = 0.04787708\n",
      "Iteration 24, loss = 0.04577378\n",
      "Iteration 25, loss = 0.04407467\n",
      "Iteration 26, loss = 0.04217512\n",
      "Iteration 27, loss = 0.04046984\n",
      "Iteration 28, loss = 0.03891518\n",
      "Iteration 29, loss = 0.03774295\n",
      "Iteration 30, loss = 0.03675201\n",
      "Iteration 31, loss = 0.03542609\n",
      "Iteration 32, loss = 0.03424462\n",
      "Iteration 33, loss = 0.03302100\n",
      "Iteration 34, loss = 0.03284876\n",
      "Iteration 35, loss = 0.03173191\n",
      "Iteration 36, loss = 0.03066143\n",
      "Iteration 37, loss = 0.03003940\n",
      "Iteration 38, loss = 0.02973333\n",
      "Iteration 39, loss = 0.02925488\n",
      "Iteration 40, loss = 0.02928075\n",
      "Iteration 41, loss = 0.02773442\n",
      "Iteration 42, loss = 0.02763992\n",
      "Iteration 43, loss = 0.02687330\n",
      "Iteration 44, loss = 0.02635128\n",
      "Iteration 45, loss = 0.02562589\n",
      "Iteration 46, loss = 0.02533201\n",
      "Iteration 47, loss = 0.02544407\n",
      "Iteration 48, loss = 0.02469035\n",
      "Iteration 49, loss = 0.02413283\n",
      "Iteration 50, loss = 0.02359529\n",
      "Iteration 51, loss = 0.02345047\n",
      "Iteration 52, loss = 0.02282818\n",
      "Iteration 53, loss = 0.02260707\n",
      "Iteration 54, loss = 0.02224630\n",
      "Iteration 55, loss = 0.02195748\n",
      "Iteration 56, loss = 0.02151720\n",
      "Iteration 57, loss = 0.02172084\n",
      "Iteration 58, loss = 0.02105637\n",
      "Iteration 59, loss = 0.02084096\n",
      "Iteration 60, loss = 0.02052028\n",
      "Iteration 61, loss = 0.02020429\n",
      "Iteration 62, loss = 0.01979576\n",
      "Iteration 63, loss = 0.01975591\n",
      "Iteration 64, loss = 0.01951380\n",
      "Iteration 65, loss = 0.01914476\n",
      "Iteration 66, loss = 0.01895892\n",
      "Iteration 67, loss = 0.01895410\n",
      "Iteration 68, loss = 0.01917371\n",
      "Iteration 69, loss = 0.01878703\n",
      "Iteration 70, loss = 0.01852387\n",
      "Iteration 71, loss = 0.01822860\n",
      "Iteration 72, loss = 0.01776454\n",
      "Iteration 73, loss = 0.01745904\n",
      "Iteration 74, loss = 0.01722683\n",
      "Iteration 75, loss = 0.01706344\n",
      "Iteration 76, loss = 0.01684063\n",
      "Iteration 77, loss = 0.01685768\n",
      "Iteration 78, loss = 0.01695124\n",
      "Iteration 79, loss = 0.01655928\n",
      "Iteration 80, loss = 0.01656697\n",
      "Iteration 81, loss = 0.01627732\n",
      "Iteration 82, loss = 0.01609291\n",
      "Iteration 83, loss = 0.01617915\n",
      "Iteration 84, loss = 0.01590880\n",
      "Iteration 85, loss = 0.01569004\n",
      "Iteration 86, loss = 0.01562575\n",
      "Iteration 87, loss = 0.01539445\n",
      "Iteration 88, loss = 0.01521280\n",
      "Iteration 89, loss = 0.01546349\n",
      "Iteration 90, loss = 0.01513890\n",
      "Iteration 91, loss = 0.01496977\n",
      "Iteration 92, loss = 0.01551720\n",
      "Iteration 93, loss = 0.01488573\n",
      "Iteration 94, loss = 0.01462656\n",
      "Iteration 95, loss = 0.01520092\n",
      "Iteration 96, loss = 0.01543813\n",
      "Iteration 97, loss = 0.01469314\n",
      "Iteration 98, loss = 0.01419661\n",
      "Iteration 99, loss = 0.01388192\n",
      "Iteration 100, loss = 0.01387782\n",
      "Iteration 101, loss = 0.01376789\n",
      "Iteration 102, loss = 0.01353593\n",
      "Iteration 103, loss = 0.01363429\n",
      "Iteration 104, loss = 0.01322630\n",
      "Iteration 105, loss = 0.01336222\n",
      "Iteration 106, loss = 0.01326168\n",
      "Iteration 107, loss = 0.01333570\n",
      "Iteration 108, loss = 0.01365772\n",
      "Iteration 109, loss = 0.01317677\n",
      "Iteration 110, loss = 0.01297419\n",
      "Iteration 111, loss = 0.01267972\n",
      "Iteration 112, loss = 0.01253727\n",
      "Iteration 113, loss = 0.01251977\n",
      "Iteration 114, loss = 0.01261667\n",
      "Iteration 115, loss = 0.01220583\n",
      "Iteration 116, loss = 0.01202273\n",
      "Iteration 117, loss = 0.01230006\n",
      "Iteration 118, loss = 0.01226329\n",
      "Iteration 119, loss = 0.01245947\n",
      "Iteration 120, loss = 0.01212347\n",
      "Iteration 121, loss = 0.01164067\n",
      "Iteration 122, loss = 0.01149002\n",
      "Iteration 123, loss = 0.01133188\n",
      "Iteration 124, loss = 0.01130140\n",
      "Iteration 125, loss = 0.01104233\n",
      "Iteration 126, loss = 0.01170554\n",
      "Iteration 127, loss = 0.01143565\n",
      "Iteration 128, loss = 0.01131895\n",
      "Iteration 129, loss = 0.01125235\n",
      "Iteration 130, loss = 0.01082342\n",
      "Iteration 131, loss = 0.01045975\n",
      "Iteration 132, loss = 0.01069396\n",
      "Iteration 133, loss = 0.01050375\n",
      "Iteration 134, loss = 0.01039457\n",
      "Iteration 135, loss = 0.01064748\n",
      "Iteration 136, loss = 0.01015074\n",
      "Iteration 137, loss = 0.01017414\n",
      "Iteration 138, loss = 0.01039727\n",
      "Iteration 139, loss = 0.01002281\n",
      "Iteration 140, loss = 0.01016260\n",
      "Iteration 141, loss = 0.01006184\n",
      "Iteration 142, loss = 0.01009563\n",
      "Iteration 143, loss = 0.01020452\n",
      "Iteration 144, loss = 0.00980627\n",
      "Iteration 145, loss = 0.00987166\n",
      "Iteration 146, loss = 0.00963412\n",
      "Iteration 147, loss = 0.00950870\n",
      "Iteration 148, loss = 0.00963527\n",
      "Iteration 149, loss = 0.00965096\n",
      "Iteration 150, loss = 0.00954543\n",
      "Iteration 151, loss = 0.00944540\n",
      "Iteration 152, loss = 0.00928845\n",
      "Iteration 153, loss = 0.00921892\n",
      "Iteration 154, loss = 0.00938038\n",
      "Iteration 155, loss = 0.00900965\n",
      "Iteration 156, loss = 0.00899896\n",
      "Iteration 157, loss = 0.00891256\n",
      "Iteration 158, loss = 0.00892786\n",
      "LinearRegression\n",
      "NeuralNetwork\n",
      "Iteration 159, loss = 0.00880334\n",
      "Iteration 160, loss = 0.00884157\n",
      "Iteration 161, loss = 0.00867874\n",
      "Iteration 162, loss = 0.00856431\n",
      "Iteration 163, loss = 0.00872763\n",
      "Iteration 164, loss = 0.00860442\n",
      "Iteration 165, loss = 0.00839349\n",
      "Iteration 166, loss = 0.00835375\n",
      "Iteration 167, loss = 0.00836846\n",
      "Iteration 168, loss = 0.00834732\n",
      "Iteration 169, loss = 0.00848865\n",
      "Iteration 170, loss = 0.00819075\n",
      "Iteration 171, loss = 0.00804802\n",
      "Iteration 172, loss = 0.00843357\n",
      "Iteration 173, loss = 0.00843584\n",
      "Iteration 174, loss = 0.00806372\n",
      "Iteration 175, loss = 0.00831861\n",
      "Iteration 176, loss = 0.00855082\n",
      "Iteration 177, loss = 0.00995355\n",
      "Iteration 178, loss = 0.00937096\n",
      "Iteration 179, loss = 0.00913077\n",
      "Iteration 180, loss = 0.00862903\n",
      "Iteration 181, loss = 0.00829257\n",
      "Iteration 182, loss = 0.00795383\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.44976404\n",
      "Iteration 2, loss = 0.33482426\n",
      "Iteration 3, loss = 0.26733383\n",
      "Iteration 4, loss = 0.23176380\n",
      "Iteration 5, loss = 0.20436286\n",
      "Iteration 6, loss = 0.18034207\n",
      "Iteration 7, loss = 0.16248878\n",
      "Iteration 8, loss = 0.15113515\n",
      "Iteration 9, loss = 0.13946712\n",
      "Iteration 10, loss = 0.12778798\n",
      "Iteration 11, loss = 0.11654424\n",
      "Iteration 12, loss = 0.10608287\n",
      "Iteration 13, loss = 0.09628932\n",
      "Iteration 14, loss = 0.08755476\n",
      "Iteration 15, loss = 0.07948183\n",
      "Iteration 16, loss = 0.07356740\n",
      "Iteration 17, loss = 0.06777461\n",
      "Iteration 18, loss = 0.06356523\n",
      "Iteration 19, loss = 0.05950453\n",
      "Iteration 20, loss = 0.05623670\n",
      "Iteration 21, loss = 0.05267694\n",
      "Iteration 22, loss = 0.05016734\n",
      "Iteration 23, loss = 0.04787708\n",
      "Iteration 24, loss = 0.04577378\n",
      "Iteration 25, loss = 0.04407467\n",
      "Iteration 26, loss = 0.04217512\n",
      "Iteration 27, loss = 0.04046984\n",
      "Iteration 28, loss = 0.03891518\n",
      "Iteration 29, loss = 0.03774295\n",
      "Iteration 30, loss = 0.03675201\n",
      "Iteration 31, loss = 0.03542609\n",
      "Iteration 32, loss = 0.03424462\n",
      "Iteration 33, loss = 0.03302100\n",
      "Iteration 34, loss = 0.03284876\n",
      "Iteration 35, loss = 0.03173191\n",
      "Iteration 36, loss = 0.03066143\n",
      "Iteration 37, loss = 0.03003940\n",
      "Iteration 38, loss = 0.02973333\n",
      "Iteration 39, loss = 0.02925488\n",
      "Iteration 40, loss = 0.02928075\n",
      "Iteration 41, loss = 0.02773442\n",
      "Iteration 42, loss = 0.02763992\n",
      "Iteration 43, loss = 0.02687330\n",
      "Iteration 44, loss = 0.02635128\n",
      "Iteration 45, loss = 0.02562589\n",
      "Iteration 46, loss = 0.02533201\n",
      "Iteration 47, loss = 0.02544407\n",
      "Iteration 48, loss = 0.02469035\n",
      "Iteration 49, loss = 0.02413283\n",
      "Iteration 50, loss = 0.02359529\n",
      "Iteration 51, loss = 0.02345047\n",
      "Iteration 52, loss = 0.02282818\n",
      "Iteration 53, loss = 0.02260707\n",
      "Iteration 54, loss = 0.02224630\n",
      "Iteration 55, loss = 0.02195748\n",
      "Iteration 56, loss = 0.02151720\n",
      "Iteration 57, loss = 0.02172084\n",
      "Iteration 58, loss = 0.02105637\n",
      "Iteration 59, loss = 0.02084096\n",
      "Iteration 60, loss = 0.02052028\n",
      "Iteration 61, loss = 0.02020429\n",
      "Iteration 62, loss = 0.01979576\n",
      "Iteration 63, loss = 0.01975591\n",
      "Iteration 64, loss = 0.01951380\n",
      "Iteration 65, loss = 0.01914476\n",
      "Iteration 66, loss = 0.01895892\n",
      "Iteration 67, loss = 0.01895410\n",
      "Iteration 68, loss = 0.01917371\n",
      "Iteration 69, loss = 0.01878703\n",
      "Iteration 70, loss = 0.01852387\n",
      "Iteration 71, loss = 0.01822860\n",
      "Iteration 72, loss = 0.01776454\n",
      "Iteration 73, loss = 0.01745904\n",
      "Iteration 74, loss = 0.01722683\n",
      "Iteration 75, loss = 0.01706344\n",
      "Iteration 76, loss = 0.01684063\n",
      "Iteration 77, loss = 0.01685768\n",
      "Iteration 78, loss = 0.01695124\n",
      "Iteration 79, loss = 0.01655928\n",
      "Iteration 80, loss = 0.01656697\n",
      "Iteration 81, loss = 0.01627732\n",
      "Iteration 82, loss = 0.01609291\n",
      "Iteration 83, loss = 0.01617915\n",
      "Iteration 84, loss = 0.01590880\n",
      "Iteration 85, loss = 0.01569004\n",
      "Iteration 86, loss = 0.01562575\n",
      "Iteration 87, loss = 0.01539445\n",
      "Iteration 88, loss = 0.01521280\n",
      "Iteration 89, loss = 0.01546349\n",
      "Iteration 90, loss = 0.01513890\n",
      "Iteration 91, loss = 0.01496977\n",
      "Iteration 92, loss = 0.01551720\n",
      "Iteration 93, loss = 0.01488573\n",
      "Iteration 94, loss = 0.01462656\n",
      "Iteration 95, loss = 0.01520092\n",
      "Iteration 96, loss = 0.01543813\n",
      "Iteration 97, loss = 0.01469314\n",
      "Iteration 98, loss = 0.01419661\n",
      "Iteration 99, loss = 0.01388192\n",
      "Iteration 100, loss = 0.01387782\n",
      "Iteration 101, loss = 0.01376789\n",
      "Iteration 102, loss = 0.01353593\n",
      "Iteration 103, loss = 0.01363429\n",
      "Iteration 104, loss = 0.01322630\n",
      "Iteration 105, loss = 0.01336222\n",
      "Iteration 106, loss = 0.01326168\n",
      "Iteration 107, loss = 0.01333570\n",
      "Iteration 108, loss = 0.01365772\n",
      "Iteration 109, loss = 0.01317677\n",
      "Iteration 110, loss = 0.01297419\n",
      "Iteration 111, loss = 0.01267972\n",
      "Iteration 112, loss = 0.01253727\n",
      "Iteration 113, loss = 0.01251977\n",
      "Iteration 114, loss = 0.01261667\n",
      "Iteration 115, loss = 0.01220583\n",
      "Iteration 116, loss = 0.01202273\n",
      "Iteration 117, loss = 0.01230006\n",
      "Iteration 118, loss = 0.01226329\n",
      "Iteration 119, loss = 0.01245947\n",
      "Iteration 120, loss = 0.01212347\n",
      "Iteration 121, loss = 0.01164067\n",
      "Iteration 122, loss = 0.01149002\n",
      "Iteration 123, loss = 0.01133188\n",
      "Iteration 124, loss = 0.01130140\n",
      "Iteration 125, loss = 0.01104233\n",
      "Iteration 126, loss = 0.01170554\n",
      "Iteration 127, loss = 0.01143565\n",
      "Iteration 128, loss = 0.01131895\n",
      "Iteration 129, loss = 0.01125235\n",
      "Iteration 130, loss = 0.01082342\n",
      "Iteration 131, loss = 0.01045975\n",
      "Iteration 132, loss = 0.01069396\n",
      "Iteration 133, loss = 0.01050375\n",
      "Iteration 134, loss = 0.01039457\n",
      "Iteration 135, loss = 0.01064748\n",
      "Iteration 136, loss = 0.01015074\n",
      "Iteration 137, loss = 0.01017414\n",
      "Iteration 138, loss = 0.01039727\n",
      "Iteration 139, loss = 0.01002281\n",
      "Iteration 140, loss = 0.01016260\n",
      "Iteration 141, loss = 0.01006184\n",
      "Iteration 142, loss = 0.01009563\n",
      "Iteration 143, loss = 0.01020452\n",
      "Iteration 144, loss = 0.00980627\n",
      "Iteration 145, loss = 0.00987166\n",
      "Iteration 146, loss = 0.00963412\n",
      "Iteration 147, loss = 0.00950870\n",
      "Iteration 148, loss = 0.00963527\n",
      "Iteration 149, loss = 0.00965096\n",
      "Iteration 150, loss = 0.00954543\n",
      "Iteration 151, loss = 0.00944540\n",
      "Iteration 152, loss = 0.00928845\n",
      "Iteration 153, loss = 0.00921892\n",
      "Iteration 154, loss = 0.00938038\n",
      "Iteration 155, loss = 0.00900965\n",
      "Iteration 156, loss = 0.00899896\n",
      "Iteration 157, loss = 0.00891256\n",
      "Iteration 158, loss = 0.00892786\n",
      "Iteration 159, loss = 0.00880334\n",
      "Iteration 160, loss = 0.00884157\n",
      "Iteration 161, loss = 0.00867874\n",
      "Iteration 162, loss = 0.00856431\n",
      "Iteration 163, loss = 0.00872763\n",
      "Iteration 164, loss = 0.00860442\n",
      "Iteration 165, loss = 0.00839349\n",
      "Iteration 166, loss = 0.00835375\n",
      "Iteration 167, loss = 0.00836846\n",
      "Iteration 168, loss = 0.00834732\n",
      "Iteration 169, loss = 0.00848865\n",
      "Iteration 170, loss = 0.00819075\n",
      "Iteration 171, loss = 0.00804802\n",
      "Iteration 172, loss = 0.00843357\n",
      "Iteration 173, loss = 0.00843584\n",
      "Iteration 174, loss = 0.00806372\n",
      "Iteration 175, loss = 0.00831861\n",
      "Iteration 176, loss = 0.00855082\n",
      "Iteration 177, loss = 0.00995355\n",
      "Iteration 178, loss = 0.00937096\n",
      "Iteration 179, loss = 0.00913077\n",
      "Iteration 180, loss = 0.00862903\n",
      "Iteration 181, loss = 0.00829257\n",
      "Iteration 182, loss = 0.00795383\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.44304542\n",
      "Iteration 2, loss = 0.33482287\n",
      "Iteration 3, loss = 0.27658335\n",
      "Iteration 4, loss = 0.23432735\n",
      "Iteration 5, loss = 0.20469056\n",
      "Iteration 6, loss = 0.18218226\n",
      "Iteration 7, loss = 0.16514129\n",
      "Iteration 8, loss = 0.15162352\n",
      "Iteration 9, loss = 0.13772224\n",
      "Iteration 10, loss = 0.12573544\n",
      "Iteration 11, loss = 0.11462134\n",
      "Iteration 12, loss = 0.10511500\n",
      "Iteration 13, loss = 0.09550367\n",
      "Iteration 14, loss = 0.08827227\n",
      "Iteration 15, loss = 0.08111413\n",
      "Iteration 16, loss = 0.07557755\n",
      "Iteration 17, loss = 0.07061241\n",
      "Iteration 18, loss = 0.06630399\n",
      "Iteration 19, loss = 0.06325588\n",
      "Iteration 20, loss = 0.05889145\n",
      "Iteration 21, loss = 0.05600828\n",
      "Iteration 22, loss = 0.05311228\n",
      "Iteration 23, loss = 0.05037045\n",
      "Iteration 24, loss = 0.04877396\n",
      "Iteration 25, loss = 0.04594823\n",
      "Iteration 26, loss = 0.04401149\n",
      "Iteration 27, loss = 0.04240181\n",
      "Iteration 28, loss = 0.04062057\n",
      "Iteration 29, loss = 0.03933759\n",
      "Iteration 30, loss = 0.03796509\n",
      "Iteration 31, loss = 0.03632067\n",
      "Iteration 32, loss = 0.03583427\n",
      "Iteration 33, loss = 0.03438873\n",
      "Iteration 34, loss = 0.03369679\n",
      "Iteration 35, loss = 0.03253175\n",
      "Iteration 36, loss = 0.03169115\n",
      "Iteration 37, loss = 0.03145527\n",
      "Iteration 38, loss = 0.03045871\n",
      "Iteration 39, loss = 0.02966230\n",
      "Iteration 40, loss = 0.02916590\n",
      "Iteration 41, loss = 0.02865195\n",
      "Iteration 42, loss = 0.02788878\n",
      "Iteration 43, loss = 0.02753286\n",
      "Iteration 44, loss = 0.02688258\n",
      "Iteration 45, loss = 0.02679442\n",
      "Iteration 46, loss = 0.02627510\n",
      "Iteration 47, loss = 0.02576807\n",
      "Iteration 48, loss = 0.02531065\n",
      "Iteration 49, loss = 0.02497885\n",
      "Iteration 50, loss = 0.02451083\n",
      "Iteration 51, loss = 0.02430999\n",
      "Iteration 52, loss = 0.02420325\n",
      "Iteration 53, loss = 0.02369290\n",
      "Iteration 54, loss = 0.02346037\n",
      "Iteration 55, loss = 0.02327858\n",
      "Iteration 56, loss = 0.02313564\n",
      "Iteration 57, loss = 0.02261938\n",
      "Iteration 58, loss = 0.02243515\n",
      "Iteration 59, loss = 0.02203557\n",
      "Iteration 60, loss = 0.02220041\n",
      "Iteration 61, loss = 0.02190465\n",
      "Iteration 62, loss = 0.02143532\n",
      "Iteration 63, loss = 0.02156063\n",
      "Iteration 64, loss = 0.02186024\n",
      "Iteration 65, loss = 0.02100200\n",
      "Iteration 66, loss = 0.02066728\n",
      "Iteration 67, loss = 0.02066031\n",
      "Iteration 68, loss = 0.02041851\n",
      "Iteration 69, loss = 0.01996640\n",
      "Iteration 70, loss = 0.02011332\n",
      "Iteration 71, loss = 0.01978578\n",
      "Iteration 72, loss = 0.01943283\n",
      "Iteration 73, loss = 0.01922834\n",
      "Iteration 74, loss = 0.01918697\n",
      "Iteration 75, loss = 0.01926940\n",
      "Iteration 76, loss = 0.01929062\n",
      "Iteration 77, loss = 0.01884556\n",
      "Iteration 78, loss = 0.01858790\n",
      "Iteration 79, loss = 0.01863388\n",
      "Iteration 80, loss = 0.01836955\n",
      "Iteration 81, loss = 0.01814990\n",
      "Iteration 82, loss = 0.01816821\n",
      "Iteration 83, loss = 0.01794568\n",
      "Iteration 84, loss = 0.01826330\n",
      "Iteration 85, loss = 0.01791804\n",
      "Iteration 86, loss = 0.01775262\n",
      "Iteration 87, loss = 0.01791212\n",
      "Iteration 88, loss = 0.01762798\n",
      "Iteration 89, loss = 0.01740506\n",
      "Iteration 90, loss = 0.01727339\n",
      "Iteration 91, loss = 0.01755259\n",
      "Iteration 92, loss = 0.01860887\n",
      "Iteration 93, loss = 0.01769753\n",
      "Iteration 94, loss = 0.01777680\n",
      "Iteration 95, loss = 0.01735030\n",
      "Iteration 96, loss = 0.01730407\n",
      "Iteration 97, loss = 0.01659497\n",
      "Iteration 98, loss = 0.01655680\n",
      "Iteration 99, loss = 0.01705038\n",
      "Iteration 100, loss = 0.01632131\n",
      "Iteration 101, loss = 0.01627353\n",
      "Iteration 102, loss = 0.01625071\n",
      "Iteration 103, loss = 0.01599952\n",
      "Iteration 104, loss = 0.01608630\n",
      "Iteration 105, loss = 0.01590322\n",
      "Iteration 106, loss = 0.01590418\n",
      "Iteration 107, loss = 0.01614259\n",
      "Iteration 108, loss = 0.01590242\n",
      "Iteration 109, loss = 0.01581235\n",
      "Iteration 110, loss = 0.01547168\n",
      "Iteration 111, loss = 0.01561461\n",
      "Iteration 112, loss = 0.01531713\n",
      "Iteration 113, loss = 0.01538386\n",
      "Iteration 114, loss = 0.01523050\n",
      "Iteration 115, loss = 0.01515671\n",
      "Iteration 116, loss = 0.01508110\n",
      "Iteration 117, loss = 0.01501435\n",
      "Iteration 118, loss = 0.01503033\n",
      "Iteration 119, loss = 0.01492111\n",
      "Iteration 120, loss = 0.01506867\n",
      "Iteration 121, loss = 0.01510741\n",
      "Iteration 122, loss = 0.01518939\n",
      "Iteration 123, loss = 0.01513330\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.44304542\n",
      "Iteration 2, loss = 0.33482287\n",
      "Iteration 3, loss = 0.27658335\n",
      "Iteration 4, loss = 0.23432735\n",
      "Iteration 5, loss = 0.20469056\n",
      "Iteration 6, loss = 0.18218226\n",
      "Iteration 7, loss = 0.16514129\n",
      "Iteration 8, loss = 0.15162352\n",
      "Iteration 9, loss = 0.13772224\n",
      "Iteration 10, loss = 0.12573544\n",
      "Iteration 11, loss = 0.11462134\n",
      "Iteration 12, loss = 0.10511500\n",
      "Iteration 13, loss = 0.09550367\n",
      "Iteration 14, loss = 0.08827227\n",
      "Iteration 15, loss = 0.08111413\n",
      "Iteration 16, loss = 0.07557755\n",
      "Iteration 17, loss = 0.07061241\n",
      "Iteration 18, loss = 0.06630399\n",
      "Iteration 19, loss = 0.06325588\n",
      "Iteration 20, loss = 0.05889145\n",
      "Iteration 21, loss = 0.05600828\n",
      "Iteration 22, loss = 0.05311228\n",
      "Iteration 23, loss = 0.05037045\n",
      "Iteration 24, loss = 0.04877396\n",
      "Iteration 25, loss = 0.04594823\n",
      "Iteration 26, loss = 0.04401149\n",
      "Iteration 27, loss = 0.04240181\n",
      "Iteration 28, loss = 0.04062057\n",
      "Iteration 29, loss = 0.03933759\n",
      "Iteration 30, loss = 0.03796509\n",
      "Iteration 31, loss = 0.03632067\n",
      "Iteration 32, loss = 0.03583427\n",
      "Iteration 33, loss = 0.03438873\n",
      "Iteration 34, loss = 0.03369679\n",
      "Iteration 35, loss = 0.03253175\n",
      "Iteration 36, loss = 0.03169115\n",
      "Iteration 37, loss = 0.03145527\n",
      "Iteration 38, loss = 0.03045871\n",
      "Iteration 39, loss = 0.02966230\n",
      "Iteration 40, loss = 0.02916590\n",
      "Iteration 41, loss = 0.02865195\n",
      "Iteration 42, loss = 0.02788878\n",
      "Iteration 43, loss = 0.02753286\n",
      "Iteration 44, loss = 0.02688258\n",
      "Iteration 45, loss = 0.02679442\n",
      "Iteration 46, loss = 0.02627510\n",
      "Iteration 47, loss = 0.02576807\n",
      "Iteration 48, loss = 0.02531065\n",
      "Iteration 49, loss = 0.02497885\n",
      "Iteration 50, loss = 0.02451083\n",
      "Iteration 51, loss = 0.02430999\n",
      "Iteration 52, loss = 0.02420325\n",
      "Iteration 53, loss = 0.02369290\n",
      "Iteration 54, loss = 0.02346037\n",
      "Iteration 55, loss = 0.02327858\n",
      "Iteration 56, loss = 0.02313564\n",
      "Iteration 57, loss = 0.02261938\n",
      "Iteration 58, loss = 0.02243515\n",
      "Iteration 59, loss = 0.02203557\n",
      "Iteration 60, loss = 0.02220041\n",
      "Iteration 61, loss = 0.02190465\n",
      "Iteration 62, loss = 0.02143532\n",
      "Iteration 63, loss = 0.02156063\n",
      "Iteration 64, loss = 0.02186024\n",
      "Iteration 65, loss = 0.02100200\n",
      "Iteration 66, loss = 0.02066728\n",
      "Iteration 67, loss = 0.02066031\n",
      "Iteration 68, loss = 0.02041851\n",
      "Iteration 69, loss = 0.01996640\n",
      "Iteration 70, loss = 0.02011332\n",
      "Iteration 71, loss = 0.01978578\n",
      "Iteration 72, loss = 0.01943283\n",
      "Iteration 73, loss = 0.01922834\n",
      "Iteration 74, loss = 0.01918697\n",
      "Iteration 75, loss = 0.01926940\n",
      "Iteration 76, loss = 0.01929062\n",
      "Iteration 77, loss = 0.01884556\n",
      "Iteration 78, loss = 0.01858790\n",
      "Iteration 79, loss = 0.01863388\n",
      "Iteration 80, loss = 0.01836955\n",
      "Iteration 81, loss = 0.01814990\n",
      "Iteration 82, loss = 0.01816821\n",
      "Iteration 83, loss = 0.01794568\n",
      "Iteration 84, loss = 0.01826330\n",
      "Iteration 85, loss = 0.01791804\n",
      "Iteration 86, loss = 0.01775262\n",
      "Iteration 87, loss = 0.01791212\n",
      "Iteration 88, loss = 0.01762798\n",
      "Iteration 89, loss = 0.01740506\n",
      "Iteration 90, loss = 0.01727339\n",
      "Iteration 91, loss = 0.01755259\n",
      "Iteration 92, loss = 0.01860887\n",
      "Iteration 93, loss = 0.01769753\n",
      "Iteration 94, loss = 0.01777680\n",
      "Iteration 95, loss = 0.01735030\n",
      "Iteration 96, loss = 0.01730407\n",
      "Iteration 97, loss = 0.01659497\n",
      "Iteration 98, loss = 0.01655680\n",
      "Iteration 99, loss = 0.01705038\n",
      "Iteration 100, loss = 0.01632131\n",
      "Iteration 101, loss = 0.01627353\n",
      "Iteration 102, loss = 0.01625071\n",
      "Iteration 103, loss = 0.01599952\n",
      "Iteration 104, loss = 0.01608630\n",
      "Iteration 105, loss = 0.01590322\n",
      "Iteration 106, loss = 0.01590418\n",
      "Iteration 107, loss = 0.01614259\n",
      "Iteration 108, loss = 0.01590242\n",
      "Iteration 109, loss = 0.01581235\n",
      "Iteration 110, loss = 0.01547168\n",
      "Iteration 111, loss = 0.01561461\n",
      "Iteration 112, loss = 0.01531713\n",
      "Iteration 113, loss = 0.01538386\n",
      "Iteration 114, loss = 0.01523050\n",
      "Iteration 115, loss = 0.01515671\n",
      "Iteration 116, loss = 0.01508110\n",
      "Iteration 117, loss = 0.01501435\n",
      "Iteration 118, loss = 0.01503033\n",
      "Iteration 119, loss = 0.01492111\n",
      "Iteration 120, loss = 0.01506867\n",
      "Iteration 121, loss = 0.01510741\n",
      "Iteration 122, loss = 0.01518939\n",
      "Iteration 123, loss = 0.01513330\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.44304542\n",
      "Iteration 2, loss = 0.33482287\n",
      "Iteration 3, loss = 0.27658335\n",
      "Iteration 4, loss = 0.23432735\n",
      "Iteration 5, loss = 0.20469056\n",
      "Iteration 6, loss = 0.18218226\n",
      "Iteration 7, loss = 0.16514129\n",
      "Iteration 8, loss = 0.15162352\n",
      "Iteration 9, loss = 0.13772224\n",
      "Iteration 10, loss = 0.12573544\n",
      "Iteration 11, loss = 0.11462134\n",
      "Iteration 12, loss = 0.10511500\n",
      "Iteration 13, loss = 0.09550367\n",
      "Iteration 14, loss = 0.08827227\n",
      "Iteration 15, loss = 0.08111413\n",
      "Iteration 16, loss = 0.07557755\n",
      "Iteration 17, loss = 0.07061241\n",
      "Iteration 18, loss = 0.06630399\n",
      "Iteration 19, loss = 0.06325588\n",
      "Iteration 20, loss = 0.05889145\n",
      "Iteration 21, loss = 0.05600828\n",
      "Iteration 22, loss = 0.05311228\n",
      "Iteration 23, loss = 0.05037045\n",
      "Iteration 24, loss = 0.04877396\n",
      "Iteration 25, loss = 0.04594823\n",
      "Iteration 26, loss = 0.04401149\n",
      "Iteration 27, loss = 0.04240181\n",
      "Iteration 28, loss = 0.04062057\n",
      "Iteration 29, loss = 0.03933759\n",
      "Iteration 30, loss = 0.03796509\n",
      "Iteration 31, loss = 0.03632067\n",
      "Iteration 32, loss = 0.03583427\n",
      "Iteration 33, loss = 0.03438873\n",
      "Iteration 34, loss = 0.03369679\n",
      "Iteration 35, loss = 0.03253175\n",
      "Iteration 36, loss = 0.03169115\n",
      "Iteration 37, loss = 0.03145527\n",
      "Iteration 38, loss = 0.03045871\n",
      "Iteration 39, loss = 0.02966230\n",
      "Iteration 40, loss = 0.02916590\n",
      "Iteration 41, loss = 0.02865195\n",
      "Iteration 42, loss = 0.02788878\n",
      "Iteration 43, loss = 0.02753286\n",
      "Iteration 44, loss = 0.02688258\n",
      "LinearRegression\n",
      "NeuralNetwork\n",
      "Iteration 45, loss = 0.02679442\n",
      "Iteration 46, loss = 0.02627510\n",
      "Iteration 47, loss = 0.02576807\n",
      "Iteration 48, loss = 0.02531065\n",
      "Iteration 49, loss = 0.02497885\n",
      "Iteration 50, loss = 0.02451083\n",
      "Iteration 51, loss = 0.02430999\n",
      "Iteration 52, loss = 0.02420325\n",
      "Iteration 53, loss = 0.02369290\n",
      "Iteration 54, loss = 0.02346037\n",
      "Iteration 55, loss = 0.02327858\n",
      "Iteration 56, loss = 0.02313564\n",
      "Iteration 57, loss = 0.02261938\n",
      "Iteration 58, loss = 0.02243515\n",
      "Iteration 59, loss = 0.02203557\n",
      "Iteration 60, loss = 0.02220041\n",
      "Iteration 61, loss = 0.02190465\n",
      "Iteration 62, loss = 0.02143532\n",
      "Iteration 63, loss = 0.02156063\n",
      "Iteration 64, loss = 0.02186024\n",
      "Iteration 65, loss = 0.02100200\n",
      "Iteration 66, loss = 0.02066728\n",
      "Iteration 67, loss = 0.02066031\n",
      "Iteration 68, loss = 0.02041851\n",
      "Iteration 69, loss = 0.01996640\n",
      "Iteration 70, loss = 0.02011332\n",
      "Iteration 71, loss = 0.01978578\n",
      "Iteration 72, loss = 0.01943283\n",
      "Iteration 73, loss = 0.01922834\n",
      "Iteration 74, loss = 0.01918697\n",
      "Iteration 75, loss = 0.01926940\n",
      "Iteration 76, loss = 0.01929062\n",
      "Iteration 77, loss = 0.01884556\n",
      "Iteration 78, loss = 0.01858790\n",
      "Iteration 79, loss = 0.01863388\n",
      "Iteration 80, loss = 0.01836955\n",
      "Iteration 81, loss = 0.01814990\n",
      "Iteration 82, loss = 0.01816821\n",
      "Iteration 83, loss = 0.01794568\n",
      "Iteration 84, loss = 0.01826330\n",
      "Iteration 85, loss = 0.01791804\n",
      "Iteration 86, loss = 0.01775262\n",
      "Iteration 87, loss = 0.01791212\n",
      "Iteration 88, loss = 0.01762798\n",
      "Iteration 89, loss = 0.01740506\n",
      "Iteration 90, loss = 0.01727339\n",
      "Iteration 91, loss = 0.01755259\n",
      "Iteration 92, loss = 0.01860887\n",
      "Iteration 93, loss = 0.01769753\n",
      "Iteration 94, loss = 0.01777680\n",
      "Iteration 95, loss = 0.01735030\n",
      "Iteration 96, loss = 0.01730407\n",
      "Iteration 97, loss = 0.01659497\n",
      "Iteration 98, loss = 0.01655680\n",
      "Iteration 99, loss = 0.01705038\n",
      "Iteration 100, loss = 0.01632131\n",
      "Iteration 101, loss = 0.01627353\n",
      "Iteration 102, loss = 0.01625071\n",
      "Iteration 103, loss = 0.01599952\n",
      "Iteration 104, loss = 0.01608630\n",
      "Iteration 105, loss = 0.01590322\n",
      "Iteration 106, loss = 0.01590418\n",
      "Iteration 107, loss = 0.01614259\n",
      "Iteration 108, loss = 0.01590242\n",
      "Iteration 109, loss = 0.01581235\n",
      "Iteration 110, loss = 0.01547168\n",
      "Iteration 111, loss = 0.01561461\n",
      "Iteration 112, loss = 0.01531713\n",
      "Iteration 113, loss = 0.01538386\n",
      "Iteration 114, loss = 0.01523050\n",
      "Iteration 115, loss = 0.01515671\n",
      "Iteration 116, loss = 0.01508110\n",
      "Iteration 117, loss = 0.01501435\n",
      "Iteration 118, loss = 0.01503033\n",
      "Iteration 119, loss = 0.01492111\n",
      "Iteration 120, loss = 0.01506867\n",
      "Iteration 121, loss = 0.01510741\n",
      "Iteration 122, loss = 0.01518939\n",
      "Iteration 123, loss = 0.01513330\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46555644\n",
      "Iteration 2, loss = 0.34596326\n",
      "Iteration 3, loss = 0.27683071\n",
      "Iteration 4, loss = 0.23027065\n",
      "Iteration 5, loss = 0.19680122\n",
      "Iteration 6, loss = 0.17489640\n",
      "Iteration 7, loss = 0.15821526\n",
      "Iteration 8, loss = 0.14290289\n",
      "Iteration 9, loss = 0.12733278\n",
      "Iteration 10, loss = 0.11432204\n",
      "Iteration 11, loss = 0.10342409\n",
      "Iteration 12, loss = 0.09368185\n",
      "Iteration 13, loss = 0.08521764\n",
      "Iteration 14, loss = 0.07806561\n",
      "Iteration 15, loss = 0.07291479\n",
      "Iteration 16, loss = 0.06828336\n",
      "Iteration 17, loss = 0.06316055\n",
      "Iteration 18, loss = 0.05993433\n",
      "Iteration 19, loss = 0.05586641\n",
      "Iteration 20, loss = 0.05365164\n",
      "Iteration 21, loss = 0.05039588\n",
      "Iteration 22, loss = 0.04847778\n",
      "Iteration 23, loss = 0.04599612\n",
      "Iteration 24, loss = 0.04416475\n",
      "Iteration 25, loss = 0.04297175\n",
      "Iteration 26, loss = 0.04145520\n",
      "Iteration 27, loss = 0.04056230\n",
      "Iteration 28, loss = 0.03974190\n",
      "Iteration 29, loss = 0.03785769\n",
      "Iteration 30, loss = 0.03638753\n",
      "Iteration 31, loss = 0.03550526\n",
      "Iteration 32, loss = 0.03440998\n",
      "Iteration 33, loss = 0.03374217\n",
      "Iteration 34, loss = 0.03273505\n",
      "Iteration 35, loss = 0.03251677\n",
      "Iteration 36, loss = 0.03153075\n",
      "Iteration 37, loss = 0.03065594\n",
      "Iteration 38, loss = 0.03049264\n",
      "Iteration 39, loss = 0.02994004\n",
      "Iteration 40, loss = 0.02897649\n",
      "Iteration 41, loss = 0.02810980\n",
      "Iteration 42, loss = 0.02753855\n",
      "Iteration 43, loss = 0.02727925\n",
      "Iteration 44, loss = 0.02738169\n",
      "Iteration 45, loss = 0.02917210\n",
      "Iteration 46, loss = 0.02607176\n",
      "Iteration 47, loss = 0.02651226\n",
      "Iteration 48, loss = 0.02577858\n",
      "Iteration 49, loss = 0.02558617\n",
      "Iteration 50, loss = 0.02505079\n",
      "Iteration 51, loss = 0.02400545\n",
      "Iteration 52, loss = 0.02394066\n",
      "Iteration 53, loss = 0.02358606\n",
      "Iteration 54, loss = 0.02319045\n",
      "Iteration 55, loss = 0.02410940\n",
      "Iteration 56, loss = 0.02357133\n",
      "Iteration 57, loss = 0.02260892\n",
      "Iteration 58, loss = 0.02198626\n",
      "Iteration 59, loss = 0.02265544\n",
      "Iteration 60, loss = 0.02669928\n",
      "Iteration 61, loss = 0.02499326\n",
      "Iteration 62, loss = 0.02241134\n",
      "Iteration 63, loss = 0.02247917\n",
      "Iteration 64, loss = 0.02215603\n",
      "Iteration 65, loss = 0.02118275\n",
      "Iteration 66, loss = 0.02226842\n",
      "Iteration 67, loss = 0.02153358\n",
      "Iteration 68, loss = 0.02028269\n",
      "Iteration 69, loss = 0.02099691\n",
      "Iteration 70, loss = 0.02066267\n",
      "Iteration 71, loss = 0.02008186\n",
      "Iteration 72, loss = 0.01972269\n",
      "Iteration 73, loss = 0.01944973\n",
      "Iteration 74, loss = 0.01910696\n",
      "Iteration 75, loss = 0.01967300\n",
      "Iteration 76, loss = 0.01944618\n",
      "Iteration 77, loss = 0.01896019\n",
      "Iteration 78, loss = 0.01873404\n",
      "Iteration 79, loss = 0.01828013\n",
      "Iteration 80, loss = 0.01832217\n",
      "Iteration 81, loss = 0.01815674\n",
      "Iteration 82, loss = 0.01796923\n",
      "Iteration 83, loss = 0.01805728\n",
      "Iteration 84, loss = 0.01819981\n",
      "Iteration 85, loss = 0.01858843\n",
      "Iteration 86, loss = 0.01792308\n",
      "Iteration 87, loss = 0.01772022\n",
      "Iteration 88, loss = 0.01745860\n",
      "Iteration 89, loss = 0.01708105\n",
      "Iteration 90, loss = 0.01864194\n",
      "Iteration 91, loss = 0.01924217\n",
      "Iteration 92, loss = 0.01849439\n",
      "Iteration 93, loss = 0.01903466\n",
      "Iteration 94, loss = 0.01866959\n",
      "Iteration 95, loss = 0.01882751\n",
      "Iteration 96, loss = 0.01720469\n",
      "Iteration 97, loss = 0.01707662\n",
      "Iteration 98, loss = 0.01678018\n",
      "Iteration 99, loss = 0.01645537\n",
      "Iteration 100, loss = 0.01620754\n",
      "Iteration 101, loss = 0.01598659\n",
      "Iteration 102, loss = 0.01706080\n",
      "Iteration 103, loss = 0.01617576\n",
      "Iteration 104, loss = 0.01619200\n",
      "Iteration 105, loss = 0.01559050\n",
      "Iteration 106, loss = 0.01590452\n",
      "Iteration 107, loss = 0.01557313\n",
      "Iteration 108, loss = 0.01521528\n",
      "Iteration 109, loss = 0.01529040\n",
      "Iteration 110, loss = 0.01530978\n",
      "Iteration 111, loss = 0.01505544\n",
      "Iteration 112, loss = 0.01509793\n",
      "Iteration 113, loss = 0.01482169\n",
      "Iteration 114, loss = 0.01488586\n",
      "Iteration 115, loss = 0.01500923\n",
      "Iteration 116, loss = 0.01503207\n",
      "Iteration 117, loss = 0.01482877\n",
      "Iteration 118, loss = 0.01436923\n",
      "Iteration 119, loss = 0.01437711\n",
      "Iteration 120, loss = 0.01446715\n",
      "Iteration 121, loss = 0.01420276\n",
      "Iteration 122, loss = 0.01493174\n",
      "Iteration 123, loss = 0.01495738\n",
      "Iteration 124, loss = 0.01459163\n",
      "Iteration 125, loss = 0.01476774\n",
      "Iteration 126, loss = 0.01389513\n",
      "Iteration 127, loss = 0.01504787\n",
      "Iteration 128, loss = 0.01497330\n",
      "Iteration 129, loss = 0.01437266\n",
      "Iteration 130, loss = 0.01419967\n",
      "Iteration 131, loss = 0.01420273\n",
      "Iteration 132, loss = 0.01383213\n",
      "Iteration 133, loss = 0.01414007\n",
      "Iteration 134, loss = 0.01433191\n",
      "Iteration 135, loss = 0.01430347\n",
      "Iteration 136, loss = 0.01354634\n",
      "Iteration 137, loss = 0.01345941\n",
      "Iteration 138, loss = 0.01345926\n",
      "Iteration 139, loss = 0.01341132\n",
      "Iteration 140, loss = 0.01324807\n",
      "Iteration 141, loss = 0.01306616\n",
      "Iteration 142, loss = 0.01321416\n",
      "Iteration 143, loss = 0.01338651\n",
      "Iteration 144, loss = 0.01357547\n",
      "Iteration 145, loss = 0.01328919\n",
      "Iteration 146, loss = 0.01314004\n",
      "Iteration 147, loss = 0.01291842\n",
      "Iteration 148, loss = 0.01287317\n",
      "Iteration 149, loss = 0.01276528\n",
      "Iteration 150, loss = 0.01279027\n",
      "Iteration 151, loss = 0.01264181\n",
      "Iteration 152, loss = 0.01245057\n",
      "Iteration 153, loss = 0.01285756\n",
      "Iteration 154, loss = 0.01271480\n",
      "Iteration 155, loss = 0.01247778\n",
      "Iteration 156, loss = 0.01280726\n",
      "Iteration 157, loss = 0.01474907\n",
      "Iteration 158, loss = 0.01383104\n",
      "Iteration 159, loss = 0.01287368\n",
      "Iteration 160, loss = 0.01260019\n",
      "Iteration 161, loss = 0.01298705\n",
      "Iteration 162, loss = 0.01312049\n",
      "Iteration 163, loss = 0.01278154\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46555644\n",
      "Iteration 2, loss = 0.34596326\n",
      "Iteration 3, loss = 0.27683071\n",
      "Iteration 4, loss = 0.23027065\n",
      "Iteration 5, loss = 0.19680122\n",
      "Iteration 6, loss = 0.17489640\n",
      "Iteration 7, loss = 0.15821526\n",
      "Iteration 8, loss = 0.14290289\n",
      "Iteration 9, loss = 0.12733278\n",
      "Iteration 10, loss = 0.11432204\n",
      "Iteration 11, loss = 0.10342409\n",
      "Iteration 12, loss = 0.09368185\n",
      "Iteration 13, loss = 0.08521764\n",
      "Iteration 14, loss = 0.07806561\n",
      "Iteration 15, loss = 0.07291479\n",
      "Iteration 16, loss = 0.06828336\n",
      "Iteration 17, loss = 0.06316055\n",
      "Iteration 18, loss = 0.05993433\n",
      "Iteration 19, loss = 0.05586641\n",
      "Iteration 20, loss = 0.05365164\n",
      "Iteration 21, loss = 0.05039588\n",
      "Iteration 22, loss = 0.04847778\n",
      "Iteration 23, loss = 0.04599612\n",
      "Iteration 24, loss = 0.04416475\n",
      "Iteration 25, loss = 0.04297175\n",
      "Iteration 26, loss = 0.04145520\n",
      "Iteration 27, loss = 0.04056230\n",
      "Iteration 28, loss = 0.03974190\n",
      "Iteration 29, loss = 0.03785769\n",
      "Iteration 30, loss = 0.03638753\n",
      "Iteration 31, loss = 0.03550526\n",
      "Iteration 32, loss = 0.03440998\n",
      "Iteration 33, loss = 0.03374217\n",
      "Iteration 34, loss = 0.03273505\n",
      "Iteration 35, loss = 0.03251677\n",
      "Iteration 36, loss = 0.03153075\n",
      "Iteration 37, loss = 0.03065594\n",
      "Iteration 38, loss = 0.03049264\n",
      "Iteration 39, loss = 0.02994004\n",
      "Iteration 40, loss = 0.02897649\n",
      "Iteration 41, loss = 0.02810980\n",
      "Iteration 42, loss = 0.02753855\n",
      "Iteration 43, loss = 0.02727925\n",
      "Iteration 44, loss = 0.02738169\n",
      "Iteration 45, loss = 0.02917210\n",
      "Iteration 46, loss = 0.02607176\n",
      "Iteration 47, loss = 0.02651226\n",
      "Iteration 48, loss = 0.02577858\n",
      "Iteration 49, loss = 0.02558617\n",
      "Iteration 50, loss = 0.02505079\n",
      "Iteration 51, loss = 0.02400545\n",
      "Iteration 52, loss = 0.02394066\n",
      "Iteration 53, loss = 0.02358606\n",
      "Iteration 54, loss = 0.02319045\n",
      "Iteration 55, loss = 0.02410940\n",
      "Iteration 56, loss = 0.02357133\n",
      "Iteration 57, loss = 0.02260892\n",
      "Iteration 58, loss = 0.02198626\n",
      "Iteration 59, loss = 0.02265544\n",
      "Iteration 60, loss = 0.02669928\n",
      "Iteration 61, loss = 0.02499326\n",
      "Iteration 62, loss = 0.02241134\n",
      "Iteration 63, loss = 0.02247917\n",
      "Iteration 64, loss = 0.02215603\n",
      "Iteration 65, loss = 0.02118275\n",
      "Iteration 66, loss = 0.02226842\n",
      "Iteration 67, loss = 0.02153358\n",
      "Iteration 68, loss = 0.02028269\n",
      "Iteration 69, loss = 0.02099691\n",
      "Iteration 70, loss = 0.02066267\n",
      "Iteration 71, loss = 0.02008186\n",
      "Iteration 72, loss = 0.01972269\n",
      "Iteration 73, loss = 0.01944973\n",
      "Iteration 74, loss = 0.01910696\n",
      "Iteration 75, loss = 0.01967300\n",
      "Iteration 76, loss = 0.01944618\n",
      "Iteration 77, loss = 0.01896019\n",
      "Iteration 78, loss = 0.01873404\n",
      "Iteration 79, loss = 0.01828013\n",
      "Iteration 80, loss = 0.01832217\n",
      "Iteration 81, loss = 0.01815674\n",
      "Iteration 82, loss = 0.01796923\n",
      "Iteration 83, loss = 0.01805728\n",
      "Iteration 84, loss = 0.01819981\n",
      "Iteration 85, loss = 0.01858843\n",
      "Iteration 86, loss = 0.01792308\n",
      "Iteration 87, loss = 0.01772022\n",
      "Iteration 88, loss = 0.01745860\n",
      "Iteration 89, loss = 0.01708105\n",
      "Iteration 90, loss = 0.01864194\n",
      "Iteration 91, loss = 0.01924217\n",
      "Iteration 92, loss = 0.01849439\n",
      "Iteration 93, loss = 0.01903466\n",
      "Iteration 94, loss = 0.01866959\n",
      "Iteration 95, loss = 0.01882751\n",
      "Iteration 96, loss = 0.01720469\n",
      "Iteration 97, loss = 0.01707662\n",
      "Iteration 98, loss = 0.01678018\n",
      "Iteration 99, loss = 0.01645537\n",
      "Iteration 100, loss = 0.01620754\n",
      "Iteration 101, loss = 0.01598659\n",
      "Iteration 102, loss = 0.01706080\n",
      "Iteration 103, loss = 0.01617576\n",
      "Iteration 104, loss = 0.01619200\n",
      "Iteration 105, loss = 0.01559050\n",
      "Iteration 106, loss = 0.01590452\n",
      "Iteration 107, loss = 0.01557313\n",
      "Iteration 108, loss = 0.01521528\n",
      "Iteration 109, loss = 0.01529040\n",
      "Iteration 110, loss = 0.01530978\n",
      "Iteration 111, loss = 0.01505544\n",
      "Iteration 112, loss = 0.01509793\n",
      "Iteration 113, loss = 0.01482169\n",
      "Iteration 114, loss = 0.01488586\n",
      "Iteration 115, loss = 0.01500923\n",
      "Iteration 116, loss = 0.01503207\n",
      "Iteration 117, loss = 0.01482877\n",
      "Iteration 118, loss = 0.01436923\n",
      "Iteration 119, loss = 0.01437711\n",
      "Iteration 120, loss = 0.01446715\n",
      "Iteration 121, loss = 0.01420276\n",
      "Iteration 122, loss = 0.01493174\n",
      "Iteration 123, loss = 0.01495738\n",
      "Iteration 124, loss = 0.01459163\n",
      "Iteration 125, loss = 0.01476774\n",
      "Iteration 126, loss = 0.01389513\n",
      "Iteration 127, loss = 0.01504787\n",
      "Iteration 128, loss = 0.01497330\n",
      "Iteration 129, loss = 0.01437266\n",
      "Iteration 130, loss = 0.01419967\n",
      "Iteration 131, loss = 0.01420273\n",
      "Iteration 132, loss = 0.01383213\n",
      "Iteration 133, loss = 0.01414007\n",
      "Iteration 134, loss = 0.01433191\n",
      "Iteration 135, loss = 0.01430347\n",
      "Iteration 136, loss = 0.01354634\n",
      "Iteration 137, loss = 0.01345941\n",
      "Iteration 138, loss = 0.01345926\n",
      "Iteration 139, loss = 0.01341132\n",
      "Iteration 140, loss = 0.01324807\n",
      "Iteration 141, loss = 0.01306616\n",
      "Iteration 142, loss = 0.01321416\n",
      "Iteration 143, loss = 0.01338651\n",
      "Iteration 144, loss = 0.01357547\n",
      "Iteration 145, loss = 0.01328919\n",
      "Iteration 146, loss = 0.01314004\n",
      "Iteration 147, loss = 0.01291842\n",
      "Iteration 148, loss = 0.01287317\n",
      "Iteration 149, loss = 0.01276528\n",
      "Iteration 150, loss = 0.01279027\n",
      "Iteration 151, loss = 0.01264181\n",
      "Iteration 152, loss = 0.01245057\n",
      "Iteration 153, loss = 0.01285756\n",
      "Iteration 154, loss = 0.01271480\n",
      "Iteration 155, loss = 0.01247778\n",
      "Iteration 156, loss = 0.01280726\n",
      "Iteration 157, loss = 0.01474907\n",
      "Iteration 158, loss = 0.01383104\n",
      "Iteration 159, loss = 0.01287368\n",
      "Iteration 160, loss = 0.01260019\n",
      "Iteration 161, loss = 0.01298705\n",
      "Iteration 162, loss = 0.01312049\n",
      "Iteration 163, loss = 0.01278154\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46555644\n",
      "Iteration 2, loss = 0.34596326\n",
      "Iteration 3, loss = 0.27683071\n",
      "Iteration 4, loss = 0.23027065\n",
      "Iteration 5, loss = 0.19680122\n",
      "Iteration 6, loss = 0.17489640\n",
      "Iteration 7, loss = 0.15821526\n",
      "Iteration 8, loss = 0.14290289\n",
      "Iteration 9, loss = 0.12733278\n",
      "Iteration 10, loss = 0.11432204\n",
      "Iteration 11, loss = 0.10342409\n",
      "Iteration 12, loss = 0.09368185\n",
      "Iteration 13, loss = 0.08521764\n",
      "Iteration 14, loss = 0.07806561\n",
      "Iteration 15, loss = 0.07291479\n",
      "Iteration 16, loss = 0.06828336\n",
      "Iteration 17, loss = 0.06316055\n",
      "Iteration 18, loss = 0.05993433\n",
      "Iteration 19, loss = 0.05586641\n",
      "Iteration 20, loss = 0.05365164\n",
      "Iteration 21, loss = 0.05039588\n",
      "Iteration 22, loss = 0.04847778\n",
      "Iteration 23, loss = 0.04599612\n",
      "Iteration 24, loss = 0.04416475\n",
      "Iteration 25, loss = 0.04297175\n",
      "Iteration 26, loss = 0.04145520\n",
      "Iteration 27, loss = 0.04056230\n",
      "Iteration 28, loss = 0.03974190\n",
      "Iteration 29, loss = 0.03785769\n",
      "Iteration 30, loss = 0.03638753\n",
      "Iteration 31, loss = 0.03550526\n",
      "Iteration 32, loss = 0.03440998\n",
      "Iteration 33, loss = 0.03374217\n",
      "Iteration 34, loss = 0.03273505\n",
      "Iteration 35, loss = 0.03251677\n",
      "Iteration 36, loss = 0.03153075\n",
      "Iteration 37, loss = 0.03065594\n",
      "Iteration 38, loss = 0.03049264\n",
      "Iteration 39, loss = 0.02994004\n",
      "Iteration 40, loss = 0.02897649\n",
      "Iteration 41, loss = 0.02810980\n",
      "Iteration 42, loss = 0.02753855\n",
      "Iteration 43, loss = 0.02727925\n",
      "Iteration 44, loss = 0.02738169\n",
      "Iteration 45, loss = 0.02917210\n",
      "Iteration 46, loss = 0.02607176\n",
      "Iteration 47, loss = 0.02651226\n",
      "Iteration 48, loss = 0.02577858\n",
      "Iteration 49, loss = 0.02558617\n",
      "Iteration 50, loss = 0.02505079\n",
      "Iteration 51, loss = 0.02400545\n",
      "Iteration 52, loss = 0.02394066\n",
      "Iteration 53, loss = 0.02358606\n",
      "Iteration 54, loss = 0.02319045\n",
      "Iteration 55, loss = 0.02410940\n",
      "Iteration 56, loss = 0.02357133\n",
      "Iteration 57, loss = 0.02260892\n",
      "Iteration 58, loss = 0.02198626\n",
      "Iteration 59, loss = 0.02265544\n",
      "Iteration 60, loss = 0.02669928\n",
      "Iteration 61, loss = 0.02499326\n",
      "Iteration 62, loss = 0.02241134\n",
      "Iteration 63, loss = 0.02247917\n",
      "Iteration 64, loss = 0.02215603\n",
      "Iteration 65, loss = 0.02118275\n",
      "Iteration 66, loss = 0.02226842\n",
      "Iteration 67, loss = 0.02153358\n",
      "Iteration 68, loss = 0.02028269\n",
      "Iteration 69, loss = 0.02099691\n",
      "Iteration 70, loss = 0.02066267\n",
      "Iteration 71, loss = 0.02008186\n",
      "Iteration 72, loss = 0.01972269\n",
      "Iteration 73, loss = 0.01944973\n",
      "Iteration 74, loss = 0.01910696\n",
      "Iteration 75, loss = 0.01967300\n",
      "Iteration 76, loss = 0.01944618\n",
      "Iteration 77, loss = 0.01896019\n",
      "Iteration 78, loss = 0.01873404\n",
      "Iteration 79, loss = 0.01828013\n",
      "Iteration 80, loss = 0.01832217\n",
      "Iteration 81, loss = 0.01815674\n",
      "Iteration 82, loss = 0.01796923\n",
      "Iteration 83, loss = 0.01805728\n",
      "Iteration 84, loss = 0.01819981\n",
      "Iteration 85, loss = 0.01858843\n",
      "Iteration 86, loss = 0.01792308\n",
      "Iteration 87, loss = 0.01772022\n",
      "Iteration 88, loss = 0.01745860\n",
      "Iteration 89, loss = 0.01708105\n",
      "Iteration 90, loss = 0.01864194\n",
      "Iteration 91, loss = 0.01924217\n",
      "Iteration 92, loss = 0.01849439\n",
      "Iteration 93, loss = 0.01903466\n",
      "Iteration 94, loss = 0.01866959\n",
      "LinearRegression\n",
      "NeuralNetwork\n",
      "Iteration 95, loss = 0.01882751\n",
      "Iteration 96, loss = 0.01720469\n",
      "Iteration 97, loss = 0.01707662\n",
      "Iteration 98, loss = 0.01678018\n",
      "Iteration 99, loss = 0.01645537\n",
      "Iteration 100, loss = 0.01620754\n",
      "Iteration 101, loss = 0.01598659\n",
      "Iteration 102, loss = 0.01706080\n",
      "Iteration 103, loss = 0.01617576\n",
      "Iteration 104, loss = 0.01619200\n",
      "Iteration 105, loss = 0.01559050\n",
      "Iteration 106, loss = 0.01590452\n",
      "Iteration 107, loss = 0.01557313\n",
      "Iteration 108, loss = 0.01521528\n",
      "Iteration 109, loss = 0.01529040\n",
      "Iteration 110, loss = 0.01530978\n",
      "Iteration 111, loss = 0.01505544\n",
      "Iteration 112, loss = 0.01509793\n",
      "Iteration 113, loss = 0.01482169\n",
      "Iteration 114, loss = 0.01488586\n",
      "Iteration 115, loss = 0.01500923\n",
      "Iteration 116, loss = 0.01503207\n",
      "Iteration 117, loss = 0.01482877\n",
      "Iteration 118, loss = 0.01436923\n",
      "Iteration 119, loss = 0.01437711\n",
      "Iteration 120, loss = 0.01446715\n",
      "Iteration 121, loss = 0.01420276\n",
      "Iteration 122, loss = 0.01493174\n",
      "Iteration 123, loss = 0.01495738\n",
      "Iteration 124, loss = 0.01459163\n",
      "Iteration 125, loss = 0.01476774\n",
      "Iteration 126, loss = 0.01389513\n",
      "Iteration 127, loss = 0.01504787\n",
      "Iteration 128, loss = 0.01497330\n",
      "Iteration 129, loss = 0.01437266\n",
      "Iteration 130, loss = 0.01419967\n",
      "Iteration 131, loss = 0.01420273\n",
      "Iteration 132, loss = 0.01383213\n",
      "Iteration 133, loss = 0.01414007\n",
      "Iteration 134, loss = 0.01433191\n",
      "Iteration 135, loss = 0.01430347\n",
      "Iteration 136, loss = 0.01354634\n",
      "Iteration 137, loss = 0.01345941\n",
      "Iteration 138, loss = 0.01345926\n",
      "Iteration 139, loss = 0.01341132\n",
      "Iteration 140, loss = 0.01324807\n",
      "Iteration 141, loss = 0.01306616\n",
      "Iteration 142, loss = 0.01321416\n",
      "Iteration 143, loss = 0.01338651\n",
      "Iteration 144, loss = 0.01357547\n",
      "Iteration 145, loss = 0.01328919\n",
      "Iteration 146, loss = 0.01314004\n",
      "Iteration 147, loss = 0.01291842\n",
      "Iteration 148, loss = 0.01287317\n",
      "Iteration 149, loss = 0.01276528\n",
      "Iteration 150, loss = 0.01279027\n",
      "Iteration 151, loss = 0.01264181\n",
      "Iteration 152, loss = 0.01245057\n",
      "Iteration 153, loss = 0.01285756\n",
      "Iteration 154, loss = 0.01271480\n",
      "Iteration 155, loss = 0.01247778\n",
      "Iteration 156, loss = 0.01280726\n",
      "Iteration 157, loss = 0.01474907\n",
      "Iteration 158, loss = 0.01383104\n",
      "Iteration 159, loss = 0.01287368\n",
      "Iteration 160, loss = 0.01260019\n",
      "Iteration 161, loss = 0.01298705\n",
      "Iteration 162, loss = 0.01312049\n",
      "Iteration 163, loss = 0.01278154\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.44294993\n",
      "Iteration 2, loss = 0.33874360\n",
      "Iteration 3, loss = 0.27247303\n",
      "Iteration 4, loss = 0.22793668\n",
      "Iteration 5, loss = 0.19333358\n",
      "Iteration 6, loss = 0.16572257\n",
      "Iteration 7, loss = 0.14799567\n",
      "Iteration 8, loss = 0.12992023\n",
      "Iteration 9, loss = 0.11574184\n",
      "Iteration 10, loss = 0.10254800\n",
      "Iteration 11, loss = 0.09143769\n",
      "Iteration 12, loss = 0.08284855\n",
      "Iteration 13, loss = 0.07507898\n",
      "Iteration 14, loss = 0.06843325\n",
      "Iteration 15, loss = 0.06301120\n",
      "Iteration 16, loss = 0.05869513\n",
      "Iteration 17, loss = 0.05450642\n",
      "Iteration 18, loss = 0.05124439\n",
      "Iteration 19, loss = 0.04799427\n",
      "Iteration 20, loss = 0.04572789\n",
      "Iteration 21, loss = 0.04338392\n",
      "Iteration 22, loss = 0.04139385\n",
      "Iteration 23, loss = 0.03959333\n",
      "Iteration 24, loss = 0.03794305\n",
      "Iteration 25, loss = 0.03624867\n",
      "Iteration 26, loss = 0.03473720\n",
      "Iteration 27, loss = 0.03377601\n",
      "Iteration 28, loss = 0.03346951\n",
      "Iteration 29, loss = 0.03103423\n",
      "Iteration 30, loss = 0.03074614\n",
      "Iteration 31, loss = 0.02946744\n",
      "Iteration 32, loss = 0.02892302\n",
      "Iteration 33, loss = 0.02828237\n",
      "Iteration 34, loss = 0.02713434\n",
      "Iteration 35, loss = 0.02618838\n",
      "Iteration 36, loss = 0.02567016\n",
      "Iteration 37, loss = 0.02500615\n",
      "Iteration 38, loss = 0.02491759\n",
      "Iteration 39, loss = 0.02412078\n",
      "Iteration 40, loss = 0.02362279\n",
      "Iteration 41, loss = 0.02304294\n",
      "Iteration 42, loss = 0.02251944\n",
      "Iteration 43, loss = 0.02228321\n",
      "Iteration 44, loss = 0.02226104\n",
      "Iteration 45, loss = 0.02181508\n",
      "Iteration 46, loss = 0.02130556\n",
      "Iteration 47, loss = 0.02085015\n",
      "Iteration 48, loss = 0.02129991\n",
      "Iteration 49, loss = 0.02034117\n",
      "Iteration 50, loss = 0.02104685\n",
      "Iteration 51, loss = 0.02017852\n",
      "Iteration 52, loss = 0.01969565\n",
      "Iteration 53, loss = 0.01956866\n",
      "Iteration 54, loss = 0.01919095\n",
      "Iteration 55, loss = 0.01888559\n",
      "Iteration 56, loss = 0.01871371\n",
      "Iteration 57, loss = 0.01872747\n",
      "Iteration 58, loss = 0.01959834\n",
      "Iteration 59, loss = 0.01865843\n",
      "Iteration 60, loss = 0.01805885\n",
      "Iteration 61, loss = 0.01814830\n",
      "Iteration 62, loss = 0.01761911\n",
      "Iteration 63, loss = 0.01758344\n",
      "Iteration 64, loss = 0.01747468\n",
      "Iteration 65, loss = 0.01732590\n",
      "Iteration 66, loss = 0.01734127\n",
      "Iteration 67, loss = 0.01697799\n",
      "Iteration 68, loss = 0.01695388\n",
      "Iteration 69, loss = 0.01710019\n",
      "Iteration 70, loss = 0.01668986\n",
      "Iteration 71, loss = 0.01636209\n",
      "Iteration 72, loss = 0.01648614\n",
      "Iteration 73, loss = 0.01659632\n",
      "Iteration 74, loss = 0.01636643\n",
      "Iteration 75, loss = 0.01603781\n",
      "Iteration 76, loss = 0.01622466\n",
      "Iteration 77, loss = 0.01596480\n",
      "Iteration 78, loss = 0.01585608\n",
      "Iteration 79, loss = 0.01625026\n",
      "Iteration 80, loss = 0.01572947\n",
      "Iteration 81, loss = 0.01562140\n",
      "Iteration 82, loss = 0.01546843\n",
      "Iteration 83, loss = 0.01523239\n",
      "Iteration 84, loss = 0.01532326\n",
      "Iteration 85, loss = 0.01501396\n",
      "Iteration 86, loss = 0.01531481\n",
      "Iteration 87, loss = 0.01494852\n",
      "Iteration 88, loss = 0.01634016\n",
      "Iteration 89, loss = 0.01467214\n",
      "Iteration 90, loss = 0.01549365\n",
      "Iteration 91, loss = 0.01499407\n",
      "Iteration 92, loss = 0.01536076\n",
      "Iteration 93, loss = 0.01465109\n",
      "Iteration 94, loss = 0.01464551\n",
      "Iteration 95, loss = 0.01496918\n",
      "Iteration 96, loss = 0.01426707\n",
      "Iteration 97, loss = 0.01435240\n",
      "Iteration 98, loss = 0.01409890\n",
      "Iteration 99, loss = 0.01404832\n",
      "Iteration 100, loss = 0.01409924\n",
      "Iteration 101, loss = 0.01437361\n",
      "Iteration 102, loss = 0.01435513\n",
      "Iteration 103, loss = 0.01391296\n",
      "Iteration 104, loss = 0.01406994\n",
      "Iteration 105, loss = 0.01372271\n",
      "Iteration 106, loss = 0.01380733\n",
      "Iteration 107, loss = 0.01374652\n",
      "Iteration 108, loss = 0.01342941\n",
      "Iteration 109, loss = 0.01358207\n",
      "Iteration 110, loss = 0.01388760\n",
      "Iteration 111, loss = 0.01329687\n",
      "Iteration 112, loss = 0.01345571\n",
      "Iteration 113, loss = 0.01373825\n",
      "Iteration 114, loss = 0.01316658\n",
      "Iteration 115, loss = 0.01323617\n",
      "Iteration 116, loss = 0.01311024\n",
      "Iteration 117, loss = 0.01319712\n",
      "Iteration 118, loss = 0.01423298\n",
      "Iteration 119, loss = 0.01413777\n",
      "Iteration 120, loss = 0.01354265\n",
      "Iteration 121, loss = 0.01357511\n",
      "Iteration 122, loss = 0.01318399\n",
      "Iteration 123, loss = 0.01286114\n",
      "Iteration 124, loss = 0.01370790\n",
      "Iteration 125, loss = 0.01436188\n",
      "Iteration 126, loss = 0.01327999\n",
      "Iteration 127, loss = 0.01319008\n",
      "Iteration 128, loss = 0.01302286\n",
      "Iteration 129, loss = 0.01282277\n",
      "Iteration 130, loss = 0.01293478\n",
      "Iteration 131, loss = 0.01254758\n",
      "Iteration 132, loss = 0.01254189\n",
      "Iteration 133, loss = 0.01246204\n",
      "Iteration 134, loss = 0.01254583\n",
      "Iteration 135, loss = 0.01277672\n",
      "Iteration 136, loss = 0.01299206\n",
      "Iteration 137, loss = 0.01293613\n",
      "Iteration 138, loss = 0.01221744\n",
      "Iteration 139, loss = 0.01233707\n",
      "Iteration 140, loss = 0.01219587\n",
      "Iteration 141, loss = 0.01245814\n",
      "Iteration 142, loss = 0.01221987\n",
      "Iteration 143, loss = 0.01212342\n",
      "Iteration 144, loss = 0.01219123\n",
      "Iteration 145, loss = 0.01214321\n",
      "Iteration 146, loss = 0.01187251\n",
      "Iteration 147, loss = 0.01193522\n",
      "Iteration 148, loss = 0.01187852\n",
      "Iteration 149, loss = 0.01208690\n",
      "Iteration 150, loss = 0.01247943\n",
      "Iteration 151, loss = 0.01222743\n",
      "Iteration 152, loss = 0.01202768\n",
      "Iteration 153, loss = 0.01177598\n",
      "Iteration 154, loss = 0.01172071\n",
      "Iteration 155, loss = 0.01179897\n",
      "Iteration 156, loss = 0.01232066\n",
      "Iteration 157, loss = 0.01228632\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.44294993\n",
      "Iteration 2, loss = 0.33874360\n",
      "Iteration 3, loss = 0.27247303\n",
      "Iteration 4, loss = 0.22793668\n",
      "Iteration 5, loss = 0.19333358\n",
      "Iteration 6, loss = 0.16572257\n",
      "Iteration 7, loss = 0.14799567\n",
      "Iteration 8, loss = 0.12992023\n",
      "Iteration 9, loss = 0.11574184\n",
      "Iteration 10, loss = 0.10254800\n",
      "Iteration 11, loss = 0.09143769\n",
      "Iteration 12, loss = 0.08284855\n",
      "Iteration 13, loss = 0.07507898\n",
      "Iteration 14, loss = 0.06843325\n",
      "Iteration 15, loss = 0.06301120\n",
      "Iteration 16, loss = 0.05869513\n",
      "Iteration 17, loss = 0.05450642\n",
      "Iteration 18, loss = 0.05124439\n",
      "Iteration 19, loss = 0.04799427\n",
      "Iteration 20, loss = 0.04572789\n",
      "Iteration 21, loss = 0.04338392\n",
      "Iteration 22, loss = 0.04139385\n",
      "Iteration 23, loss = 0.03959333\n",
      "Iteration 24, loss = 0.03794305\n",
      "Iteration 25, loss = 0.03624867\n",
      "Iteration 26, loss = 0.03473720\n",
      "Iteration 27, loss = 0.03377601\n",
      "Iteration 28, loss = 0.03346951\n",
      "Iteration 29, loss = 0.03103423\n",
      "Iteration 30, loss = 0.03074614\n",
      "Iteration 31, loss = 0.02946744\n",
      "Iteration 32, loss = 0.02892302\n",
      "Iteration 33, loss = 0.02828237\n",
      "Iteration 34, loss = 0.02713434\n",
      "Iteration 35, loss = 0.02618838\n",
      "Iteration 36, loss = 0.02567016\n",
      "Iteration 37, loss = 0.02500615\n",
      "Iteration 38, loss = 0.02491759\n",
      "Iteration 39, loss = 0.02412078\n",
      "Iteration 40, loss = 0.02362279\n",
      "Iteration 41, loss = 0.02304294\n",
      "Iteration 42, loss = 0.02251944\n",
      "Iteration 43, loss = 0.02228321\n",
      "Iteration 44, loss = 0.02226104\n",
      "Iteration 45, loss = 0.02181508\n",
      "Iteration 46, loss = 0.02130556\n",
      "Iteration 47, loss = 0.02085015\n",
      "Iteration 48, loss = 0.02129991\n",
      "Iteration 49, loss = 0.02034117\n",
      "Iteration 50, loss = 0.02104685\n",
      "Iteration 51, loss = 0.02017852\n",
      "Iteration 52, loss = 0.01969565\n",
      "Iteration 53, loss = 0.01956866\n",
      "Iteration 54, loss = 0.01919095\n",
      "Iteration 55, loss = 0.01888559\n",
      "Iteration 56, loss = 0.01871371\n",
      "Iteration 57, loss = 0.01872747\n",
      "Iteration 58, loss = 0.01959834\n",
      "Iteration 59, loss = 0.01865843\n",
      "Iteration 60, loss = 0.01805885\n",
      "Iteration 61, loss = 0.01814830\n",
      "Iteration 62, loss = 0.01761911\n",
      "Iteration 63, loss = 0.01758344\n",
      "Iteration 64, loss = 0.01747468\n",
      "Iteration 65, loss = 0.01732590\n",
      "Iteration 66, loss = 0.01734127\n",
      "Iteration 67, loss = 0.01697799\n",
      "Iteration 68, loss = 0.01695388\n",
      "Iteration 69, loss = 0.01710019\n",
      "Iteration 70, loss = 0.01668986\n",
      "Iteration 71, loss = 0.01636209\n",
      "Iteration 72, loss = 0.01648614\n",
      "Iteration 73, loss = 0.01659632\n",
      "Iteration 74, loss = 0.01636643\n",
      "Iteration 75, loss = 0.01603781\n",
      "Iteration 76, loss = 0.01622466\n",
      "Iteration 77, loss = 0.01596480\n",
      "Iteration 78, loss = 0.01585608\n",
      "Iteration 79, loss = 0.01625026\n",
      "Iteration 80, loss = 0.01572947\n",
      "Iteration 81, loss = 0.01562140\n",
      "Iteration 82, loss = 0.01546843\n",
      "Iteration 83, loss = 0.01523239\n",
      "Iteration 84, loss = 0.01532326\n",
      "Iteration 85, loss = 0.01501396\n",
      "Iteration 86, loss = 0.01531481\n",
      "Iteration 87, loss = 0.01494852\n",
      "Iteration 88, loss = 0.01634016\n",
      "Iteration 89, loss = 0.01467214\n",
      "Iteration 90, loss = 0.01549365\n",
      "Iteration 91, loss = 0.01499407\n",
      "Iteration 92, loss = 0.01536076\n",
      "Iteration 93, loss = 0.01465109\n",
      "Iteration 94, loss = 0.01464551\n",
      "Iteration 95, loss = 0.01496918\n",
      "Iteration 96, loss = 0.01426707\n",
      "Iteration 97, loss = 0.01435240\n",
      "Iteration 98, loss = 0.01409890\n",
      "Iteration 99, loss = 0.01404832\n",
      "Iteration 100, loss = 0.01409924\n",
      "Iteration 101, loss = 0.01437361\n",
      "Iteration 102, loss = 0.01435513\n",
      "Iteration 103, loss = 0.01391296\n",
      "Iteration 104, loss = 0.01406994\n",
      "Iteration 105, loss = 0.01372271\n",
      "Iteration 106, loss = 0.01380733\n",
      "Iteration 107, loss = 0.01374652\n",
      "Iteration 108, loss = 0.01342941\n",
      "Iteration 109, loss = 0.01358207\n",
      "Iteration 110, loss = 0.01388760\n",
      "Iteration 111, loss = 0.01329687\n",
      "Iteration 112, loss = 0.01345571\n",
      "Iteration 113, loss = 0.01373825\n",
      "Iteration 114, loss = 0.01316658\n",
      "Iteration 115, loss = 0.01323617\n",
      "Iteration 116, loss = 0.01311024\n",
      "Iteration 117, loss = 0.01319712\n",
      "Iteration 118, loss = 0.01423298\n",
      "Iteration 119, loss = 0.01413777\n",
      "Iteration 120, loss = 0.01354265\n",
      "Iteration 121, loss = 0.01357511\n",
      "Iteration 122, loss = 0.01318399\n",
      "Iteration 123, loss = 0.01286114\n",
      "Iteration 124, loss = 0.01370790\n",
      "Iteration 125, loss = 0.01436188\n",
      "Iteration 126, loss = 0.01327999\n",
      "Iteration 127, loss = 0.01319008\n",
      "Iteration 128, loss = 0.01302286\n",
      "Iteration 129, loss = 0.01282277\n",
      "Iteration 130, loss = 0.01293478\n",
      "Iteration 131, loss = 0.01254758\n",
      "Iteration 132, loss = 0.01254189\n",
      "Iteration 133, loss = 0.01246204\n",
      "Iteration 134, loss = 0.01254583\n",
      "Iteration 135, loss = 0.01277672\n",
      "Iteration 136, loss = 0.01299206\n",
      "Iteration 137, loss = 0.01293613\n",
      "Iteration 138, loss = 0.01221744\n",
      "Iteration 139, loss = 0.01233707\n",
      "Iteration 140, loss = 0.01219587\n",
      "Iteration 141, loss = 0.01245814\n",
      "Iteration 142, loss = 0.01221987\n",
      "Iteration 143, loss = 0.01212342\n",
      "Iteration 144, loss = 0.01219123\n",
      "Iteration 145, loss = 0.01214321\n",
      "Iteration 146, loss = 0.01187251\n",
      "Iteration 147, loss = 0.01193522\n",
      "Iteration 148, loss = 0.01187852\n",
      "Iteration 149, loss = 0.01208690\n",
      "Iteration 150, loss = 0.01247943\n",
      "Iteration 151, loss = 0.01222743\n",
      "Iteration 152, loss = 0.01202768\n",
      "Iteration 153, loss = 0.01177598\n",
      "Iteration 154, loss = 0.01172071\n",
      "Iteration 155, loss = 0.01179897\n",
      "Iteration 156, loss = 0.01232066\n",
      "Iteration 157, loss = 0.01228632\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.44294993\n",
      "Iteration 2, loss = 0.33874360\n",
      "Iteration 3, loss = 0.27247303\n",
      "Iteration 4, loss = 0.22793668\n",
      "Iteration 5, loss = 0.19333358\n",
      "Iteration 6, loss = 0.16572257\n",
      "Iteration 7, loss = 0.14799567\n",
      "Iteration 8, loss = 0.12992023\n",
      "Iteration 9, loss = 0.11574184\n",
      "Iteration 10, loss = 0.10254800\n",
      "Iteration 11, loss = 0.09143769\n",
      "Iteration 12, loss = 0.08284855\n",
      "Iteration 13, loss = 0.07507898\n",
      "Iteration 14, loss = 0.06843325\n",
      "Iteration 15, loss = 0.06301120\n",
      "Iteration 16, loss = 0.05869513\n",
      "Iteration 17, loss = 0.05450642\n",
      "Iteration 18, loss = 0.05124439\n",
      "Iteration 19, loss = 0.04799427\n",
      "Iteration 20, loss = 0.04572789\n",
      "Iteration 21, loss = 0.04338392\n",
      "Iteration 22, loss = 0.04139385\n",
      "Iteration 23, loss = 0.03959333\n",
      "Iteration 24, loss = 0.03794305\n",
      "Iteration 25, loss = 0.03624867\n",
      "Iteration 26, loss = 0.03473720\n",
      "Iteration 27, loss = 0.03377601\n",
      "Iteration 28, loss = 0.03346951\n",
      "Iteration 29, loss = 0.03103423\n",
      "Iteration 30, loss = 0.03074614\n",
      "Iteration 31, loss = 0.02946744\n",
      "Iteration 32, loss = 0.02892302\n",
      "Iteration 33, loss = 0.02828237\n",
      "Iteration 34, loss = 0.02713434\n",
      "Iteration 35, loss = 0.02618838\n",
      "Iteration 36, loss = 0.02567016\n",
      "Iteration 37, loss = 0.02500615\n",
      "Iteration 38, loss = 0.02491759\n",
      "Iteration 39, loss = 0.02412078\n",
      "Iteration 40, loss = 0.02362279\n",
      "Iteration 41, loss = 0.02304294\n",
      "Iteration 42, loss = 0.02251944\n",
      "Iteration 43, loss = 0.02228321\n",
      "Iteration 44, loss = 0.02226104\n",
      "Iteration 45, loss = 0.02181508\n",
      "Iteration 46, loss = 0.02130556\n",
      "Iteration 47, loss = 0.02085015\n",
      "Iteration 48, loss = 0.02129991\n",
      "Iteration 49, loss = 0.02034117\n",
      "Iteration 50, loss = 0.02104685\n",
      "Iteration 51, loss = 0.02017852\n",
      "Iteration 52, loss = 0.01969565\n",
      "Iteration 53, loss = 0.01956866\n",
      "Iteration 54, loss = 0.01919095\n",
      "Iteration 55, loss = 0.01888559\n",
      "Iteration 56, loss = 0.01871371\n",
      "Iteration 57, loss = 0.01872747\n",
      "Iteration 58, loss = 0.01959834\n",
      "Iteration 59, loss = 0.01865843\n",
      "Iteration 60, loss = 0.01805885\n",
      "Iteration 61, loss = 0.01814830\n",
      "Iteration 62, loss = 0.01761911\n",
      "Iteration 63, loss = 0.01758344\n",
      "Iteration 64, loss = 0.01747468\n",
      "Iteration 65, loss = 0.01732590\n",
      "Iteration 66, loss = 0.01734127\n",
      "Iteration 67, loss = 0.01697799\n",
      "Iteration 68, loss = 0.01695388\n",
      "Iteration 69, loss = 0.01710019\n",
      "Iteration 70, loss = 0.01668986\n",
      "Iteration 71, loss = 0.01636209\n",
      "Iteration 72, loss = 0.01648614\n",
      "Iteration 73, loss = 0.01659632\n",
      "Iteration 74, loss = 0.01636643\n",
      "Iteration 75, loss = 0.01603781\n",
      "Iteration 76, loss = 0.01622466\n",
      "Iteration 77, loss = 0.01596480\n",
      "Iteration 78, loss = 0.01585608\n",
      "Iteration 79, loss = 0.01625026\n",
      "Iteration 80, loss = 0.01572947\n",
      "Iteration 81, loss = 0.01562140\n",
      "Iteration 82, loss = 0.01546843\n",
      "Iteration 83, loss = 0.01523239\n",
      "Iteration 84, loss = 0.01532326\n",
      "Iteration 85, loss = 0.01501396\n",
      "Iteration 86, loss = 0.01531481\n",
      "Iteration 87, loss = 0.01494852\n",
      "Iteration 88, loss = 0.01634016\n",
      "Iteration 89, loss = 0.01467214\n",
      "Iteration 90, loss = 0.01549365\n",
      "Iteration 91, loss = 0.01499407\n",
      "Iteration 92, loss = 0.01536076\n",
      "Iteration 93, loss = 0.01465109\n",
      "Iteration 94, loss = 0.01464551\n",
      "Iteration 95, loss = 0.01496918\n",
      "Iteration 96, loss = 0.01426707\n",
      "Iteration 97, loss = 0.01435240\n",
      "Iteration 98, loss = 0.01409890\n",
      "Iteration 99, loss = 0.01404832\n",
      "Iteration 100, loss = 0.01409924\n",
      "Iteration 101, loss = 0.01437361\n",
      "Iteration 102, loss = 0.01435513\n",
      "Iteration 103, loss = 0.01391296\n",
      "Iteration 104, loss = 0.01406994\n",
      "Iteration 105, loss = 0.01372271\n",
      "Iteration 106, loss = 0.01380733\n",
      "Iteration 107, loss = 0.01374652\n",
      "Iteration 108, loss = 0.01342941\n",
      "Iteration 109, loss = 0.01358207\n",
      "Iteration 110, loss = 0.01388760\n",
      "Iteration 111, loss = 0.01329687\n",
      "Iteration 112, loss = 0.01345571\n",
      "Iteration 113, loss = 0.01373825\n",
      "Iteration 114, loss = 0.01316658\n",
      "Iteration 115, loss = 0.01323617\n",
      "LinearRegression\n",
      "NeuralNetwork\n",
      "Iteration 116, loss = 0.01311024\n",
      "Iteration 117, loss = 0.01319712\n",
      "Iteration 118, loss = 0.01423298\n",
      "Iteration 119, loss = 0.01413777\n",
      "Iteration 120, loss = 0.01354265\n",
      "Iteration 121, loss = 0.01357511\n",
      "Iteration 122, loss = 0.01318399\n",
      "Iteration 123, loss = 0.01286114\n",
      "Iteration 124, loss = 0.01370790\n",
      "Iteration 125, loss = 0.01436188\n",
      "Iteration 126, loss = 0.01327999\n",
      "Iteration 127, loss = 0.01319008\n",
      "Iteration 128, loss = 0.01302286\n",
      "Iteration 129, loss = 0.01282277\n",
      "Iteration 130, loss = 0.01293478\n",
      "Iteration 131, loss = 0.01254758\n",
      "Iteration 132, loss = 0.01254189\n",
      "Iteration 133, loss = 0.01246204\n",
      "Iteration 134, loss = 0.01254583\n",
      "Iteration 135, loss = 0.01277672\n",
      "Iteration 136, loss = 0.01299206\n",
      "Iteration 137, loss = 0.01293613\n",
      "Iteration 138, loss = 0.01221744\n",
      "Iteration 139, loss = 0.01233707\n",
      "Iteration 140, loss = 0.01219587\n",
      "Iteration 141, loss = 0.01245814\n",
      "Iteration 142, loss = 0.01221987\n",
      "Iteration 143, loss = 0.01212342\n",
      "Iteration 144, loss = 0.01219123\n",
      "Iteration 145, loss = 0.01214321\n",
      "Iteration 146, loss = 0.01187251\n",
      "Iteration 147, loss = 0.01193522\n",
      "Iteration 148, loss = 0.01187852\n",
      "Iteration 149, loss = 0.01208690\n",
      "Iteration 150, loss = 0.01247943\n",
      "Iteration 151, loss = 0.01222743\n",
      "Iteration 152, loss = 0.01202768\n",
      "Iteration 153, loss = 0.01177598\n",
      "Iteration 154, loss = 0.01172071\n",
      "Iteration 155, loss = 0.01179897\n",
      "Iteration 156, loss = 0.01232066\n",
      "Iteration 157, loss = 0.01228632\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45266589\n",
      "Iteration 2, loss = 0.34333713\n",
      "Iteration 3, loss = 0.28065062\n",
      "Iteration 4, loss = 0.23687385\n",
      "Iteration 5, loss = 0.20242563\n",
      "Iteration 6, loss = 0.17575603\n",
      "Iteration 7, loss = 0.15683473\n",
      "Iteration 8, loss = 0.13894261\n",
      "Iteration 9, loss = 0.12403416\n",
      "Iteration 10, loss = 0.11200804\n",
      "Iteration 11, loss = 0.10178319\n",
      "Iteration 12, loss = 0.09177020\n",
      "Iteration 13, loss = 0.08315142\n",
      "Iteration 14, loss = 0.07704703\n",
      "Iteration 15, loss = 0.07043609\n",
      "Iteration 16, loss = 0.06494194\n",
      "Iteration 17, loss = 0.06085682\n",
      "Iteration 18, loss = 0.05602822\n",
      "Iteration 19, loss = 0.05243338\n",
      "Iteration 20, loss = 0.04893377\n",
      "Iteration 21, loss = 0.04620010\n",
      "Iteration 22, loss = 0.04341694\n",
      "Iteration 23, loss = 0.04130589\n",
      "Iteration 24, loss = 0.03944269\n",
      "Iteration 25, loss = 0.03779537\n",
      "Iteration 26, loss = 0.03545159\n",
      "Iteration 27, loss = 0.03401499\n",
      "Iteration 28, loss = 0.03243555\n",
      "Iteration 29, loss = 0.03075905\n",
      "Iteration 30, loss = 0.02994654\n",
      "Iteration 31, loss = 0.02900919\n",
      "Iteration 32, loss = 0.02764090\n",
      "Iteration 33, loss = 0.02671134\n",
      "Iteration 34, loss = 0.02596460\n",
      "Iteration 35, loss = 0.02490519\n",
      "Iteration 36, loss = 0.02462935\n",
      "Iteration 37, loss = 0.02386605\n",
      "Iteration 38, loss = 0.02309817\n",
      "Iteration 39, loss = 0.02259673\n",
      "Iteration 40, loss = 0.02254200\n",
      "Iteration 41, loss = 0.02167549\n",
      "Iteration 42, loss = 0.02138187\n",
      "Iteration 43, loss = 0.02094277\n",
      "Iteration 44, loss = 0.02020027\n",
      "Iteration 45, loss = 0.02033096\n",
      "Iteration 46, loss = 0.01997445\n",
      "Iteration 47, loss = 0.01972127\n",
      "Iteration 48, loss = 0.01903215\n",
      "Iteration 49, loss = 0.01873717\n",
      "Iteration 50, loss = 0.01838449\n",
      "Iteration 51, loss = 0.01801062\n",
      "Iteration 52, loss = 0.01815662\n",
      "Iteration 53, loss = 0.01768883\n",
      "Iteration 54, loss = 0.01806722\n",
      "Iteration 55, loss = 0.01772725\n",
      "Iteration 56, loss = 0.01773269\n",
      "Iteration 57, loss = 0.01691766\n",
      "Iteration 58, loss = 0.01649569\n",
      "Iteration 59, loss = 0.01635651\n",
      "Iteration 60, loss = 0.01606227\n",
      "Iteration 61, loss = 0.01588946\n",
      "Iteration 62, loss = 0.01601838\n",
      "Iteration 63, loss = 0.01556158\n",
      "Iteration 64, loss = 0.01566812\n",
      "Iteration 65, loss = 0.01551414\n",
      "Iteration 66, loss = 0.01527554\n",
      "Iteration 67, loss = 0.01548387\n",
      "Iteration 68, loss = 0.01500959\n",
      "Iteration 69, loss = 0.01485864\n",
      "Iteration 70, loss = 0.01522738\n",
      "Iteration 71, loss = 0.01484380\n",
      "Iteration 72, loss = 0.01436552\n",
      "Iteration 73, loss = 0.01436256\n",
      "Iteration 74, loss = 0.01422265\n",
      "Iteration 75, loss = 0.01450008\n",
      "Iteration 76, loss = 0.01413818\n",
      "Iteration 77, loss = 0.01413410\n",
      "Iteration 78, loss = 0.01436797\n",
      "Iteration 79, loss = 0.01368868\n",
      "Iteration 80, loss = 0.01364806\n",
      "Iteration 81, loss = 0.01353474\n",
      "Iteration 82, loss = 0.01347782\n",
      "Iteration 83, loss = 0.01401897\n",
      "Iteration 84, loss = 0.01338842\n",
      "Iteration 85, loss = 0.01325766\n",
      "Iteration 86, loss = 0.01318873\n",
      "Iteration 87, loss = 0.01341255\n",
      "Iteration 88, loss = 0.01306659\n",
      "Iteration 89, loss = 0.01274513\n",
      "Iteration 90, loss = 0.01274396\n",
      "Iteration 91, loss = 0.01259795\n",
      "Iteration 92, loss = 0.01239593\n",
      "Iteration 93, loss = 0.01259650\n",
      "Iteration 94, loss = 0.01229495\n",
      "Iteration 95, loss = 0.01232178\n",
      "Iteration 96, loss = 0.01216739\n",
      "Iteration 97, loss = 0.01199001\n",
      "Iteration 98, loss = 0.01218580\n",
      "Iteration 99, loss = 0.01221790\n",
      "Iteration 100, loss = 0.01188147\n",
      "Iteration 101, loss = 0.01164811\n",
      "Iteration 102, loss = 0.01198219\n",
      "Iteration 103, loss = 0.01184303\n",
      "Iteration 104, loss = 0.01168711\n",
      "Iteration 105, loss = 0.01151625\n",
      "Iteration 106, loss = 0.01141009\n",
      "Iteration 107, loss = 0.01147531\n",
      "Iteration 108, loss = 0.01122730\n",
      "Iteration 109, loss = 0.01121738\n",
      "Iteration 110, loss = 0.01112780\n",
      "Iteration 111, loss = 0.01099722\n",
      "Iteration 112, loss = 0.01094382\n",
      "Iteration 113, loss = 0.01090932\n",
      "Iteration 114, loss = 0.01094642\n",
      "Iteration 115, loss = 0.01088869\n",
      "Iteration 116, loss = 0.01064943\n",
      "Iteration 117, loss = 0.01085081\n",
      "Iteration 118, loss = 0.01079128\n",
      "Iteration 119, loss = 0.01056221\n",
      "Iteration 120, loss = 0.01125067\n",
      "Iteration 121, loss = 0.01062788\n",
      "Iteration 122, loss = 0.01076942\n",
      "Iteration 123, loss = 0.01058567\n",
      "Iteration 124, loss = 0.01025836\n",
      "Iteration 125, loss = 0.01075445\n",
      "Iteration 126, loss = 0.01038572\n",
      "Iteration 127, loss = 0.01023559\n",
      "Iteration 128, loss = 0.01027304\n",
      "Iteration 129, loss = 0.01004141\n",
      "Iteration 130, loss = 0.01049790\n",
      "Iteration 131, loss = 0.01078116\n",
      "Iteration 132, loss = 0.00990103\n",
      "Iteration 133, loss = 0.00986629\n",
      "Iteration 134, loss = 0.00994123\n",
      "Iteration 135, loss = 0.00982166\n",
      "Iteration 136, loss = 0.00984260\n",
      "Iteration 137, loss = 0.01012103\n",
      "Iteration 138, loss = 0.00982487\n",
      "Iteration 139, loss = 0.00971793\n",
      "Iteration 140, loss = 0.01031044\n",
      "Iteration 141, loss = 0.00963458\n",
      "Iteration 142, loss = 0.00955052\n",
      "Iteration 143, loss = 0.00989365\n",
      "Iteration 144, loss = 0.00963704\n",
      "Iteration 145, loss = 0.00958326\n",
      "Iteration 146, loss = 0.00932547\n",
      "Iteration 147, loss = 0.00963385\n",
      "Iteration 148, loss = 0.00938542\n",
      "Iteration 149, loss = 0.00934890\n",
      "Iteration 150, loss = 0.00955090\n",
      "Iteration 151, loss = 0.00922566\n",
      "Iteration 152, loss = 0.00963669\n",
      "Iteration 153, loss = 0.00896020\n",
      "Iteration 154, loss = 0.00935229\n",
      "Iteration 155, loss = 0.00920341\n",
      "Iteration 156, loss = 0.00923385\n",
      "Iteration 157, loss = 0.00905357\n",
      "Iteration 158, loss = 0.00904967\n",
      "Iteration 159, loss = 0.00903936\n",
      "Iteration 160, loss = 0.00895413\n",
      "Iteration 161, loss = 0.00903390\n",
      "Iteration 162, loss = 0.00904950\n",
      "Iteration 163, loss = 0.00927259\n",
      "Iteration 164, loss = 0.00881082\n",
      "Iteration 165, loss = 0.00902811\n",
      "Iteration 166, loss = 0.00889243\n",
      "Iteration 167, loss = 0.00855733\n",
      "Iteration 168, loss = 0.00875628\n",
      "Iteration 169, loss = 0.00878469\n",
      "Iteration 170, loss = 0.00846651\n",
      "Iteration 171, loss = 0.00849656\n",
      "Iteration 172, loss = 0.00897088\n",
      "Iteration 173, loss = 0.00849153\n",
      "Iteration 174, loss = 0.00854196\n",
      "Iteration 175, loss = 0.00858937\n",
      "Iteration 176, loss = 0.00839828\n",
      "Iteration 177, loss = 0.00832776\n",
      "Iteration 178, loss = 0.00835883\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45266589\n",
      "Iteration 2, loss = 0.34333713\n",
      "Iteration 3, loss = 0.28065062\n",
      "Iteration 4, loss = 0.23687385\n",
      "Iteration 5, loss = 0.20242563\n",
      "Iteration 6, loss = 0.17575603\n",
      "Iteration 7, loss = 0.15683473\n",
      "Iteration 8, loss = 0.13894261\n",
      "Iteration 9, loss = 0.12403416\n",
      "Iteration 10, loss = 0.11200804\n",
      "Iteration 11, loss = 0.10178319\n",
      "Iteration 12, loss = 0.09177020\n",
      "Iteration 13, loss = 0.08315142\n",
      "Iteration 14, loss = 0.07704703\n",
      "Iteration 15, loss = 0.07043609\n",
      "Iteration 16, loss = 0.06494194\n",
      "Iteration 17, loss = 0.06085682\n",
      "Iteration 18, loss = 0.05602822\n",
      "Iteration 19, loss = 0.05243338\n",
      "Iteration 20, loss = 0.04893377\n",
      "Iteration 21, loss = 0.04620010\n",
      "Iteration 22, loss = 0.04341694\n",
      "Iteration 23, loss = 0.04130589\n",
      "Iteration 24, loss = 0.03944269\n",
      "Iteration 25, loss = 0.03779537\n",
      "Iteration 26, loss = 0.03545159\n",
      "Iteration 27, loss = 0.03401499\n",
      "Iteration 28, loss = 0.03243555\n",
      "Iteration 29, loss = 0.03075905\n",
      "Iteration 30, loss = 0.02994654\n",
      "Iteration 31, loss = 0.02900919\n",
      "Iteration 32, loss = 0.02764090\n",
      "Iteration 33, loss = 0.02671134\n",
      "Iteration 34, loss = 0.02596460\n",
      "Iteration 35, loss = 0.02490519\n",
      "Iteration 36, loss = 0.02462935\n",
      "Iteration 37, loss = 0.02386605\n",
      "Iteration 38, loss = 0.02309817\n",
      "Iteration 39, loss = 0.02259673\n",
      "Iteration 40, loss = 0.02254200\n",
      "Iteration 41, loss = 0.02167549\n",
      "Iteration 42, loss = 0.02138187\n",
      "Iteration 43, loss = 0.02094277\n",
      "Iteration 44, loss = 0.02020027\n",
      "Iteration 45, loss = 0.02033096\n",
      "Iteration 46, loss = 0.01997445\n",
      "Iteration 47, loss = 0.01972127\n",
      "Iteration 48, loss = 0.01903215\n",
      "Iteration 49, loss = 0.01873717\n",
      "Iteration 50, loss = 0.01838449\n",
      "Iteration 51, loss = 0.01801062\n",
      "Iteration 52, loss = 0.01815662\n",
      "Iteration 53, loss = 0.01768883\n",
      "Iteration 54, loss = 0.01806722\n",
      "Iteration 55, loss = 0.01772725\n",
      "Iteration 56, loss = 0.01773269\n",
      "Iteration 57, loss = 0.01691766\n",
      "Iteration 58, loss = 0.01649569\n",
      "Iteration 59, loss = 0.01635651\n",
      "Iteration 60, loss = 0.01606227\n",
      "Iteration 61, loss = 0.01588946\n",
      "Iteration 62, loss = 0.01601838\n",
      "Iteration 63, loss = 0.01556158\n",
      "Iteration 64, loss = 0.01566812\n",
      "Iteration 65, loss = 0.01551414\n",
      "Iteration 66, loss = 0.01527554\n",
      "Iteration 67, loss = 0.01548387\n",
      "Iteration 68, loss = 0.01500959\n",
      "Iteration 69, loss = 0.01485864\n",
      "Iteration 70, loss = 0.01522738\n",
      "Iteration 71, loss = 0.01484380\n",
      "Iteration 72, loss = 0.01436552\n",
      "Iteration 73, loss = 0.01436256\n",
      "Iteration 74, loss = 0.01422265\n",
      "Iteration 75, loss = 0.01450008\n",
      "Iteration 76, loss = 0.01413818\n",
      "Iteration 77, loss = 0.01413410\n",
      "Iteration 78, loss = 0.01436797\n",
      "Iteration 79, loss = 0.01368868\n",
      "Iteration 80, loss = 0.01364806\n",
      "Iteration 81, loss = 0.01353474\n",
      "Iteration 82, loss = 0.01347782\n",
      "Iteration 83, loss = 0.01401897\n",
      "Iteration 84, loss = 0.01338842\n",
      "Iteration 85, loss = 0.01325766\n",
      "Iteration 86, loss = 0.01318873\n",
      "Iteration 87, loss = 0.01341255\n",
      "Iteration 88, loss = 0.01306659\n",
      "Iteration 89, loss = 0.01274513\n",
      "Iteration 90, loss = 0.01274396\n",
      "Iteration 91, loss = 0.01259795\n",
      "Iteration 92, loss = 0.01239593\n",
      "Iteration 93, loss = 0.01259650\n",
      "Iteration 94, loss = 0.01229495\n",
      "Iteration 95, loss = 0.01232178\n",
      "Iteration 96, loss = 0.01216739\n",
      "Iteration 97, loss = 0.01199001\n",
      "Iteration 98, loss = 0.01218580\n",
      "Iteration 99, loss = 0.01221790\n",
      "Iteration 100, loss = 0.01188147\n",
      "Iteration 101, loss = 0.01164811\n",
      "Iteration 102, loss = 0.01198219\n",
      "Iteration 103, loss = 0.01184303\n",
      "Iteration 104, loss = 0.01168711\n",
      "Iteration 105, loss = 0.01151625\n",
      "Iteration 106, loss = 0.01141009\n",
      "Iteration 107, loss = 0.01147531\n",
      "Iteration 108, loss = 0.01122730\n",
      "Iteration 109, loss = 0.01121738\n",
      "Iteration 110, loss = 0.01112780\n",
      "Iteration 111, loss = 0.01099722\n",
      "Iteration 112, loss = 0.01094382\n",
      "Iteration 113, loss = 0.01090932\n",
      "Iteration 114, loss = 0.01094642\n",
      "Iteration 115, loss = 0.01088869\n",
      "Iteration 116, loss = 0.01064943\n",
      "Iteration 117, loss = 0.01085081\n",
      "Iteration 118, loss = 0.01079128\n",
      "Iteration 119, loss = 0.01056221\n",
      "Iteration 120, loss = 0.01125067\n",
      "Iteration 121, loss = 0.01062788\n",
      "Iteration 122, loss = 0.01076942\n",
      "Iteration 123, loss = 0.01058567\n",
      "Iteration 124, loss = 0.01025836\n",
      "Iteration 125, loss = 0.01075445\n",
      "Iteration 126, loss = 0.01038572\n",
      "Iteration 127, loss = 0.01023559\n",
      "Iteration 128, loss = 0.01027304\n",
      "Iteration 129, loss = 0.01004141\n",
      "Iteration 130, loss = 0.01049790\n",
      "Iteration 131, loss = 0.01078116\n",
      "Iteration 132, loss = 0.00990103\n",
      "Iteration 133, loss = 0.00986629\n",
      "Iteration 134, loss = 0.00994123\n",
      "Iteration 135, loss = 0.00982166\n",
      "Iteration 136, loss = 0.00984260\n",
      "Iteration 137, loss = 0.01012103\n",
      "Iteration 138, loss = 0.00982487\n",
      "Iteration 139, loss = 0.00971793\n",
      "Iteration 140, loss = 0.01031044\n",
      "Iteration 141, loss = 0.00963458\n",
      "Iteration 142, loss = 0.00955052\n",
      "Iteration 143, loss = 0.00989365\n",
      "Iteration 144, loss = 0.00963704\n",
      "Iteration 145, loss = 0.00958326\n",
      "Iteration 146, loss = 0.00932547\n",
      "Iteration 147, loss = 0.00963385\n",
      "Iteration 148, loss = 0.00938542\n",
      "Iteration 149, loss = 0.00934890\n",
      "Iteration 150, loss = 0.00955090\n",
      "Iteration 151, loss = 0.00922566\n",
      "Iteration 152, loss = 0.00963669\n",
      "Iteration 153, loss = 0.00896020\n",
      "Iteration 154, loss = 0.00935229\n",
      "Iteration 155, loss = 0.00920341\n",
      "Iteration 156, loss = 0.00923385\n",
      "Iteration 157, loss = 0.00905357\n",
      "Iteration 158, loss = 0.00904967\n",
      "Iteration 159, loss = 0.00903936\n",
      "Iteration 160, loss = 0.00895413\n",
      "Iteration 161, loss = 0.00903390\n",
      "Iteration 162, loss = 0.00904950\n",
      "Iteration 163, loss = 0.00927259\n",
      "Iteration 164, loss = 0.00881082\n",
      "Iteration 165, loss = 0.00902811\n",
      "Iteration 166, loss = 0.00889243\n",
      "Iteration 167, loss = 0.00855733\n",
      "Iteration 168, loss = 0.00875628\n",
      "Iteration 169, loss = 0.00878469\n",
      "Iteration 170, loss = 0.00846651\n",
      "Iteration 171, loss = 0.00849656\n",
      "Iteration 172, loss = 0.00897088\n",
      "Iteration 173, loss = 0.00849153\n",
      "Iteration 174, loss = 0.00854196\n",
      "Iteration 175, loss = 0.00858937\n",
      "Iteration 176, loss = 0.00839828\n",
      "Iteration 177, loss = 0.00832776\n",
      "Iteration 178, loss = 0.00835883\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45266589\n",
      "Iteration 2, loss = 0.34333713\n",
      "Iteration 3, loss = 0.28065062\n",
      "Iteration 4, loss = 0.23687385\n",
      "Iteration 5, loss = 0.20242563\n",
      "Iteration 6, loss = 0.17575603\n",
      "Iteration 7, loss = 0.15683473\n",
      "Iteration 8, loss = 0.13894261\n",
      "Iteration 9, loss = 0.12403416\n",
      "Iteration 10, loss = 0.11200804\n",
      "Iteration 11, loss = 0.10178319\n",
      "Iteration 12, loss = 0.09177020\n",
      "Iteration 13, loss = 0.08315142\n",
      "Iteration 14, loss = 0.07704703\n",
      "Iteration 15, loss = 0.07043609\n",
      "Iteration 16, loss = 0.06494194\n",
      "Iteration 17, loss = 0.06085682\n",
      "Iteration 18, loss = 0.05602822\n",
      "Iteration 19, loss = 0.05243338\n",
      "Iteration 20, loss = 0.04893377\n",
      "Iteration 21, loss = 0.04620010\n",
      "Iteration 22, loss = 0.04341694\n",
      "Iteration 23, loss = 0.04130589\n",
      "Iteration 24, loss = 0.03944269\n",
      "Iteration 25, loss = 0.03779537\n",
      "Iteration 26, loss = 0.03545159\n",
      "Iteration 27, loss = 0.03401499\n",
      "Iteration 28, loss = 0.03243555\n",
      "Iteration 29, loss = 0.03075905\n",
      "Iteration 30, loss = 0.02994654\n",
      "Iteration 31, loss = 0.02900919\n",
      "Iteration 32, loss = 0.02764090\n",
      "Iteration 33, loss = 0.02671134\n",
      "Iteration 34, loss = 0.02596460\n",
      "Iteration 35, loss = 0.02490519\n",
      "Iteration 36, loss = 0.02462935\n",
      "Iteration 37, loss = 0.02386605\n",
      "Iteration 38, loss = 0.02309817\n",
      "Iteration 39, loss = 0.02259673\n",
      "Iteration 40, loss = 0.02254200\n",
      "Iteration 41, loss = 0.02167549\n",
      "Iteration 42, loss = 0.02138187\n",
      "Iteration 43, loss = 0.02094277\n",
      "Iteration 44, loss = 0.02020027\n",
      "Iteration 45, loss = 0.02033096\n",
      "Iteration 46, loss = 0.01997445\n",
      "Iteration 47, loss = 0.01972127\n",
      "Iteration 48, loss = 0.01903215\n",
      "Iteration 49, loss = 0.01873717\n",
      "Iteration 50, loss = 0.01838449\n",
      "Iteration 51, loss = 0.01801062\n",
      "Iteration 52, loss = 0.01815662\n",
      "Iteration 53, loss = 0.01768883\n",
      "Iteration 54, loss = 0.01806722\n",
      "Iteration 55, loss = 0.01772725\n",
      "Iteration 56, loss = 0.01773269\n",
      "Iteration 57, loss = 0.01691766\n",
      "Iteration 58, loss = 0.01649569\n",
      "Iteration 59, loss = 0.01635651\n",
      "Iteration 60, loss = 0.01606227\n",
      "Iteration 61, loss = 0.01588946\n",
      "Iteration 62, loss = 0.01601838\n",
      "Iteration 63, loss = 0.01556158\n",
      "Iteration 64, loss = 0.01566812\n",
      "Iteration 65, loss = 0.01551414\n",
      "Iteration 66, loss = 0.01527554\n",
      "Iteration 67, loss = 0.01548387\n",
      "Iteration 68, loss = 0.01500959\n",
      "Iteration 69, loss = 0.01485864\n",
      "Iteration 70, loss = 0.01522738\n",
      "Iteration 71, loss = 0.01484380\n",
      "Iteration 72, loss = 0.01436552\n",
      "Iteration 73, loss = 0.01436256\n",
      "Iteration 74, loss = 0.01422265\n",
      "Iteration 75, loss = 0.01450008\n",
      "Iteration 76, loss = 0.01413818\n",
      "Iteration 77, loss = 0.01413410\n",
      "Iteration 78, loss = 0.01436797\n",
      "Iteration 79, loss = 0.01368868\n",
      "Iteration 80, loss = 0.01364806\n",
      "Iteration 81, loss = 0.01353474\n",
      "Iteration 82, loss = 0.01347782\n",
      "Iteration 83, loss = 0.01401897\n",
      "Iteration 84, loss = 0.01338842\n",
      "Iteration 85, loss = 0.01325766\n",
      "Iteration 86, loss = 0.01318873\n",
      "Iteration 87, loss = 0.01341255\n",
      "Iteration 88, loss = 0.01306659\n",
      "Iteration 89, loss = 0.01274513\n",
      "Iteration 90, loss = 0.01274396\n",
      "Iteration 91, loss = 0.01259795\n",
      "Iteration 92, loss = 0.01239593\n",
      "Iteration 93, loss = 0.01259650\n",
      "Iteration 94, loss = 0.01229495\n",
      "Iteration 95, loss = 0.01232178\n",
      "Iteration 96, loss = 0.01216739\n",
      "Iteration 97, loss = 0.01199001\n",
      "Iteration 98, loss = 0.01218580\n",
      "Iteration 99, loss = 0.01221790\n",
      "Iteration 100, loss = 0.01188147LinearRegression\n"
     ]
    }
   ],
   "source": [
    "step_size = 100\n",
    "df_full = DataFrame(Method=String[],  Value=Float64[], Size=Int[]);\n",
    "n_runs = 3\n",
    "\n",
    "for n ∈ 100:step_size:size(df_raw)[1] \n",
    "    \n",
    "    # clustering the entire subspace to identify n samples\n",
    "    idx = kmeans_subset(select(df_raw, Not(:Cost)), n)\n",
    "    df = df_raw[idx,:]\n",
    "\n",
    "    ### split the data into test and training ###\n",
    "    X_train, y_train, X_test, y_test = P2H_CapacityExpansion.partitionTrainTest(df, [:Cost, :Emission, :Generation], 0.8)\n",
    "\n",
    "    ### scale the data ###\n",
    "    X_train_scaled, μX, σX  = P2H_CapacityExpansion.scaling(X_train)\n",
    "    X_test_scaled = (X_test .- μX) ./ σX\n",
    "    y_train_scaled, μy, σy  = P2H_CapacityExpansion.scaling(y_train)\n",
    "    \n",
    "    # remove np.nan #\n",
    "    for i in eachindex(X_test_scaled)\n",
    "        if isnan(X_test_scaled[i])\n",
    "            X_test_scaled[i] = 0.0\n",
    "        end\n",
    "    end\n",
    "\n",
    "    ### train ML model and compute R2 ### \n",
    "    for (name, fun) ∈ models\n",
    "        println(name)\n",
    "        # determine average for non-deterministic models\n",
    "        iter = name ∈ stochastic_models ? n_runs : 1\n",
    "        \n",
    "        r2 = 0\n",
    "        for k ∈ 1:iter\n",
    "            sg = fun(X_train_scaled, y_train_scaled, X_test_scaled)\n",
    "            ŷ_rescaled = sg.prediction .* σy .+ μy\n",
    "            r2 += P2H_CapacityExpansion.r2_score(y_test, ŷ_rescaled)\n",
    "        end\n",
    "\n",
    "        ### add to the df ### \n",
    "        push!(df_full, (Method = name, Value = r2/iter, Size = size(X_train)[1]))\n",
    "    end \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = AbstractTrace[]  # Correct type for individual traces\n",
    "sort!(df_full, [:Method, :Size], rev=true)\n",
    "\n",
    "for m in unique(df_full.Method)\n",
    "    subdf = filter(:Method => ==(m), df_full)\n",
    "    trace = scatter(\n",
    "        x = subdf.Size,\n",
    "        y = subdf.Value,\n",
    "        mode = \"lines+markers\",\n",
    "        name = string(m)\n",
    "    )\n",
    "    push!(traces, trace)\n",
    "end\n",
    "\n",
    "# Define the layout\n",
    "layout = Layout(\n",
    "    title = \"Value vs Size by Method\",\n",
    "    xaxis_title = \"Training Data\",\n",
    "    yaxis_title = \"Accuracy\"\n",
    ")\n",
    "\n",
    "# Create a single Plot from traces and layout\n",
    "plt = plot(traces, layout)\n",
    "\n",
    "# Show the plot\n",
    "display(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-sampling strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = filter(f -> endswith(f, \".txt\"), readdir(dir, join=true));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 50\n",
    "df_full = DataFrame(Method=String[],  Value=Float64[], Size=Int[]);\n",
    "\n",
    "\n",
    "### ALTERNATIVE 1: RANDOM SAMPLING ### \n",
    "for n in 140:step_size:size(df_raw)[1] #unique(ceil(Int, size(df_raw)[1]/n) for n in 140:step_size:size(df_raw)[1])\n",
    "    df = df_raw[StatsBase.sample(1:nrow(df_raw), n; replace=false), :]\n",
    "end\n",
    "\n",
    "### ALTERNATIVE 2: EQUAL SELECTION SAMPLING ### \n",
    "for n in unique(ceil(Int, size(df_raw)[1]/n) for n in 140:step_size:size(df_raw)[1])\n",
    "    df = df_raw[1:n:end, :]\n",
    "    df = select(df, names(df)[[sum(df[!, col]) != 0 for col in names(df)]])\n",
    "end\n",
    "\n",
    "### ALTERNATIVE 3: LHS ### \n",
    "for f in files_list\n",
    "    df = P2H_CapacityExpansion.read_txt_file(f)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read, normalize and transpose the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = P2H_CapacityExpansion.read_txt_file(file);\n",
    "df_raw = select(df_raw, Not(:Cost))\n",
    "m = Matrix(df_raw)\n",
    "x_norm, μ, σ = P2H_CapacityExpansion.scaling(m)\n",
    "X = x_norm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate PCA model\n",
    "model = fit(PCA, X; maxoutdim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose the data back again \n",
    "X_transform = MultivariateStats.transform(model, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca =   DataFrame(permutedims(X_transform), :auto)\n",
    "df_pca.Cost = df_raw.Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.reddit.com/r/deeplearning/comments/14vnfe8/how_to_decrease_high_loss_values/\n",
    "https://discourse.julialang.org/t/how-to-efficiently-and-precisely-fit-a-function-with-neural-networks/73726\n",
    "https://stackoverflow.com/questions/59153248/why-is-my-neural-network-stuck-at-high-loss-value-after-the-first-epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = DataFrame(Method=String[],  Value=Float64[], Size=Int[]);\n",
    "\n",
    "df = P2H_CapacityExpansion.read_txt_file(file);\n",
    "\n",
    "### split the data into test and training ###\n",
    "X_train, y_train, X_test, y_test = P2H_CapacityExpansion.partitionTrainTest(df, :Cost, 0.7)\n",
    "\n",
    "### scale the data ###\n",
    "X_train_scaled, μX, σX  = P2H_CapacityExpansion.scaling(X_train)\n",
    "X_test_scaled = (X_test .- μX) ./ σX\n",
    "\n",
    "    # remove np.nan #\n",
    "for i in eachindex(X_test_scaled)\n",
    "    if isnan(X_test_scaled[i])\n",
    "        X_test_scaled[i] = 0.0\n",
    "    end\n",
    "end\n",
    "y_train_scaled, μy, σy  = P2H_CapacityExpansion.scaling(y_train)\n",
    "\n",
    "### train ML model and compute R2 ### \n",
    "for (name,fun) ∈ models\n",
    "    sg = fun(X_train_scaled, y_train_scaled, X_test_scaled)\n",
    "    ŷ_rescaled = sg.prediction .* σy .+ μy\n",
    "    r2 = P2H_CapacityExpansion.r2_score(y_test, ŷ_rescaled)\n",
    "\n",
    "    ### add to the df ### \n",
    "    push!(df_full, (Method = name, Value = r2, Size = size(X_train)[1]))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = DataFrame(Method=String[],  Value=Float64[], Size=Int[]);\n",
    "\n",
    "df = df_pca\n",
    "\n",
    "### split the data into test and training ###\n",
    "X_train, y_train, X_test, y_test = P2H_CapacityExpansion.partitionTrainTest(df, :Cost, 0.7)\n",
    "\n",
    "### scale the data ###\n",
    "X_train_scaled, μX, σX  = P2H_CapacityExpansion.scaling(X_train)\n",
    "X_test_scaled = (X_test .- μX) ./ σX\n",
    "\n",
    "# remove np.nan #\n",
    "for i in eachindex(X_test_scaled)\n",
    "    if isnan(X_test_scaled[i])\n",
    "        X_test_scaled[i] = 0.0\n",
    "    end\n",
    "end\n",
    "y_train_scaled, μy, σy  = P2H_CapacityExpansion.scaling(y_train)\n",
    "\n",
    "### train ML model and compute R2 ### \n",
    "for (name,fun) ∈ models\n",
    "    sg = fun(X_train_scaled, y_train_scaled, X_test_scaled)\n",
    "    ŷ_rescaled = sg.prediction .* σy .+ μy\n",
    "    r2 = P2H_CapacityExpansion.r2_score(y_test, ŷ_rescaled)\n",
    "\n",
    "    ### add to the df ### \n",
    "    push!(df_full, (Method = name, Value = r2, Size = size(X_train)[1]))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new sampling technique\n",
    "iterate more often through the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Loop through files with index\n",
    "\n",
    "# Parameters\n",
    "techs = setdiff([key for (key, val) ∈ config[\"techs\"] if get(val, \"inv\", \"\")  == true], [key for (key, val) ∈ config[\"techs\"] if get(val, \"tech_group\", \"\")  == \"transmission\"] ) \n",
    "years = config[\"year\"]\n",
    "scenarios = 1:length(txt_files)\n",
    "\n",
    "# Column names\n",
    "columns = [:scenario, :year, :cost] ∪ Symbol.(techs)\n",
    "\n",
    "df = DataFrame(;\n",
    "    :scenario => repeat(scenarios, inner=length(years)),\n",
    "    :year => repeat(years, outer=length(txt_files)),\n",
    "    :cost => fill(0.0, length(txt_files)*length(years)),\n",
    ")\n",
    "\n",
    "# Add technology columns, initialized to 0.0\n",
    "for tech in techs\n",
    "    df[!, Symbol(tech)] = fill(0.0, length(txt_files)*length(years))\n",
    "end\n",
    "\n",
    "## fill in the values\n",
    "for file in txt_files\n",
    "    lines = readlines(file)\n",
    "    \n",
    "    i = parse(Int64, split(split(file, \"/\")[end], \"_\")[1])\n",
    "\n",
    "    # Parse technology capacities\n",
    "    for line in lines\n",
    "        if occursin(\"TotalCapacityAnnual\", line)\n",
    "            g = split(line, \",\")[2]   \n",
    "            if g in techs\n",
    "                val = parse(Float64, strip(split(line, \"=\")[2]))\n",
    "                y = parse(Int64, split(split(line, \"]\")[1], \",\")[end])\n",
    "\n",
    "                # insert into the dataframe\n",
    "                idx = findfirst((df.year .== y) .& (df.scenario .== i))\n",
    "                df[idx, Symbol(g)] = val\n",
    "            end\n",
    "\n",
    "        \n",
    "        elseif occursin(\"COSTvar\", line)\n",
    "            y = parse(Int64, line[8:12])\n",
    "            val = parse(Float64, strip(split(line, \"=\")[2]))\n",
    "            # insert into the dataframe\n",
    "            idx = findfirst((df.year .== y) .& (df.scenario .== i))\n",
    "            df[idx, :cost] = val\n",
    "        end\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = df[:, Not(:year, :cost, :scenario)]\n",
    "X = transpose(Matrix(df_agg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ub = [1.25, 620, 460, 300, 0.06, 0.93, 20]\n",
    "lb = [0.75, 420, 260, 100, 0.015, 0.56, 13]\n",
    "\n",
    "\n",
    "# Number of samples\n",
    "n = 3\n",
    "\n",
    "# Latin Hypercube Sampling\n",
    "scenarios = Surrogates.sample(n,lb,ub, Surrogates.LatinHypercubeSample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. TRAIN THE MODEL #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function perclass_splits(y, percent)\n",
    "    uniq_class = unique(y)\n",
    "    keep_index = []\n",
    "    for class in uniq_class\n",
    "        class_index = findall(y .== class)\n",
    "        row_index = randsubseq(class_index, percent)\n",
    "        push!(keep_index, row_index...)\n",
    "    end\n",
    "    return keep_index\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[!, :cost]\n",
    "\n",
    "# split data between train and test\n",
    "Random.seed!(1)\n",
    "train_index = perclass_splits(y, 0.67)\n",
    "test_index = setdiff(1:length(y), train_index)\n",
    "\n",
    "# spit features\n",
    "X_train = X[:, train_index]\n",
    "X_test = X[:, test_index]\n",
    "\n",
    "# split classes\n",
    "y_train = transpose(Array{Float64}(y[train_index]))\n",
    "y_test = transpose(Array{Float64}(y[test_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Chain(\n",
    "    Dense(12, 32, relu),\n",
    "    Dense(32, 1)  # output: a single float\n",
    ")\n",
    "\n",
    "loss(x, y) = Flux.Losses.mse(model(x), y)  # or Flux.Losses.mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track parameters\n",
    "ps = Flux.params(model)\n",
    " # select an optimizer\n",
    "learning_rate = 0.01\n",
    "opt = ADAM(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "loss_history = []\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "for epoch in 1:epochs\n",
    "    # train the model\n",
    "    train!(loss, ps, [(X_train, y_train)], opt)\n",
    "    # print report\n",
    "    train_loss = loss(X_train, y_train)\n",
    "    push!(loss_history, train_loss)\n",
    "    println(\"Epoch = $epoch : Training loss = $train_loss\")\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a combined scenario-year column\n",
    "df_long[!, :scenario_year] = string.(df_long.scenario, \"_\", df_long.year)\n",
    "\n",
    "# Step 2: Pivot wide: rows = technology, columns = scenario_year, values = value\n",
    "df_wide = unstack(df_long, :technology, :scenario_year, :value)\n",
    "\n",
    "# Show the result as a matrix\n",
    "X = Matrix(df_wide[:, Not(:technology)])\n",
    "X = coalesce.(X, 0.0)\n",
    "#https://medium.com/@mandarangchekar7/a-neural-network-explained-and-implemented-in-julia-1fbfe4aaf0df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_hat_raw = model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = onecold(y_hat_raw) .- 1\n",
    "y = y_test_raw\n",
    "mean(y_hat .== y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
