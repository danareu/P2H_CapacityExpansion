{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module P2H_CapacityExpansion.\n",
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/git`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "include(\"./P2H_CapacityExpansion.jl\")\n",
    "cd(\"/cluster/home/danare/git\")\n",
    "Pkg.activate(\".\")\n",
    "using .P2H_CapacityExpansion\n",
    "using DataFrames\n",
    "using Parameters\n",
    "using Flux\n",
    "using Surrogates\n",
    "using ScikitLearn\n",
    "using LinearAlgebra, Random, Statistics\n",
    "using JuMP\n",
    "using XLSX\n",
    "using PlotlyJS\n",
    "using Clustering\n",
    "using CSV\n",
    "using Dates\n",
    "using PyCall\n",
    "@pyimport joblib\n",
    "using BSON\n",
    "using StatsBase, MultivariateStats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Each time-series is averaged in 3-hourly steps\n",
      "└ @ Main.P2H_CapacityExpansion /cluster/home/danare/git/P2H_CapacityExpansion/utils/load_data.jl:76\n"
     ]
    }
   ],
   "source": [
    "# read in the data\n",
    "config = P2H_CapacityExpansion.read_yaml_file();\n",
    "data = P2H_CapacityExpansion.load_cep_data(config=config);\n",
    "ts_data = P2H_CapacityExpansion.load_timeseries_data_full(config=config);\n",
    "gas_gen = [gen for (gen, props) ∈ config[\"techs\"] if haskey(props, \"input\") && get(props[\"input\"], \"fuel\", nothing) == \"R_Gas\"];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/cluster/home/danare/git/P2H_CapacityExpansion/results/500_scenarios.txt\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir = \"/cluster/home/danare/git/P2H_CapacityExpansion/results/aggregated_results/\"\n",
    "file = \"/cluster/home/danare/git/P2H_CapacityExpansion/results/500_scenarios.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = P2H_CapacityExpansion.read_txt_file(file);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(628, 13)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "size(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate though the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = Dict(\n",
    "    \"RandomForest\" => P2H_CapacityExpansion.random_forest_sklearn,\n",
    "    \"DecisionTree\" => P2H_CapacityExpansion.decision_tree_sklearn,\n",
    "    \"LinearRegression\" => P2H_CapacityExpansion.linear_regression_sklearn,\n",
    "    \"NeuralNetwork\" => P2H_CapacityExpansion.simple_neural_network_sklearn,\n",
    "    \"GaussianProcesses\" => P2H_CapacityExpansion.gaussian_process,\n",
    "    \"SVR\" => P2H_CapacityExpansion.svr_sklearn,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kmeans_subset (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function kmeans_subset(X, n)\n",
    "    R = kmeans(Matrix(X)', n) \n",
    "    idx = [findfirst(==(i), R.assignments) for i in 1:n]\n",
    "    return idx\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Each time-series is averaged in 3-hourly steps\n",
      "└ @ Main.P2H_CapacityExpansion /cluster/home/danare/git/P2H_CapacityExpansion/utils/load_data.jl:76\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2-element Vector{String}:\n",
       " \"P_Gas_CCGT\"\n",
       " \"X_ATR_CCS\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# read in the data\n",
    "config = P2H_CapacityExpansion.read_yaml_file();\n",
    "data = P2H_CapacityExpansion.load_cep_data(config=config);\n",
    "ts_data = P2H_CapacityExpansion.load_timeseries_data_full(config=config);\n",
    "gas_gen = [gen for (gen, props) ∈ config[\"techs\"] if haskey(props, \"input\") && get(props[\"input\"], \"fuel\", nothing) == \"R_Gas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Reading the data ...\n",
      "└ @ Main.P2H_CapacityExpansion /cluster/home/danare/git/P2H_CapacityExpansion/src/opt.jl:17\n",
      "┌ Info: Setting up the optimization variables ...\n",
      "└ @ Main.P2H_CapacityExpansion /cluster/home/danare/git/P2H_CapacityExpansion/src/opt.jl:20\n"
     ]
    }
   ],
   "source": [
    "cep = P2H_CapacityExpansion.run_opt(ts_data=ts_data, data=data, config=config)\n",
    "result = P2H_CapacityExpansion.optimize_and_output(cep=cep, config=config, data=data, ts_data=ts_data, name=\"_scenario\", short_sol=true)\n",
    "#3m33s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_models = Set([\"RandomForest\", \"NeuralNetwork\", \"DecisionTree\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 68, loss = 0.01569819\n",
      "Iteration 69, loss = 0.01525565\n",
      "Iteration 70, loss = 0.01521764\n",
      "Iteration 71, loss = 0.01475422\n",
      "Iteration 72, loss = 0.01478179\n",
      "Iteration 73, loss = 0.01444561\n",
      "Iteration 74, loss = 0.01396121\n",
      "Iteration 75, loss = 0.01372459\n",
      "Iteration 76, loss = 0.01356081\n",
      "Iteration 77, loss = 0.01360341\n",
      "Iteration 78, loss = 0.01330896\n",
      "Iteration 79, loss = 0.01313113\n",
      "Iteration 80, loss = 0.01309484\n",
      "Iteration 81, loss = 0.01274517\n",
      "Iteration 82, loss = 0.01280228\n",
      "Iteration 83, loss = 0.01233258\n",
      "Iteration 84, loss = 0.01216195\n",
      "Iteration 85, loss = 0.01202949\n",
      "Iteration 86, loss = 0.01172370\n",
      "Iteration 87, loss = 0.01153841\n",
      "Iteration 88, loss = 0.01174672\n",
      "Iteration 89, loss = 0.01134232\n",
      "Iteration 90, loss = 0.01118495\n",
      "Iteration 91, loss = 0.01094761\n",
      "Iteration 92, loss = 0.01090845\n",
      "Iteration 93, loss = 0.01060558\n",
      "Iteration 94, loss = 0.01052420\n",
      "Iteration 95, loss = 0.01039175\n",
      "Iteration 96, loss = 0.01068086\n",
      "Iteration 97, loss = 0.01027306\n",
      "Iteration 98, loss = 0.01009932\n",
      "Iteration 99, loss = 0.01015253\n",
      "Iteration 100, loss = 0.01012070\n",
      "Iteration 101, loss = 0.01005368\n",
      "Iteration 102, loss = 0.00985375\n",
      "Iteration 103, loss = 0.00941052\n",
      "Iteration 104, loss = 0.00948942\n",
      "Iteration 105, loss = 0.00955661\n",
      "Iteration 106, loss = 0.00990313\n",
      "Iteration 107, loss = 0.00963207\n",
      "Iteration 108, loss = 0.00934465\n",
      "Iteration 109, loss = 0.00915311\n",
      "Iteration 110, loss = 0.00875745\n",
      "Iteration 111, loss = 0.00864178\n",
      "Iteration 112, loss = 0.00872644\n",
      "Iteration 113, loss = 0.00845190\n",
      "Iteration 114, loss = 0.00844899\n",
      "Iteration 115, loss = 0.00883490\n",
      "Iteration 116, loss = 0.00944448\n",
      "Iteration 117, loss = 0.00859289\n",
      "Iteration 118, loss = 0.00820483\n",
      "Iteration 119, loss = 0.00855227\n",
      "Iteration 120, loss = 0.00843775\n",
      "Iteration 121, loss = 0.00809151\n",
      "Iteration 122, loss = 0.00786432\n",
      "Iteration 123, loss = 0.00838917\n",
      "Iteration 124, loss = 0.00812085\n",
      "Iteration 125, loss = 0.00770181\n",
      "Iteration 126, loss = 0.00783630\n",
      "Iteration 127, loss = 0.00755083\n",
      "Iteration 128, loss = 0.00785842\n",
      "Iteration 129, loss = 0.00830919\n",
      "Iteration 130, loss = 0.00798301\n",
      "Iteration 131, loss = 0.00871385\n",
      "Iteration 132, loss = 0.00758511\n",
      "Iteration 133, loss = 0.00753096\n",
      "Iteration 134, loss = 0.00732308\n",
      "Iteration 135, loss = 0.00695556\n",
      "Iteration 136, loss = 0.00749037\n",
      "Iteration 137, loss = 0.00762245\n",
      "Iteration 138, loss = 0.00753925\n",
      "Iteration 139, loss = 0.00734664\n",
      "Iteration 140, loss = 0.00742509\n",
      "Iteration 141, loss = 0.00719049\n",
      "Iteration 142, loss = 0.00730140\n",
      "Iteration 143, loss = 0.00734375\n",
      "Iteration 144, loss = 0.00716044\n",
      "Iteration 145, loss = 0.00741999\n",
      "Iteration 146, loss = 0.00709106\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.47221987\n",
      "Iteration 2, loss = 0.44459268\n",
      "Iteration 3, loss = 0.41957243\n",
      "Iteration 4, loss = 0.39683954\n",
      "Iteration 5, loss = 0.37606038\n",
      "Iteration 6, loss = 0.35716056\n",
      "Iteration 7, loss = 0.33999541\n",
      "Iteration 8, loss = 0.32438154\n",
      "Iteration 9, loss = 0.31020697\n",
      "Iteration 10, loss = 0.29731129\n",
      "Iteration 11, loss = 0.28539320\n",
      "Iteration 12, loss = 0.27430226\n",
      "Iteration 13, loss = 0.26387360\n",
      "Iteration 14, loss = 0.25397779\n",
      "Iteration 15, loss = 0.24449357\n",
      "Iteration 16, loss = 0.23542331\n",
      "Iteration 17, loss = 0.22670164\n",
      "Iteration 18, loss = 0.21835410\n",
      "Iteration 19, loss = 0.21026710\n",
      "Iteration 20, loss = 0.20250441\n",
      "Iteration 21, loss = 0.19506460\n",
      "Iteration 22, loss = 0.18776576\n",
      "Iteration 23, loss = 0.18058848\n",
      "Iteration 24, loss = 0.17354452\n",
      "Iteration 25, loss = 0.16661335\n",
      "Iteration 26, loss = 0.15983560\n",
      "Iteration 27, loss = 0.15320347\n",
      "Iteration 28, loss = 0.14673035\n",
      "Iteration 29, loss = 0.14049374\n",
      "Iteration 30, loss = 0.13450540\n",
      "Iteration 31, loss = 0.12879133\n",
      "Iteration 32, loss = 0.12338440\n",
      "Iteration 33, loss = 0.11820979\n",
      "Iteration 34, loss = 0.11321669\n",
      "Iteration 35, loss = 0.10840444\n",
      "Iteration 36, loss = 0.10375667\n",
      "Iteration 37, loss = 0.09926261\n",
      "Iteration 38, loss = 0.09495499\n",
      "Iteration 39, loss = 0.09085395\n",
      "Iteration 40, loss = 0.08690820\n",
      "Iteration 41, loss = 0.08312364\n",
      "Iteration 42, loss = 0.07952051\n",
      "Iteration 43, loss = 0.07613717\n",
      "Iteration 44, loss = 0.07293465\n",
      "Iteration 45, loss = 0.06993991\n",
      "Iteration 46, loss = 0.06723385\n",
      "Iteration 47, loss = 0.06473551\n",
      "Iteration 48, loss = 0.06240239\n",
      "Iteration 49, loss = 0.06023765\n",
      "Iteration 50, loss = 0.05819409\n",
      "Iteration 51, loss = 0.05628166\n",
      "Iteration 52, loss = 0.05449250\n",
      "Iteration 53, loss = 0.05283358\n",
      "Iteration 54, loss = 0.05131866\n",
      "Iteration 55, loss = 0.04995528\n",
      "Iteration 56, loss = 0.04871843\n",
      "Iteration 57, loss = 0.04757420\n",
      "Iteration 58, loss = 0.04650381\n",
      "Iteration 59, loss = 0.04550387\n",
      "Iteration 60, loss = 0.04460255\n",
      "Iteration 61, loss = 0.04377364\n",
      "Iteration 62, loss = 0.04298921\n",
      "Iteration 63, loss = 0.04225631\n",
      "Iteration 64, loss = 0.04157184\n",
      "Iteration 65, loss = 0.04094810\n",
      "Iteration 66, loss = 0.04036162\n",
      "Iteration 67, loss = 0.03982604\n",
      "Iteration 68, loss = 0.03929342\n",
      "Iteration 69, loss = 0.03876883\n",
      "Iteration 70, loss = 0.03825506\n",
      "Iteration 71, loss = 0.03773998\n",
      "Iteration 72, loss = 0.03723390\n",
      "Iteration 73, loss = 0.03671506\n",
      "Iteration 74, loss = 0.03619556\n",
      "Iteration 75, loss = 0.03567806\n",
      "Iteration 76, loss = 0.03518451\n",
      "Iteration 77, loss = 0.03469310\n",
      "Iteration 78, loss = 0.03420875\n",
      "Iteration 79, loss = 0.03371787\n",
      "Iteration 80, loss = 0.03324799\n",
      "Iteration 81, loss = 0.03278165\n",
      "Iteration 82, loss = 0.03231343\n",
      "Iteration 83, loss = 0.03186212\n",
      "Iteration 84, loss = 0.03143373\n",
      "Iteration 85, loss = 0.03101201\n",
      "Iteration 86, loss = 0.03058951\n",
      "Iteration 87, loss = 0.03016232\n",
      "Iteration 88, loss = 0.02973124\n",
      "Iteration 89, loss = 0.02930398\n",
      "Iteration 90, loss = 0.02889320\n",
      "Iteration 91, loss = 0.02849053\n",
      "Iteration 92, loss = 0.02807597\n",
      "Iteration 93, loss = 0.02766147\n",
      "Iteration 94, loss = 0.02725899\n",
      "Iteration 95, loss = 0.02686545\n",
      "Iteration 96, loss = 0.02648699\n",
      "Iteration 97, loss = 0.02611596\n",
      "Iteration 98, loss = 0.02573970\n",
      "Iteration 99, loss = 0.02537002\n",
      "Iteration 100, loss = 0.02498967\n",
      "Iteration 101, loss = 0.02460687\n",
      "Iteration 102, loss = 0.02423758\n",
      "Iteration 103, loss = 0.02387729\n",
      "Iteration 104, loss = 0.02352700\n",
      "Iteration 105, loss = 0.02316070\n",
      "Iteration 106, loss = 0.02279632\n",
      "Iteration 107, loss = 0.02243444\n",
      "Iteration 108, loss = 0.02207971\n",
      "Iteration 109, loss = 0.02172714\n",
      "Iteration 110, loss = 0.02138419\n",
      "Iteration 111, loss = 0.02104909\n",
      "Iteration 112, loss = 0.02072169\n",
      "Iteration 113, loss = 0.02039624\n",
      "Iteration 114, loss = 0.02008088\n",
      "Iteration 115, loss = 0.01976909\n",
      "Iteration 116, loss = 0.01945775\n",
      "Iteration 117, loss = 0.01914595\n",
      "Iteration 118, loss = 0.01883742\n",
      "Iteration 119, loss = 0.01852763\n",
      "Iteration 120, loss = 0.01822413\n",
      "Iteration 121, loss = 0.01791768\n",
      "Iteration 122, loss = 0.01760944\n",
      "Iteration 123, loss = 0.01730546\n",
      "Iteration 124, loss = 0.01700753\n",
      "Iteration 125, loss = 0.01670855\n",
      "Iteration 126, loss = 0.01640089\n",
      "Iteration 127, loss = 0.01609061\n",
      "Iteration 128, loss = 0.01578083\n",
      "Iteration 129, loss = 0.01547665\n",
      "Iteration 130, loss = 0.01517918\n",
      "Iteration 131, loss = 0.01489314\n",
      "Iteration 132, loss = 0.01461726\n",
      "Iteration 133, loss = 0.01432874\n",
      "Iteration 134, loss = 0.01403852\n",
      "Iteration 135, loss = 0.01376295\n",
      "Iteration 136, loss = 0.01349179\n",
      "Iteration 137, loss = 0.01321423\n",
      "Iteration 138, loss = 0.01295200\n",
      "Iteration 139, loss = 0.01269505\n",
      "Iteration 140, loss = 0.01243198\n",
      "Iteration 141, loss = 0.01216896\n",
      "Iteration 142, loss = 0.01192472\n",
      "Iteration 143, loss = 0.01167622\n",
      "Iteration 144, loss = 0.01142844\n",
      "Iteration 145, loss = 0.01118528\n",
      "Iteration 146, loss = 0.01094728\n",
      "Iteration 147, loss = 0.01071611\n",
      "Iteration 148, loss = 0.01048380\n",
      "Iteration 149, loss = 0.01025461\n",
      "Iteration 150, loss = 0.01002990\n",
      "Iteration 151, loss = 0.00981164\n",
      "Iteration 152, loss = 0.00959224\n",
      "Iteration 153, loss = 0.00938225\n",
      "Iteration 154, loss = 0.00916972\n",
      "Iteration 155, loss = 0.00896577\n",
      "Iteration 156, loss = 0.00876477\n",
      "Iteration 157, loss = 0.00856824\n",
      "Iteration 158, loss = 0.00837823\n",
      "Iteration 159, loss = 0.00819577\n",
      "Iteration 160, loss = 0.00802127\n",
      "Iteration 161, loss = 0.00784755\n",
      "Iteration 162, loss = 0.00767474\n",
      "Iteration 163, loss = 0.00751079\n",
      "Iteration 164, loss = 0.00734193\n",
      "Iteration 165, loss = 0.00717759\n",
      "Iteration 166, loss = 0.00701706\n",
      "Iteration 167, loss = 0.00685819\n",
      "Iteration 168, loss = 0.00670546\n",
      "Iteration 169, loss = 0.00655955\n",
      "Iteration 170, loss = 0.00641311\n",
      "Iteration 171, loss = 0.00626884\n",
      "Iteration 172, loss = 0.00612727\n",
      "Iteration 173, loss = 0.00598987\n",
      "Iteration 174, loss = 0.00585887\n",
      "Iteration 175, loss = 0.00572662\n",
      "Iteration 176, loss = 0.00559767\n",
      "Iteration 177, loss = 0.00547069\n",
      "Iteration 178, loss = 0.00535357\n",
      "Iteration 179, loss = 0.00523663\n",
      "Iteration 180, loss = 0.00512047\n",
      "Iteration 181, loss = 0.00500822\n",
      "Iteration 182, loss = 0.00490061\n",
      "Iteration 183, loss = 0.00479172\n",
      "Iteration 184, loss = 0.00468853\n",
      "Iteration 185, loss = 0.00458775\n",
      "Iteration 186, loss = 0.00449048\n",
      "Iteration 187, loss = 0.00439242\n",
      "Iteration 188, loss = 0.00429875\n",
      "Iteration 189, loss = 0.00420707\n",
      "Iteration 190, loss = 0.00411823\n",
      "Iteration 191, loss = 0.00403266\n",
      "Iteration 192, loss = 0.00394735\n",
      "Iteration 193, loss = 0.00386289\n",
      "Iteration 194, loss = 0.00378258\n",
      "Iteration 195, loss = 0.00370694\n",
      "Iteration 196, loss = 0.00363096\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.47221987\n",
      "Iteration 2, loss = 0.44459268\n",
      "Iteration 3, loss = 0.41957243\n",
      "Iteration 4, loss = 0.39683954\n",
      "Iteration 5, loss = 0.37606038\n",
      "Iteration 6, loss = 0.35716056\n",
      "Iteration 7, loss = 0.33999541\n",
      "Iteration 8, loss = 0.32438154\n",
      "Iteration 9, loss = 0.31020697\n",
      "Iteration 10, loss = 0.29731129\n",
      "Iteration 11, loss = 0.28539320\n",
      "Iteration 12, loss = 0.27430226\n",
      "Iteration 13, loss = 0.26387360\n",
      "Iteration 14, loss = 0.25397779\n",
      "Iteration 15, loss = 0.24449357\n",
      "Iteration 16, loss = 0.23542331\n",
      "Iteration 17, loss = 0.22670164\n",
      "Iteration 18, loss = 0.21835410\n",
      "Iteration 19, loss = 0.21026710\n",
      "Iteration 20, loss = 0.20250441\n",
      "Iteration 21, loss = 0.19506460\n",
      "Iteration 22, loss = 0.18776576\n",
      "Iteration 23, loss = 0.18058848\n",
      "Iteration 24, loss = 0.17354452\n",
      "Iteration 25, loss = 0.16661335\n",
      "Iteration 26, loss = 0.15983560\n",
      "Iteration 27, loss = 0.15320347\n",
      "Iteration 28, loss = 0.14673035\n",
      "Iteration 29, loss = 0.14049374\n",
      "Iteration 30, loss = 0.13450540\n",
      "Iteration 31, loss = 0.12879133\n",
      "Iteration 32, loss = 0.12338440\n",
      "Iteration 33, loss = 0.11820979\n",
      "Iteration 34, loss = 0.11321669\n",
      "Iteration 35, loss = 0.10840444\n",
      "Iteration 36, loss = 0.10375667\n",
      "Iteration 37, loss = 0.09926261\n",
      "Iteration 38, loss = 0.09495499\n",
      "Iteration 39, loss = 0.09085395\n",
      "Iteration 40, loss = 0.08690820\n",
      "Iteration 41, loss = 0.08312364\n",
      "Iteration 42, loss = 0.07952051\n",
      "Iteration 43, loss = 0.07613717\n",
      "Iteration 44, loss = 0.07293465\n",
      "Iteration 45, loss = 0.06993991\n",
      "Iteration 46, loss = 0.06723385\n",
      "Iteration 47, loss = 0.06473551\n",
      "Iteration 48, loss = 0.06240239\n",
      "Iteration 49, loss = 0.06023765\n",
      "Iteration 50, loss = 0.05819409\n",
      "Iteration 51, loss = 0.05628166\n",
      "Iteration 52, loss = 0.05449250\n",
      "Iteration 53, loss = 0.05283358\n",
      "Iteration 54, loss = 0.05131866\n",
      "Iteration 55, loss = 0.04995528\n",
      "Iteration 56, loss = 0.04871843\n",
      "Iteration 57, loss = 0.04757420\n",
      "Iteration 58, loss = 0.04650381\n",
      "Iteration 59, loss = 0.04550387\n",
      "Iteration 60, loss = 0.04460255\n",
      "Iteration 61, loss = 0.04377364\n",
      "Iteration 62, loss = 0.04298921\n",
      "Iteration 63, loss = 0.04225631\n",
      "Iteration 64, loss = 0.04157184\n",
      "Iteration 65, loss = 0.04094810\n",
      "Iteration 66, loss = 0.04036162\n",
      "Iteration 67, loss = 0.03982604\n",
      "Iteration 68, loss = 0.03929342\n",
      "Iteration 69, loss = 0.03876883\n",
      "Iteration 70, loss = 0.03825506\n",
      "Iteration 71, loss = 0.03773998\n",
      "Iteration 72, loss = 0.03723390\n",
      "Iteration 73, loss = 0.03671506\n",
      "Iteration 74, loss = 0.03619556\n",
      "Iteration 75, loss = 0.03567806\n",
      "Iteration 76, loss = 0.03518451\n",
      "Iteration 77, loss = 0.03469310\n",
      "Iteration 78, loss = 0.03420875\n",
      "Iteration 79, loss = 0.03371787\n",
      "Iteration 80, loss = 0.03324799\n",
      "Iteration 81, loss = 0.03278165\n",
      "Iteration 82, loss = 0.03231343\n",
      "Iteration 83, loss = 0.03186212\n",
      "Iteration 84, loss = 0.03143373\n",
      "Iteration 85, loss = 0.03101201\n",
      "Iteration 86, loss = 0.03058951\n",
      "Iteration 87, loss = 0.03016232\n",
      "Iteration 88, loss = 0.02973124\n",
      "Iteration 89, loss = 0.02930398\n",
      "Iteration 90, loss = 0.02889320\n",
      "Iteration 91, loss = 0.02849053\n",
      "Iteration 92, loss = 0.02807597\n",
      "Iteration 93, loss = 0.02766147\n",
      "Iteration 94, loss = 0.02725899\n",
      "Iteration 95, loss = 0.02686545\n",
      "Iteration 96, loss = 0.02648699\n",
      "Iteration 97, loss = 0.02611596\n",
      "Iteration 98, loss = 0.02573970\n",
      "Iteration 99, loss = 0.02537002\n",
      "Iteration 100, loss = 0.02498967\n",
      "Iteration 101, loss = 0.02460687\n",
      "Iteration 102, loss = 0.02423758\n",
      "Iteration 103, loss = 0.02387729\n",
      "Iteration 104, loss = 0.02352700\n",
      "Iteration 105, loss = 0.02316070\n",
      "Iteration 106, loss = 0.02279632\n",
      "Iteration 107, loss = 0.02243444\n",
      "Iteration 108, loss = 0.02207971\n",
      "Iteration 109, loss = 0.02172714\n",
      "Iteration 110, loss = 0.02138419\n",
      "Iteration 111, loss = 0.02104909\n",
      "Iteration 112, loss = 0.02072169\n",
      "Iteration 113, loss = 0.02039624\n",
      "Iteration 114, loss = 0.02008088\n",
      "Iteration 115, loss = 0.01976909\n",
      "Iteration 116, loss = 0.01945775\n",
      "Iteration 117, loss = 0.01914595\n",
      "Iteration 118, loss = 0.01883742\n",
      "Iteration 119, loss = 0.01852763\n",
      "Iteration 120, loss = 0.01822413\n",
      "Iteration 121, loss = 0.01791768\n",
      "Iteration 122, loss = 0.01760944\n",
      "Iteration 123, loss = 0.01730546\n",
      "Iteration 124, loss = 0.01700753\n",
      "Iteration 125, loss = 0.01670855\n",
      "Iteration 126, loss = 0.01640089\n",
      "Iteration 127, loss = 0.01609061\n",
      "Iteration 128, loss = 0.01578083\n",
      "Iteration 129, loss = 0.01547665\n",
      "Iteration 130, loss = 0.01517918\n",
      "Iteration 131, loss = 0.01489314\n",
      "Iteration 132, loss = 0.01461726\n",
      "Iteration 133, loss = 0.01432874\n",
      "Iteration 134, loss = 0.01403852\n",
      "Iteration 135, loss = 0.01376295\n",
      "Iteration 136, loss = 0.01349179\n",
      "Iteration 137, loss = 0.01321423\n",
      "Iteration 138, loss = 0.01295200\n",
      "Iteration 139, loss = 0.01269505\n",
      "Iteration 140, loss = 0.01243198\n",
      "Iteration 141, loss = 0.01216896\n",
      "Iteration 142, loss = 0.01192472\n",
      "Iteration 143, loss = 0.01167622\n",
      "Iteration 144, loss = 0.01142844\n",
      "Iteration 145, loss = 0.01118528\n",
      "Iteration 146, loss = 0.01094728\n",
      "Iteration 147, loss = 0.01071611\n",
      "Iteration 148, loss = 0.01048380\n",
      "Iteration 149, loss = 0.01025461\n",
      "Iteration 150, loss = 0.01002990\n",
      "Iteration 151, loss = 0.00981164\n",
      "Iteration 152, loss = 0.00959224\n",
      "Iteration 153, loss = 0.00938225\n",
      "Iteration 154, loss = 0.00916972\n",
      "Iteration 155, loss = 0.00896577\n",
      "Iteration 156, loss = 0.00876477\n",
      "Iteration 157, loss = 0.00856824\n",
      "Iteration 158, loss = 0.00837823\n",
      "Iteration 159, loss = 0.00819577\n",
      "Iteration 160, loss = 0.00802127\n",
      "Iteration 161, loss = 0.00784755\n",
      "Iteration 162, loss = 0.00767474\n",
      "Iteration 163, loss = 0.00751079\n",
      "Iteration 164, loss = 0.00734193\n",
      "Iteration 165, loss = 0.00717759\n",
      "Iteration 166, loss = 0.00701706\n",
      "Iteration 167, loss = 0.00685819\n",
      "Iteration 168, loss = 0.00670546\n",
      "Iteration 169, loss = 0.00655955\n",
      "Iteration 170, loss = 0.00641311\n",
      "Iteration 171, loss = 0.00626884\n",
      "Iteration 172, loss = 0.00612727\n",
      "Iteration 173, loss = 0.00598987\n",
      "Iteration 174, loss = 0.00585887\n",
      "Iteration 175, loss = 0.00572662\n",
      "Iteration 176, loss = 0.00559767\n",
      "Iteration 177, loss = 0.00547069\n",
      "Iteration 178, loss = 0.00535357\n",
      "Iteration 179, loss = 0.00523663\n",
      "Iteration 180, loss = 0.00512047\n",
      "Iteration 181, loss = 0.00500822\n",
      "Iteration 182, loss = 0.00490061\n",
      "Iteration 183, loss = 0.00479172\n",
      "Iteration 184, loss = 0.00468853\n",
      "Iteration 185, loss = 0.00458775\n",
      "Iteration 186, loss = 0.00449048\n",
      "Iteration 187, loss = 0.00439242\n",
      "Iteration 188, loss = 0.00429875\n",
      "Iteration 189, loss = 0.00420707\n",
      "Iteration 190, loss = 0.00411823\n",
      "Iteration 191, loss = 0.00403266\n",
      "Iteration 192, loss = 0.00394735\n",
      "Iteration 193, loss = 0.00386289\n",
      "Iteration 194, loss = 0.00378258\n",
      "Iteration 195, loss = 0.00370694\n",
      "Iteration 196, loss = 0.00363096\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.47221987\n",
      "Iteration 2, loss = 0.44459268\n",
      "Iteration 3, loss = 0.41957243\n",
      "Iteration 4, loss = 0.39683954\n",
      "Iteration 5, loss = 0.37606038\n",
      "Iteration 6, loss = 0.35716056\n",
      "Iteration 7, loss = 0.33999541\n",
      "Iteration 8, loss = 0.32438154\n",
      "Iteration 9, loss = 0.31020697\n",
      "Iteration 10, loss = 0.29731129\n",
      "Iteration 11, loss = 0.28539320\n",
      "Iteration 12, loss = 0.27430226\n",
      "Iteration 13, loss = 0.26387360\n",
      "Iteration 14, loss = 0.25397779\n",
      "Iteration 15, loss = 0.24449357\n",
      "Iteration 16, loss = 0.23542331\n",
      "Iteration 17, loss = 0.22670164\n",
      "Iteration 18, loss = 0.21835410\n",
      "Iteration 19, loss = 0.21026710\n",
      "Iteration 20, loss = 0.20250441\n",
      "Iteration 21, loss = 0.19506460\n",
      "Iteration 22, loss = 0.18776576\n",
      "Iteration 23, loss = 0.18058848\n",
      "Iteration 24, loss = 0.17354452\n",
      "Iteration 25, loss = 0.16661335\n",
      "Iteration 26, loss = 0.15983560\n",
      "Iteration 27, loss = 0.15320347\n",
      "Iteration 28, loss = 0.14673035\n",
      "Iteration 29, loss = 0.14049374\n",
      "Iteration 30, loss = 0.13450540\n",
      "Iteration 31, loss = 0.12879133\n",
      "Iteration 32, loss = 0.12338440\n",
      "Iteration 33, loss = 0.11820979\n",
      "Iteration 34, loss = 0.11321669\n",
      "Iteration 35, loss = 0.10840444\n",
      "Iteration 36, loss = 0.10375667\n",
      "Iteration 37, loss = 0.09926261\n",
      "Iteration 38, loss = 0.09495499\n",
      "Iteration 39, loss = 0.09085395\n",
      "Iteration 40, loss = 0.08690820\n",
      "Iteration 41, loss = 0.08312364\n",
      "Iteration 42, loss = 0.07952051\n",
      "Iteration 43, loss = 0.07613717\n",
      "Iteration 44, loss = 0.07293465\n",
      "Iteration 45, loss = 0.06993991\n",
      "Iteration 46, loss = 0.06723385\n",
      "Iteration 47, loss = 0.06473551\n",
      "Iteration 48, loss = 0.06240239\n",
      "Iteration 49, loss = 0.06023765\n",
      "Iteration 50, loss = 0.05819409\n",
      "Iteration 51, loss = 0.05628166\n",
      "Iteration 52, loss = 0.05449250\n",
      "Iteration 53, loss = 0.05283358\n",
      "Iteration 54, loss = 0.05131866\n",
      "Iteration 55, loss = 0.04995528\n",
      "Iteration 56, loss = 0.04871843\n",
      "Iteration 57, loss = 0.04757420\n",
      "Iteration 58, loss = 0.04650381\n",
      "Iteration 59, loss = 0.04550387\n",
      "Iteration 60, loss = 0.04460255\n",
      "Iteration 61, loss = 0.04377364\n",
      "Iteration 62, loss = 0.04298921\n",
      "Iteration 63, loss = 0.04225631\n",
      "Iteration 64, loss = 0.04157184\n",
      "Iteration 65, loss = 0.04094810\n",
      "Iteration 66, loss = 0.04036162\n",
      "Iteration 67, loss = 0.03982604\n",
      "Iteration 68, loss = 0.03929342\n",
      "Iteration 69, loss = 0.03876883\n",
      "Iteration 70, loss = 0.03825506\n",
      "Iteration 71, loss = 0.03773998\n",
      "Iteration 72, loss = 0.03723390\n",
      "Iteration 73, loss = 0.03671506\n",
      "Iteration 74, loss = 0.03619556\n",
      "Iteration 75, loss = 0.03567806\n",
      "Iteration 76, loss = 0.03518451\n",
      "Iteration 77, loss = 0.03469310\n",
      "Iteration 78, loss = 0.03420875\n",
      "Iteration 79, loss = 0.03371787\n",
      "Iteration 80, loss = 0.03324799\n",
      "Iteration 81, loss = 0.03278165\n",
      "Iteration 82, loss = 0.03231343\n",
      "Iteration 83, loss = 0.03186212\n",
      "Iteration 84, loss = 0.03143373\n",
      "Iteration 85, loss = 0.03101201\n",
      "Iteration 86, loss = 0.03058951\n",
      "Iteration 87, loss = 0.03016232\n",
      "Iteration 88, loss = 0.02973124\n",
      "Iteration 89, loss = 0.02930398\n",
      "Iteration 90, loss = 0.02889320\n",
      "Iteration 91, loss = 0.02849053\n",
      "Iteration 92, loss = 0.02807597\n",
      "Iteration 93, loss = 0.02766147\n",
      "Iteration 94, loss = 0.02725899\n",
      "Iteration 95, loss = 0.02686545\n",
      "Iteration 96, loss = 0.02648699\n",
      "Iteration 97, loss = 0.02611596\n",
      "Iteration 98, loss = 0.02573970\n",
      "Iteration 99, loss = 0.02537002\n",
      "Iteration 100, loss = 0.02498967\n",
      "Iteration 101, loss = 0.02460687\n",
      "Iteration 102, loss = 0.02423758\n",
      "Iteration 103, loss = 0.02387729\n",
      "Iteration 104, loss = 0.02352700\n",
      "Iteration 105, loss = 0.02316070\n",
      "Iteration 106, loss = 0.02279632\n",
      "Iteration 107, loss = 0.02243444\n",
      "Iteration 108, loss = 0.02207971\n",
      "Iteration 109, loss = 0.02172714\n",
      "Iteration 110, loss = 0.02138419\n",
      "Iteration 111, loss = 0.02104909\n",
      "Iteration 112, loss = 0.02072169\n",
      "Iteration 113, loss = 0.02039624\n",
      "Iteration 114, loss = 0.02008088\n",
      "Iteration 115, loss = 0.01976909\n",
      "Iteration 116, loss = 0.01945775\n",
      "Iteration 117, loss = 0.01914595\n",
      "Iteration 118, loss = 0.01883742\n",
      "Iteration 119, loss = 0.01852763\n",
      "Iteration 120, loss = 0.01822413\n",
      "Iteration 121, loss = 0.01791768\n",
      "Iteration 122, loss = 0.01760944\n",
      "Iteration 123, loss = 0.01730546\n",
      "Iteration 124, loss = 0.01700753\n",
      "Iteration 125, loss = 0.01670855\n",
      "Iteration 126, loss = 0.01640089\n",
      "Iteration 127, loss = 0.01609061\n",
      "Iteration 128, loss = 0.01578083\n",
      "Iteration 129, loss = 0.01547665\n",
      "Iteration 130, loss = 0.01517918\n",
      "Iteration 131, loss = 0.01489314\n",
      "Iteration 132, loss = 0.01461726\n",
      "Iteration 133, loss = 0.01432874\n",
      "Iteration 134, loss = 0.01403852\n",
      "Iteration 135, loss = 0.01376295\n",
      "Iteration 136, loss = 0.01349179\n",
      "Iteration 137, loss = 0.01321423\n",
      "Iteration 138, loss = 0.01295200\n",
      "Iteration 139, loss = 0.01269505\n",
      "Iteration 140, loss = 0.01243198\n",
      "Iteration 141, loss = 0.01216896\n",
      "Iteration 142, loss = 0.01192472\n",
      "Iteration 143, loss = 0.01167622\n",
      "Iteration 144, loss = 0.01142844\n",
      "Iteration 145, loss = 0.01118528\n",
      "Iteration 146, loss = 0.01094728\n",
      "Iteration 147, loss = 0.01071611\n",
      "Iteration 148, loss = 0.01048380\n",
      "Iteration 149, loss = 0.01025461\n",
      "Iteration 150, loss = 0.01002990\n",
      "Iteration 151, loss = 0.00981164\n",
      "Iteration 152, loss = 0.00959224\n",
      "Iteration 153, loss = 0.00938225\n",
      "Iteration 154, loss = 0.00916972\n",
      "Iteration 155, loss = 0.00896577\n",
      "Iteration 156, loss = 0.00876477\n",
      "Iteration 157, loss = 0.00856824\n",
      "Iteration 158, loss = 0.00837823\n",
      "Iteration 159, loss = 0.00819577\n",
      "Iteration 160, loss = 0.00802127\n",
      "Iteration 161, loss = 0.00784755\n",
      "Iteration 162, loss = 0.00767474\n",
      "Iteration 163, loss = 0.00751079\n",
      "Iteration 164, loss = 0.00734193\n",
      "Iteration 165, loss = 0.00717759\n",
      "Iteration 166, loss = 0.00701706\n",
      "Iteration 167, loss = 0.00685819\n",
      "Iteration 168, loss = 0.00670546\n",
      "Iteration 169, loss = 0.00655955\n",
      "Iteration 170, loss = 0.00641311\n",
      "Iteration 171, loss = 0.00626884\n",
      "Iteration 172, loss = 0.00612727\n",
      "Iteration 173, loss = 0.00598987\n",
      "Iteration 174, loss = 0.00585887\n",
      "Iteration 175, loss = 0.00572662\n",
      "Iteration 176, loss = 0.00559767\n",
      "Iteration 177, loss = 0.00547069\n",
      "Iteration 178, loss = 0.00535357\n",
      "Iteration 179, loss = 0.00523663\n",
      "Iteration 180, loss = 0.00512047\n",
      "Iteration 181, loss = 0.00500822\n",
      "Iteration 182, loss = 0.00490061\n",
      "Iteration 183, loss = 0.00479172\n",
      "Iteration 184, loss = 0.00468853\n",
      "Iteration 185, loss = 0.00458775\n",
      "Iteration 186, loss = 0.00449048\n",
      "Iteration 187, loss = 0.00439242\n",
      "Iteration 188, loss = 0.00429875\n",
      "Iteration 189, loss = 0.00420707\n",
      "Iteration 190, loss = 0.00411823\n",
      "Iteration 191, loss = 0.00403266\n",
      "Iteration 192, loss = 0.00394735\n",
      "Iteration 193, loss = 0.00386289\n",
      "Iteration 194, loss = 0.00378258\n",
      "Iteration 195, loss = 0.00370694\n",
      "Iteration 196, loss = 0.00363096\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46495664\n",
      "Iteration 2, loss = 0.43522654\n",
      "Iteration 3, loss = 0.40888532\n",
      "Iteration 4, loss = 0.38554501\n",
      "Iteration 5, loss = 0.36486131\n",
      "Iteration 6, loss = 0.34664257\n",
      "Iteration 7, loss = 0.33050610\n",
      "Iteration 8, loss = 0.31609421\n",
      "Iteration 9, loss = 0.30309730\n",
      "Iteration 10, loss = 0.29125037\n",
      "Iteration 11, loss = 0.28026605\n",
      "Iteration 12, loss = 0.26990251\n",
      "Iteration 13, loss = 0.25990548\n",
      "Iteration 14, loss = 0.25007327\n",
      "Iteration 15, loss = 0.24030976\n",
      "Iteration 16, loss = 0.23058219\n",
      "Iteration 17, loss = 0.22087961\n",
      "Iteration 18, loss = 0.21122345\n",
      "Iteration 19, loss = 0.20168618\n",
      "Iteration 20, loss = 0.19233673\n",
      "Iteration 21, loss = 0.18320017\n",
      "Iteration 22, loss = 0.17430963\n",
      "Iteration 23, loss = 0.16568005\n",
      "Iteration 24, loss = 0.15735836\n",
      "Iteration 25, loss = 0.14930557\n",
      "Iteration 26, loss = 0.14152657\n",
      "Iteration 27, loss = 0.13402710\n",
      "Iteration 28, loss = 0.12683998\n",
      "Iteration 29, loss = 0.12001091\n",
      "Iteration 30, loss = 0.11353747\n",
      "Iteration 31, loss = 0.10750413\n",
      "Iteration 32, loss = 0.10194634\n",
      "Iteration 33, loss = 0.09687407\n",
      "Iteration 34, loss = 0.09228832\n",
      "Iteration 35, loss = 0.08812707\n",
      "Iteration 36, loss = 0.08434049\n",
      "Iteration 37, loss = 0.08089937\n",
      "Iteration 38, loss = 0.07776137\n",
      "Iteration 39, loss = 0.07491306\n",
      "Iteration 40, loss = 0.07230938\n",
      "Iteration 41, loss = 0.06989782\n",
      "Iteration 42, loss = 0.06768287\n",
      "Iteration 43, loss = 0.06561237\n",
      "Iteration 44, loss = 0.06366848\n",
      "Iteration 45, loss = 0.06183398\n",
      "Iteration 46, loss = 0.06006560\n",
      "Iteration 47, loss = 0.05834289\n",
      "Iteration 48, loss = 0.05665716\n",
      "Iteration 49, loss = 0.05504868\n",
      "Iteration 50, loss = 0.05354995\n",
      "Iteration 51, loss = 0.05216592\n",
      "Iteration 52, loss = 0.05090204\n",
      "Iteration 53, loss = 0.04972719\n",
      "Iteration 54, loss = 0.04863223\n",
      "Iteration 55, loss = 0.04761492\n",
      "Iteration 56, loss = 0.04665654\n",
      "Iteration 57, loss = 0.04576619\n",
      "Iteration 58, loss = 0.04491372\n",
      "Iteration 59, loss = 0.04411967\n",
      "Iteration 60, loss = 0.04343212\n",
      "Iteration 61, loss = 0.04277114\n",
      "Iteration 62, loss = 0.04212532\n",
      "Iteration 63, loss = 0.04149847\n",
      "Iteration 64, loss = 0.04089106\n",
      "Iteration 65, loss = 0.04030638\n",
      "Iteration 66, loss = 0.03973943\n",
      "Iteration 67, loss = 0.03919900\n",
      "Iteration 68, loss = 0.03866354\n",
      "Iteration 69, loss = 0.03813337\n",
      "Iteration 70, loss = 0.03760453\n",
      "Iteration 71, loss = 0.03709237\n",
      "Iteration 72, loss = 0.03657853\n",
      "Iteration 73, loss = 0.03606973\n",
      "Iteration 74, loss = 0.03557501\n",
      "Iteration 75, loss = 0.03508709\n",
      "Iteration 76, loss = 0.03460741\n",
      "Iteration 77, loss = 0.03413509\n",
      "Iteration 78, loss = 0.03368026\n",
      "Iteration 79, loss = 0.03323481\n",
      "Iteration 80, loss = 0.03279006\n",
      "Iteration 81, loss = 0.03234175\n",
      "Iteration 82, loss = 0.03192281\n",
      "Iteration 83, loss = 0.03150923\n",
      "Iteration 84, loss = 0.03109952\n",
      "Iteration 85, loss = 0.03069260\n",
      "Iteration 86, loss = 0.03031592\n",
      "Iteration 87, loss = 0.02992767\n",
      "Iteration 88, loss = 0.02953464\n",
      "Iteration 89, loss = 0.02914178\n",
      "Iteration 90, loss = 0.02874541\n",
      "Iteration 91, loss = 0.02834287\n",
      "Iteration 92, loss = 0.02796911\n",
      "Iteration 93, loss = 0.02760075\n",
      "Iteration 94, loss = 0.02723770\n",
      "Iteration 95, loss = 0.02687790\n",
      "Iteration 96, loss = 0.02651567\n",
      "Iteration 97, loss = 0.02615088\n",
      "Iteration 98, loss = 0.02579067\n",
      "Iteration 99, loss = 0.02544590\n",
      "Iteration 100, loss = 0.02512699\n",
      "Iteration 101, loss = 0.02481107\n",
      "Iteration 102, loss = 0.02448836\n",
      "Iteration 103, loss = 0.02416442\n",
      "Iteration 104, loss = 0.02384199\n",
      "Iteration 105, loss = 0.02353000\n",
      "Iteration 106, loss = 0.02322307\n",
      "Iteration 107, loss = 0.02291201\n",
      "Iteration 108, loss = 0.02260559\n",
      "Iteration 109, loss = 0.02230941\n",
      "Iteration 110, loss = 0.02201071\n",
      "Iteration 111, loss = 0.02170505\n",
      "Iteration 112, loss = 0.02141720\n",
      "Iteration 113, loss = 0.02113662\n",
      "Iteration 114, loss = 0.02086296\n",
      "Iteration 115, loss = 0.02058284\n",
      "Iteration 116, loss = 0.02030747\n",
      "Iteration 117, loss = 0.02003637\n",
      "Iteration 118, loss = 0.01977123\n",
      "Iteration 119, loss = 0.01952048\n",
      "Iteration 120, loss = 0.01927983\n",
      "Iteration 121, loss = 0.01904458\n",
      "Iteration 122, loss = 0.01881142\n",
      "Iteration 123, loss = 0.01857410\n",
      "Iteration 124, loss = 0.01833125\n",
      "Iteration 125, loss = 0.01809117\n",
      "Iteration 126, loss = 0.01785962\n",
      "Iteration 127, loss = 0.01762277\n",
      "Iteration 128, loss = 0.01738376\n",
      "Iteration 129, loss = 0.01714052\n",
      "Iteration 130, loss = 0.01690392\n",
      "Iteration 131, loss = 0.01666129\n",
      "Iteration 132, loss = 0.01641535\n",
      "Iteration 133, loss = 0.01617333\n",
      "Iteration 134, loss = 0.01594242\n",
      "Iteration 135, loss = 0.01570622\n",
      "Iteration 136, loss = 0.01547109\n",
      "Iteration 137, loss = 0.01524749\n",
      "Iteration 138, loss = 0.01502865\n",
      "Iteration 139, loss = 0.01481254\n",
      "Iteration 140, loss = 0.01459540\n",
      "Iteration 141, loss = 0.01437559\n",
      "Iteration 142, loss = 0.01417039\n",
      "Iteration 143, loss = 0.01396291\n",
      "Iteration 144, loss = 0.01374944\n",
      "Iteration 145, loss = 0.01354213\n",
      "Iteration 146, loss = 0.01334173\n",
      "Iteration 147, loss = 0.01313856\n",
      "Iteration 148, loss = 0.01292340\n",
      "Iteration 149, loss = 0.01271304\n",
      "Iteration 150, loss = 0.01249962\n",
      "Iteration 151, loss = 0.01229265\n",
      "Iteration 152, loss = 0.01208258\n",
      "Iteration 153, loss = 0.01187140\n",
      "Iteration 154, loss = 0.01167033\n",
      "Iteration 155, loss = 0.01146825\n",
      "Iteration 156, loss = 0.01126674\n",
      "Iteration 157, loss = 0.01106229\n",
      "Iteration 158, loss = 0.01086325\n",
      "Iteration 159, loss = 0.01066960\n",
      "Iteration 160, loss = 0.01047790\n",
      "Iteration 161, loss = 0.01028408\n",
      "Iteration 162, loss = 0.01009897\n",
      "Iteration 163, loss = 0.00992908\n",
      "Iteration 164, loss = 0.00976084\n",
      "Iteration 165, loss = 0.00959601\n",
      "Iteration 166, loss = 0.00943281\n",
      "Iteration 167, loss = 0.00927010\n",
      "Iteration 168, loss = 0.00910884\n",
      "Iteration 169, loss = 0.00895047\n",
      "Iteration 170, loss = 0.00879585\n",
      "Iteration 171, loss = 0.00864554\n",
      "Iteration 172, loss = 0.00849471\n",
      "Iteration 173, loss = 0.00834699\n",
      "Iteration 174, loss = 0.00819748\n",
      "Iteration 175, loss = 0.00804673\n",
      "Iteration 176, loss = 0.00789970\n",
      "Iteration 177, loss = 0.00775977\n",
      "Iteration 178, loss = 0.00762303\n",
      "Iteration 179, loss = 0.00748896\n",
      "Iteration 180, loss = 0.00735771\n",
      "Iteration 181, loss = 0.00722473\n",
      "Iteration 182, loss = 0.00709105\n",
      "Iteration 183, loss = 0.00696114\n",
      "Iteration 184, loss = 0.00683561\n",
      "Iteration 185, loss = 0.00671362\n",
      "Iteration 186, loss = 0.00659269\n",
      "Iteration 187, loss = 0.00647790\n",
      "Iteration 188, loss = 0.00636757\n",
      "Iteration 189, loss = 0.00625642\n",
      "Iteration 190, loss = 0.00614538\n",
      "Iteration 191, loss = 0.00603823\n",
      "Iteration 192, loss = 0.00593711\n",
      "Iteration 193, loss = 0.00583539\n",
      "Iteration 194, loss = 0.00573319\n",
      "Iteration 195, loss = 0.00563946\n",
      "Iteration 196, loss = 0.00554616\n",
      "Iteration 197, loss = 0.00545409\n",
      "Iteration 198, loss = 0.00536598\n",
      "Iteration 199, loss = 0.00527932\n",
      "Iteration 200, loss = 0.00519336\n",
      "Iteration 201, loss = 0.00510904\n",
      "Iteration 202, loss = 0.00502640\n",
      "Iteration 203, loss = 0.00494489\n",
      "Iteration 204, loss = 0.00486610\n",
      "Iteration 205, loss = 0.00478773\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46495664\n",
      "Iteration 2, loss = 0.43522654\n",
      "Iteration 3, loss = 0.40888532\n",
      "Iteration 4, loss = 0.38554501\n",
      "Iteration 5, loss = 0.36486131\n",
      "Iteration 6, loss = 0.34664257\n",
      "Iteration 7, loss = 0.33050610\n",
      "Iteration 8, loss = 0.31609421\n",
      "Iteration 9, loss = 0.30309730\n",
      "Iteration 10, loss = 0.29125037\n",
      "Iteration 11, loss = 0.28026605\n",
      "Iteration 12, loss = 0.26990251\n",
      "Iteration 13, loss = 0.25990548\n",
      "Iteration 14, loss = 0.25007327\n",
      "Iteration 15, loss = 0.24030976\n",
      "Iteration 16, loss = 0.23058219\n",
      "Iteration 17, loss = 0.22087961\n",
      "Iteration 18, loss = 0.21122345\n",
      "Iteration 19, loss = 0.20168618\n",
      "Iteration 20, loss = 0.19233673\n",
      "Iteration 21, loss = 0.18320017\n",
      "Iteration 22, loss = 0.17430963\n",
      "Iteration 23, loss = 0.16568005\n",
      "Iteration 24, loss = 0.15735836\n",
      "Iteration 25, loss = 0.14930557\n",
      "Iteration 26, loss = 0.14152657\n",
      "Iteration 27, loss = 0.13402710\n",
      "Iteration 28, loss = 0.12683998\n",
      "Iteration 29, loss = 0.12001091\n",
      "Iteration 30, loss = 0.11353747\n",
      "Iteration 31, loss = 0.10750413\n",
      "Iteration 32, loss = 0.10194634\n",
      "Iteration 33, loss = 0.09687407\n",
      "Iteration 34, loss = 0.09228832\n",
      "Iteration 35, loss = 0.08812707\n",
      "Iteration 36, loss = 0.08434049\n",
      "Iteration 37, loss = 0.08089937\n",
      "Iteration 38, loss = 0.07776137\n",
      "Iteration 39, loss = 0.07491306\n",
      "Iteration 40, loss = 0.07230938\n",
      "Iteration 41, loss = 0.06989782\n",
      "Iteration 42, loss = 0.06768287\n",
      "Iteration 43, loss = 0.06561237\n",
      "Iteration 44, loss = 0.06366848\n",
      "Iteration 45, loss = 0.06183398\n",
      "Iteration 46, loss = 0.06006560\n",
      "Iteration 47, loss = 0.05834289\n",
      "Iteration 48, loss = 0.05665716\n",
      "Iteration 49, loss = 0.05504868\n",
      "Iteration 50, loss = 0.05354995\n",
      "Iteration 51, loss = 0.05216592\n",
      "Iteration 52, loss = 0.05090204\n",
      "Iteration 53, loss = 0.04972719\n",
      "Iteration 54, loss = 0.04863223\n",
      "Iteration 55, loss = 0.04761492\n",
      "Iteration 56, loss = 0.04665654\n",
      "Iteration 57, loss = 0.04576619\n",
      "Iteration 58, loss = 0.04491372\n",
      "Iteration 59, loss = 0.04411967\n",
      "Iteration 60, loss = 0.04343212\n",
      "Iteration 61, loss = 0.04277114\n",
      "Iteration 62, loss = 0.04212532\n",
      "Iteration 63, loss = 0.04149847\n",
      "Iteration 64, loss = 0.04089106\n",
      "Iteration 65, loss = 0.04030638\n",
      "Iteration 66, loss = 0.03973943\n",
      "Iteration 67, loss = 0.03919900\n",
      "Iteration 68, loss = 0.03866354\n",
      "Iteration 69, loss = 0.03813337\n",
      "Iteration 70, loss = 0.03760453\n",
      "Iteration 71, loss = 0.03709237\n",
      "Iteration 72, loss = 0.03657853\n",
      "Iteration 73, loss = 0.03606973\n",
      "Iteration 74, loss = 0.03557501\n",
      "Iteration 75, loss = 0.03508709\n",
      "Iteration 76, loss = 0.03460741\n",
      "Iteration 77, loss = 0.03413509\n",
      "Iteration 78, loss = 0.03368026\n",
      "Iteration 79, loss = 0.03323481\n",
      "Iteration 80, loss = 0.03279006\n",
      "Iteration 81, loss = 0.03234175\n",
      "Iteration 82, loss = 0.03192281\n",
      "Iteration 83, loss = 0.03150923\n",
      "Iteration 84, loss = 0.03109952\n",
      "Iteration 85, loss = 0.03069260\n",
      "Iteration 86, loss = 0.03031592\n",
      "Iteration 87, loss = 0.02992767\n",
      "Iteration 88, loss = 0.02953464\n",
      "Iteration 89, loss = 0.02914178\n",
      "Iteration 90, loss = 0.02874541\n",
      "Iteration 91, loss = 0.02834287\n",
      "Iteration 92, loss = 0.02796911\n",
      "Iteration 93, loss = 0.02760075\n",
      "Iteration 94, loss = 0.02723770\n",
      "Iteration 95, loss = 0.02687790\n",
      "Iteration 96, loss = 0.02651567\n",
      "Iteration 97, loss = 0.02615088\n",
      "Iteration 98, loss = 0.02579067\n",
      "Iteration 99, loss = 0.02544590\n",
      "Iteration 100, loss = 0.02512699\n",
      "Iteration 101, loss = 0.02481107\n",
      "Iteration 102, loss = 0.02448836\n",
      "Iteration 103, loss = 0.02416442\n",
      "Iteration 104, loss = 0.02384199\n",
      "Iteration 105, loss = 0.02353000\n",
      "Iteration 106, loss = 0.02322307\n",
      "Iteration 107, loss = 0.02291201\n",
      "Iteration 108, loss = 0.02260559\n",
      "Iteration 109, loss = 0.02230941\n",
      "Iteration 110, loss = 0.02201071\n",
      "Iteration 111, loss = 0.02170505\n",
      "Iteration 112, loss = 0.02141720\n",
      "Iteration 113, loss = 0.02113662\n",
      "Iteration 114, loss = 0.02086296\n",
      "Iteration 115, loss = 0.02058284\n",
      "Iteration 116, loss = 0.02030747\n",
      "Iteration 117, loss = 0.02003637\n",
      "Iteration 118, loss = 0.01977123\n",
      "Iteration 119, loss = 0.01952048\n",
      "Iteration 120, loss = 0.01927983\n",
      "Iteration 121, loss = 0.01904458\n",
      "Iteration 122, loss = 0.01881142\n",
      "Iteration 123, loss = 0.01857410\n",
      "Iteration 124, loss = 0.01833125\n",
      "Iteration 125, loss = 0.01809117\n",
      "Iteration 126, loss = 0.01785962\n",
      "Iteration 127, loss = 0.01762277\n",
      "Iteration 128, loss = 0.01738376\n",
      "Iteration 129, loss = 0.01714052\n",
      "Iteration 130, loss = 0.01690392\n",
      "Iteration 131, loss = 0.01666129\n",
      "Iteration 132, loss = 0.01641535\n",
      "Iteration 133, loss = 0.01617333\n",
      "Iteration 134, loss = 0.01594242\n",
      "Iteration 135, loss = 0.01570622\n",
      "Iteration 136, loss = 0.01547109\n",
      "Iteration 137, loss = 0.01524749\n",
      "Iteration 138, loss = 0.01502865\n",
      "Iteration 139, loss = 0.01481254\n",
      "Iteration 140, loss = 0.01459540\n",
      "Iteration 141, loss = 0.01437559\n",
      "Iteration 142, loss = 0.01417039\n",
      "Iteration 143, loss = 0.01396291\n",
      "Iteration 144, loss = 0.01374944\n",
      "Iteration 145, loss = 0.01354213\n",
      "Iteration 146, loss = 0.01334173\n",
      "Iteration 147, loss = 0.01313856\n",
      "Iteration 148, loss = 0.01292340\n",
      "Iteration 149, loss = 0.01271304\n",
      "Iteration 150, loss = 0.01249962\n",
      "Iteration 151, loss = 0.01229265\n",
      "Iteration 152, loss = 0.01208258\n",
      "Iteration 153, loss = 0.01187140\n",
      "Iteration 154, loss = 0.01167033\n",
      "Iteration 155, loss = 0.01146825\n",
      "Iteration 156, loss = 0.01126674\n",
      "Iteration 157, loss = 0.01106229\n",
      "Iteration 158, loss = 0.01086325\n",
      "Iteration 159, loss = 0.01066960\n",
      "Iteration 160, loss = 0.01047790\n",
      "Iteration 161, loss = 0.01028408\n",
      "Iteration 162, loss = 0.01009897\n",
      "Iteration 163, loss = 0.00992908\n",
      "Iteration 164, loss = 0.00976084\n",
      "Iteration 165, loss = 0.00959601\n",
      "Iteration 166, loss = 0.00943281\n",
      "Iteration 167, loss = 0.00927010\n",
      "Iteration 168, loss = 0.00910884\n",
      "Iteration 169, loss = 0.00895047\n",
      "Iteration 170, loss = 0.00879585\n",
      "Iteration 171, loss = 0.00864554\n",
      "Iteration 172, loss = 0.00849471\n",
      "Iteration 173, loss = 0.00834699\n",
      "Iteration 174, loss = 0.00819748\n",
      "Iteration 175, loss = 0.00804673\n",
      "Iteration 176, loss = 0.00789970\n",
      "Iteration 177, loss = 0.00775977\n",
      "Iteration 178, loss = 0.00762303\n",
      "Iteration 179, loss = 0.00748896\n",
      "Iteration 180, loss = 0.00735771\n",
      "Iteration 181, loss = 0.00722473\n",
      "Iteration 182, loss = 0.00709105\n",
      "Iteration 183, loss = 0.00696114\n",
      "Iteration 184, loss = 0.00683561\n",
      "Iteration 185, loss = 0.00671362\n",
      "Iteration 186, loss = 0.00659269\n",
      "Iteration 187, loss = 0.00647790\n",
      "Iteration 188, loss = 0.00636757\n",
      "Iteration 189, loss = 0.00625642\n",
      "Iteration 190, loss = 0.00614538\n",
      "Iteration 191, loss = 0.00603823\n",
      "Iteration 192, loss = 0.00593711\n",
      "Iteration 193, loss = 0.00583539\n",
      "Iteration 194, loss = 0.00573319\n",
      "Iteration 195, loss = 0.00563946\n",
      "Iteration 196, loss = 0.00554616\n",
      "Iteration 197, loss = 0.00545409\n",
      "Iteration 198, loss = 0.00536598\n",
      "Iteration 199, loss = 0.00527932\n",
      "Iteration 200, loss = 0.00519336\n",
      "Iteration 201, loss = 0.00510904\n",
      "Iteration 202, loss = 0.00502640\n",
      "Iteration 203, loss = 0.00494489\n",
      "Iteration 204, loss = 0.00486610\n",
      "Iteration 205, loss = 0.00478773\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46495664\n",
      "Iteration 2, loss = 0.43522654\n",
      "Iteration 3, loss = 0.40888532\n",
      "Iteration 4, loss = 0.38554501\n",
      "Iteration 5, loss = 0.36486131\n",
      "Iteration 6, loss = 0.34664257\n",
      "Iteration 7, loss = 0.33050610\n",
      "Iteration 8, loss = 0.31609421\n",
      "Iteration 9, loss = 0.30309730\n",
      "Iteration 10, loss = 0.29125037\n",
      "Iteration 11, loss = 0.28026605\n",
      "Iteration 12, loss = 0.26990251\n",
      "Iteration 13, loss = 0.25990548\n",
      "Iteration 14, loss = 0.25007327\n",
      "Iteration 15, loss = 0.24030976\n",
      "Iteration 16, loss = 0.23058219\n",
      "Iteration 17, loss = 0.22087961\n",
      "Iteration 18, loss = 0.21122345\n",
      "Iteration 19, loss = 0.20168618\n",
      "Iteration 20, loss = 0.19233673\n",
      "Iteration 21, loss = 0.18320017\n",
      "Iteration 22, loss = 0.17430963\n",
      "Iteration 23, loss = 0.16568005\n",
      "Iteration 24, loss = 0.15735836\n",
      "Iteration 25, loss = 0.14930557\n",
      "Iteration 26, loss = 0.14152657\n",
      "Iteration 27, loss = 0.13402710\n",
      "Iteration 28, loss = 0.12683998\n",
      "Iteration 29, loss = 0.12001091\n",
      "Iteration 30, loss = 0.11353747\n",
      "Iteration 31, loss = 0.10750413\n",
      "Iteration 32, loss = 0.10194634\n",
      "Iteration 33, loss = 0.09687407\n",
      "Iteration 34, loss = 0.09228832\n",
      "Iteration 35, loss = 0.08812707\n",
      "Iteration 36, loss = 0.08434049\n",
      "Iteration 37, loss = 0.08089937\n",
      "Iteration 38, loss = 0.07776137\n",
      "Iteration 39, loss = 0.07491306\n",
      "Iteration 40, loss = 0.07230938\n",
      "Iteration 41, loss = 0.06989782\n",
      "Iteration 42, loss = 0.06768287\n",
      "Iteration 43, loss = 0.06561237\n",
      "Iteration 44, loss = 0.06366848\n",
      "Iteration 45, loss = 0.06183398\n",
      "Iteration 46, loss = 0.06006560\n",
      "Iteration 47, loss = 0.05834289\n",
      "Iteration 48, loss = 0.05665716\n",
      "Iteration 49, loss = 0.05504868\n",
      "Iteration 50, loss = 0.05354995\n",
      "Iteration 51, loss = 0.05216592\n",
      "Iteration 52, loss = 0.05090204\n",
      "Iteration 53, loss = 0.04972719\n",
      "Iteration 54, loss = 0.04863223\n",
      "Iteration 55, loss = 0.04761492\n",
      "Iteration 56, loss = 0.04665654\n",
      "Iteration 57, loss = 0.04576619\n",
      "Iteration 58, loss = 0.04491372\n",
      "Iteration 59, loss = 0.04411967\n",
      "Iteration 60, loss = 0.04343212\n",
      "Iteration 61, loss = 0.04277114\n",
      "Iteration 62, loss = 0.04212532\n",
      "Iteration 63, loss = 0.04149847\n",
      "Iteration 64, loss = 0.04089106\n",
      "Iteration 65, loss = 0.04030638\n",
      "Iteration 66, loss = 0.03973943\n",
      "Iteration 67, loss = 0.03919900\n",
      "Iteration 68, loss = 0.03866354\n",
      "Iteration 69, loss = 0.03813337\n",
      "Iteration 70, loss = 0.03760453\n",
      "Iteration 71, loss = 0.03709237\n",
      "Iteration 72, loss = 0.03657853\n",
      "Iteration 73, loss = 0.03606973\n",
      "Iteration 74, loss = 0.03557501\n",
      "Iteration 75, loss = 0.03508709\n",
      "Iteration 76, loss = 0.03460741\n",
      "Iteration 77, loss = 0.03413509\n",
      "Iteration 78, loss = 0.03368026\n",
      "Iteration 79, loss = 0.03323481\n",
      "Iteration 80, loss = 0.03279006\n",
      "Iteration 81, loss = 0.03234175\n",
      "Iteration 82, loss = 0.03192281\n",
      "Iteration 83, loss = 0.03150923\n",
      "Iteration 84, loss = 0.03109952\n",
      "Iteration 85, loss = 0.03069260\n",
      "Iteration 86, loss = 0.03031592\n",
      "Iteration 87, loss = 0.02992767\n",
      "Iteration 88, loss = 0.02953464\n",
      "Iteration 89, loss = 0.02914178\n",
      "Iteration 90, loss = 0.02874541\n",
      "Iteration 91, loss = 0.02834287\n",
      "Iteration 92, loss = 0.02796911\n",
      "Iteration 93, loss = 0.02760075\n",
      "Iteration 94, loss = 0.02723770\n",
      "Iteration 95, loss = 0.02687790\n",
      "Iteration 96, loss = 0.02651567\n",
      "Iteration 97, loss = 0.02615088\n",
      "Iteration 98, loss = 0.02579067\n",
      "Iteration 99, loss = 0.02544590\n",
      "Iteration 100, loss = 0.02512699\n",
      "Iteration 101, loss = 0.02481107\n",
      "Iteration 102, loss = 0.02448836\n",
      "Iteration 103, loss = 0.02416442\n",
      "Iteration 104, loss = 0.02384199\n",
      "Iteration 105, loss = 0.02353000\n",
      "Iteration 106, loss = 0.02322307\n",
      "Iteration 107, loss = 0.02291201\n",
      "Iteration 108, loss = 0.02260559\n",
      "Iteration 109, loss = 0.02230941\n",
      "Iteration 110, loss = 0.02201071\n",
      "Iteration 111, loss = 0.02170505\n",
      "Iteration 112, loss = 0.02141720\n",
      "Iteration 113, loss = 0.02113662\n",
      "Iteration 114, loss = 0.02086296\n",
      "Iteration 115, loss = 0.02058284\n",
      "Iteration 116, loss = 0.02030747\n",
      "Iteration 117, loss = 0.02003637\n",
      "Iteration 118, loss = 0.01977123\n",
      "Iteration 119, loss = 0.01952048\n",
      "Iteration 120, loss = 0.01927983\n",
      "Iteration 121, loss = 0.01904458\n",
      "Iteration 122, loss = 0.01881142\n",
      "Iteration 123, loss = 0.01857410\n",
      "Iteration 124, loss = 0.01833125\n",
      "Iteration 125, loss = 0.01809117\n",
      "Iteration 126, loss = 0.01785962\n",
      "Iteration 127, loss = 0.01762277\n",
      "Iteration 128, loss = 0.01738376\n",
      "Iteration 129, loss = 0.01714052\n",
      "Iteration 130, loss = 0.01690392\n",
      "Iteration 131, loss = 0.01666129\n",
      "Iteration 132, loss = 0.01641535\n",
      "Iteration 133, loss = 0.01617333\n",
      "Iteration 134, loss = 0.01594242\n",
      "Iteration 135, loss = 0.01570622\n",
      "Iteration 136, loss = 0.01547109\n",
      "Iteration 137, loss = 0.01524749\n",
      "Iteration 138, loss = 0.01502865\n",
      "Iteration 139, loss = 0.01481254\n",
      "Iteration 140, loss = 0.01459540\n",
      "Iteration 141, loss = 0.01437559\n",
      "Iteration 142, loss = 0.01417039\n",
      "Iteration 143, loss = 0.01396291\n",
      "Iteration 144, loss = 0.01374944\n",
      "Iteration 145, loss = 0.01354213\n",
      "Iteration 146, loss = 0.01334173\n",
      "Iteration 147, loss = 0.01313856\n",
      "Iteration 148, loss = 0.01292340\n",
      "Iteration 149, loss = 0.01271304\n",
      "Iteration 150, loss = 0.01249962\n",
      "Iteration 151, loss = 0.01229265\n",
      "Iteration 152, loss = 0.01208258\n",
      "Iteration 153, loss = 0.01187140\n",
      "Iteration 154, loss = 0.01167033\n",
      "Iteration 155, loss = 0.01146825\n",
      "Iteration 156, loss = 0.01126674\n",
      "Iteration 157, loss = 0.01106229\n",
      "Iteration 158, loss = 0.01086325\n",
      "Iteration 159, loss = 0.01066960\n",
      "Iteration 160, loss = 0.01047790\n",
      "Iteration 161, loss = 0.01028408\n",
      "Iteration 162, loss = 0.01009897\n",
      "Iteration 163, loss = 0.00992908\n",
      "Iteration 164, loss = 0.00976084\n",
      "Iteration 165, loss = 0.00959601\n",
      "Iteration 166, loss = 0.00943281\n",
      "Iteration 167, loss = 0.00927010\n",
      "Iteration 168, loss = 0.00910884\n",
      "Iteration 169, loss = 0.00895047\n",
      "Iteration 170, loss = 0.00879585\n",
      "Iteration 171, loss = 0.00864554\n",
      "Iteration 172, loss = 0.00849471\n",
      "Iteration 173, loss = 0.00834699\n",
      "Iteration 174, loss = 0.00819748\n",
      "Iteration 175, loss = 0.00804673\n",
      "Iteration 176, loss = 0.00789970\n",
      "Iteration 177, loss = 0.00775977\n",
      "Iteration 178, loss = 0.00762303\n",
      "Iteration 179, loss = 0.00748896\n",
      "Iteration 180, loss = 0.00735771\n",
      "Iteration 181, loss = 0.00722473\n",
      "Iteration 182, loss = 0.00709105\n",
      "Iteration 183, loss = 0.00696114\n",
      "Iteration 184, loss = 0.00683561\n",
      "Iteration 185, loss = 0.00671362\n",
      "Iteration 186, loss = 0.00659269\n",
      "Iteration 187, loss = 0.00647790\n",
      "Iteration 188, loss = 0.00636757\n",
      "Iteration 189, loss = 0.00625642\n",
      "Iteration 190, loss = 0.00614538\n",
      "Iteration 191, loss = 0.00603823\n",
      "Iteration 192, loss = 0.00593711\n",
      "Iteration 193, loss = 0.00583539\n",
      "Iteration 194, loss = 0.00573319\n",
      "Iteration 195, loss = 0.00563946\n",
      "Iteration 196, loss = 0.00554616\n",
      "Iteration 197, loss = 0.00545409\n",
      "Iteration 198, loss = 0.00536598\n",
      "Iteration 199, loss = 0.00527932\n",
      "Iteration 200, loss = 0.00519336\n",
      "Iteration 201, loss = 0.00510904\n",
      "Iteration 202, loss = 0.00502640\n",
      "Iteration 203, loss = 0.00494489\n",
      "Iteration 204, loss = 0.00486610\n",
      "Iteration 205, loss = 0.00478773\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46357991\n",
      "Iteration 2, loss = 0.40931644\n",
      "Iteration 3, loss = 0.37109462\n",
      "Iteration 4, loss = 0.33921564\n",
      "Iteration 5, loss = 0.31242043\n",
      "Iteration 6, loss = 0.29294981\n",
      "Iteration 7, loss = 0.27972505\n",
      "Iteration 8, loss = 0.26950919\n",
      "Iteration 9, loss = 0.25580734\n",
      "Iteration 10, loss = 0.24252680\n",
      "Iteration 11, loss = 0.22560320\n",
      "Iteration 12, loss = 0.20844244\n",
      "Iteration 13, loss = 0.19341336\n",
      "Iteration 14, loss = 0.18117181\n",
      "Iteration 15, loss = 0.17215624\n",
      "Iteration 16, loss = 0.16429028\n",
      "Iteration 17, loss = 0.15634620\n",
      "Iteration 18, loss = 0.14721808\n",
      "Iteration 19, loss = 0.14137263\n",
      "Iteration 20, loss = 0.13274210\n",
      "Iteration 21, loss = 0.12650958\n",
      "Iteration 22, loss = 0.12176611\n",
      "Iteration 23, loss = 0.11731058\n",
      "Iteration 24, loss = 0.11324995\n",
      "Iteration 25, loss = 0.10925883\n",
      "Iteration 26, loss = 0.10579649\n",
      "Iteration 27, loss = 0.10296442\n",
      "Iteration 28, loss = 0.10028004\n",
      "Iteration 29, loss = 0.09807632\n",
      "Iteration 30, loss = 0.09528225\n",
      "Iteration 31, loss = 0.09269294\n",
      "Iteration 32, loss = 0.09043094\n",
      "Iteration 33, loss = 0.08852409\n",
      "Iteration 34, loss = 0.08725435\n",
      "Iteration 35, loss = 0.08657664\n",
      "Iteration 36, loss = 0.08454945\n",
      "Iteration 37, loss = 0.08181576\n",
      "Iteration 38, loss = 0.07874326\n",
      "Iteration 39, loss = 0.07722109\n",
      "Iteration 40, loss = 0.07606514\n",
      "Iteration 41, loss = 0.07415438\n",
      "Iteration 42, loss = 0.07278607\n",
      "Iteration 43, loss = 0.07105202\n",
      "Iteration 44, loss = 0.06887711\n",
      "Iteration 45, loss = 0.06721643\n",
      "Iteration 46, loss = 0.06625632\n",
      "Iteration 47, loss = 0.06507689\n",
      "Iteration 48, loss = 0.06365007\n",
      "Iteration 49, loss = 0.06222695\n",
      "Iteration 50, loss = 0.06108978\n",
      "Iteration 51, loss = 0.05971590\n",
      "Iteration 52, loss = 0.05928619\n",
      "Iteration 53, loss = 0.05841869\n",
      "Iteration 54, loss = 0.05735745\n",
      "Iteration 55, loss = 0.05657479\n",
      "Iteration 56, loss = 0.05558659\n",
      "Iteration 57, loss = 0.05524928\n",
      "Iteration 58, loss = 0.05482491\n",
      "Iteration 59, loss = 0.05396917\n",
      "Iteration 60, loss = 0.05231665\n",
      "Iteration 61, loss = 0.05143637\n",
      "Iteration 62, loss = 0.05162875\n",
      "Iteration 63, loss = 0.05063603\n",
      "Iteration 64, loss = 0.04913625\n",
      "Iteration 65, loss = 0.04845730\n",
      "Iteration 66, loss = 0.04860551\n",
      "Iteration 67, loss = 0.04748413\n",
      "Iteration 68, loss = 0.04675765\n",
      "Iteration 69, loss = 0.04601327\n",
      "Iteration 70, loss = 0.04489727\n",
      "Iteration 71, loss = 0.04492848\n",
      "Iteration 72, loss = 0.04463157\n",
      "Iteration 73, loss = 0.04310436\n",
      "Iteration 74, loss = 0.04319249\n",
      "Iteration 75, loss = 0.04330432\n",
      "Iteration 76, loss = 0.04208282\n",
      "Iteration 77, loss = 0.04104590\n",
      "Iteration 78, loss = 0.04035909\n",
      "Iteration 79, loss = 0.03954014\n",
      "Iteration 80, loss = 0.03947859\n",
      "Iteration 81, loss = 0.03852814\n",
      "Iteration 82, loss = 0.03764604\n",
      "Iteration 83, loss = 0.03703899\n",
      "Iteration 84, loss = 0.03644206\n",
      "Iteration 85, loss = 0.03573461\n",
      "Iteration 86, loss = 0.03515844\n",
      "Iteration 87, loss = 0.03487327\n",
      "Iteration 88, loss = 0.03467347\n",
      "Iteration 89, loss = 0.03478281\n",
      "Iteration 90, loss = 0.03418224\n",
      "Iteration 91, loss = 0.03306390\n",
      "Iteration 92, loss = 0.03382317\n",
      "Iteration 93, loss = 0.03267399\n",
      "Iteration 94, loss = 0.03231468\n",
      "Iteration 95, loss = 0.03243040\n",
      "Iteration 96, loss = 0.03165689\n",
      "Iteration 97, loss = 0.03088483\n",
      "Iteration 98, loss = 0.03010655\n",
      "Iteration 99, loss = 0.02948201\n",
      "Iteration 100, loss = 0.03009210\n",
      "Iteration 101, loss = 0.03144486\n",
      "Iteration 102, loss = 0.03100395\n",
      "Iteration 103, loss = 0.02984479\n",
      "Iteration 104, loss = 0.02932188\n",
      "Iteration 105, loss = 0.02856800\n",
      "Iteration 106, loss = 0.02780393\n",
      "Iteration 107, loss = 0.02831790\n",
      "Iteration 108, loss = 0.02818876\n",
      "Iteration 109, loss = 0.02722435\n",
      "Iteration 110, loss = 0.02687089\n",
      "Iteration 111, loss = 0.02649936\n",
      "Iteration 112, loss = 0.02584673\n",
      "Iteration 113, loss = 0.02570313\n",
      "Iteration 114, loss = 0.02562545\n",
      "Iteration 115, loss = 0.02536121\n",
      "Iteration 116, loss = 0.02508402\n",
      "Iteration 117, loss = 0.02484985\n",
      "Iteration 118, loss = 0.02452352\n",
      "Iteration 119, loss = 0.02502681\n",
      "Iteration 120, loss = 0.02376444\n",
      "Iteration 121, loss = 0.02442550\n",
      "Iteration 122, loss = 0.02429672\n",
      "Iteration 123, loss = 0.02319916\n",
      "Iteration 124, loss = 0.02435152\n",
      "Iteration 125, loss = 0.02245530\n",
      "Iteration 126, loss = 0.02371242\n",
      "Iteration 127, loss = 0.02334550\n",
      "Iteration 128, loss = 0.02161311\n",
      "Iteration 129, loss = 0.02236798\n",
      "Iteration 130, loss = 0.02179547\n",
      "Iteration 131, loss = 0.02137016\n",
      "Iteration 132, loss = 0.02087327\n",
      "Iteration 133, loss = 0.02085435\n",
      "Iteration 134, loss = 0.02053644\n",
      "Iteration 135, loss = 0.02021340\n",
      "Iteration 136, loss = 0.02020011\n",
      "Iteration 137, loss = 0.01954573\n",
      "Iteration 138, loss = 0.02032162\n",
      "Iteration 139, loss = 0.02007755\n",
      "Iteration 140, loss = 0.01938822\n",
      "Iteration 141, loss = 0.02099042\n",
      "Iteration 142, loss = 0.01930262\n",
      "Iteration 143, loss = 0.01976923\n",
      "Iteration 144, loss = 0.01964547\n",
      "Iteration 145, loss = 0.01859873\n",
      "Iteration 146, loss = 0.01851059\n",
      "Iteration 147, loss = 0.01821093\n",
      "Iteration 148, loss = 0.01784900\n",
      "Iteration 149, loss = 0.01758391\n",
      "Iteration 150, loss = 0.01905773\n",
      "Iteration 151, loss = 0.01935223\n",
      "Iteration 152, loss = 0.01875722\n",
      "Iteration 153, loss = 0.01817894\n",
      "Iteration 154, loss = 0.01745309\n",
      "Iteration 155, loss = 0.01761573\n",
      "Iteration 156, loss = 0.01790549\n",
      "Iteration 157, loss = 0.01730371\n",
      "Iteration 158, loss = 0.01712901\n",
      "Iteration 159, loss = 0.01690513\n",
      "Iteration 160, loss = 0.01640927\n",
      "Iteration 161, loss = 0.01606307\n",
      "Iteration 162, loss = 0.01576261\n",
      "Iteration 163, loss = 0.01564163\n",
      "Iteration 164, loss = 0.01569624\n",
      "Iteration 165, loss = 0.01554549\n",
      "Iteration 166, loss = 0.01531912\n",
      "Iteration 167, loss = 0.01524874\n",
      "Iteration 168, loss = 0.01530374\n",
      "Iteration 169, loss = 0.01525674\n",
      "Iteration 170, loss = 0.01468475\n",
      "Iteration 171, loss = 0.01494294\n",
      "Iteration 172, loss = 0.01482253\n",
      "Iteration 173, loss = 0.01448413\n",
      "Iteration 174, loss = 0.01427634\n",
      "Iteration 175, loss = 0.01396359\n",
      "Iteration 176, loss = 0.01391546\n",
      "Iteration 177, loss = 0.01368450\n",
      "Iteration 178, loss = 0.01359120\n",
      "Iteration 179, loss = 0.01336736\n",
      "Iteration 180, loss = 0.01314214\n",
      "Iteration 181, loss = 0.01328958\n",
      "Iteration 182, loss = 0.01328142\n",
      "Iteration 183, loss = 0.01334836\n",
      "Iteration 184, loss = 0.01279805\n",
      "Iteration 185, loss = 0.01303505\n",
      "Iteration 186, loss = 0.01379968\n",
      "Iteration 187, loss = 0.01332121\n",
      "Iteration 188, loss = 0.01330955\n",
      "Iteration 189, loss = 0.01236013\n",
      "Iteration 190, loss = 0.01234077\n",
      "Iteration 191, loss = 0.01214391\n",
      "Iteration 192, loss = 0.01176390\n",
      "Iteration 193, loss = 0.01180869\n",
      "Iteration 194, loss = 0.01156655\n",
      "Iteration 195, loss = 0.01154612\n",
      "Iteration 196, loss = 0.01125175\n",
      "Iteration 197, loss = 0.01139783\n",
      "Iteration 198, loss = 0.01148398\n",
      "Iteration 199, loss = 0.01118250\n",
      "Iteration 200, loss = 0.01125747\n",
      "Iteration 201, loss = 0.01106007\n",
      "Iteration 202, loss = 0.01074721\n",
      "Iteration 203, loss = 0.01057621\n",
      "Iteration 204, loss = 0.01049300\n",
      "Iteration 205, loss = 0.01076881\n",
      "Iteration 206, loss = 0.01046152\n",
      "Iteration 207, loss = 0.01037400\n",
      "Iteration 208, loss = 0.01015867\n",
      "Iteration 209, loss = 0.00989364\n",
      "Iteration 210, loss = 0.00992379\n",
      "Iteration 211, loss = 0.00999517\n",
      "Iteration 212, loss = 0.00970690\n",
      "Iteration 213, loss = 0.00952339\n",
      "Iteration 214, loss = 0.00934252\n",
      "Iteration 215, loss = 0.00922054\n",
      "Iteration 216, loss = 0.00917102\n",
      "Iteration 217, loss = 0.00921883\n",
      "Iteration 218, loss = 0.00922568\n",
      "Iteration 219, loss = 0.00897204\n",
      "Iteration 220, loss = 0.00892582\n",
      "Iteration 221, loss = 0.00868717\n",
      "Iteration 222, loss = 0.00907461\n",
      "Iteration 223, loss = 0.00850333\n",
      "Iteration 224, loss = 0.00907073\n",
      "Iteration 225, loss = 0.00870376\n",
      "Iteration 226, loss = 0.00849958\n",
      "Iteration 227, loss = 0.00860471\n",
      "Iteration 228, loss = 0.00806493\n",
      "Iteration 229, loss = 0.00835034\n",
      "Iteration 230, loss = 0.00787920\n",
      "Iteration 231, loss = 0.00876730\n",
      "Iteration 232, loss = 0.00855889\n",
      "Iteration 233, loss = 0.00814342\n",
      "Iteration 234, loss = 0.00826162\n",
      "Iteration 235, loss = 0.00762258\n",
      "Iteration 236, loss = 0.00849276\n",
      "Iteration 237, loss = 0.00752377\n",
      "Iteration 238, loss = 0.00847758\n",
      "Iteration 239, loss = 0.00716293\n",
      "Iteration 240, loss = 0.00892102\n",
      "Iteration 241, loss = 0.00776661\n",
      "Iteration 242, loss = 0.00799764\n",
      "Iteration 243, loss = 0.00732995\n",
      "Iteration 244, loss = 0.00728023\n",
      "Iteration 245, loss = 0.00734337\n",
      "Iteration 246, loss = 0.00743815\n",
      "Iteration 247, loss = 0.00784897\n",
      "Iteration 248, loss = 0.00711893\n",
      "Iteration 249, loss = 0.00737682\n",
      "Iteration 250, loss = 0.00667400\n",
      "Iteration 251, loss = 0.00671216\n",
      "Iteration 252, loss = 0.00638626\n",
      "Iteration 253, loss = 0.00646659\n",
      "Iteration 254, loss = 0.00632347\n",
      "Iteration 255, loss = 0.00680107\n",
      "Iteration 256, loss = 0.00633401\n",
      "Iteration 257, loss = 0.00649335\n",
      "Iteration 258, loss = 0.00627027\n",
      "Iteration 259, loss = 0.00630019\n",
      "Iteration 260, loss = 0.00587451\n",
      "Iteration 261, loss = 0.00640742\n",
      "Iteration 262, loss = 0.00596993\n",
      "Iteration 263, loss = 0.00608964\n",
      "Iteration 264, loss = 0.00622739\n",
      "Iteration 265, loss = 0.00605311\n",
      "Iteration 266, loss = 0.00619427\n",
      "Iteration 267, loss = 0.00586979\n",
      "Iteration 268, loss = 0.00574231\n",
      "Iteration 269, loss = 0.00559329\n",
      "Iteration 270, loss = 0.00569399\n",
      "Iteration 271, loss = 0.00611465\n",
      "Iteration 272, loss = 0.00549531\n",
      "Iteration 273, loss = 0.00548639\n",
      "Iteration 274, loss = 0.00559587\n",
      "Iteration 275, loss = 0.00528665\n",
      "Iteration 276, loss = 0.00541574\n",
      "Iteration 277, loss = 0.00524700\n",
      "Iteration 278, loss = 0.00509201\n",
      "Iteration 279, loss = 0.00515793\n",
      "Iteration 280, loss = 0.00508620\n",
      "Iteration 281, loss = 0.00508526\n",
      "Iteration 282, loss = 0.00489624\n",
      "Iteration 283, loss = 0.00487372\n",
      "Iteration 284, loss = 0.00486211\n",
      "Iteration 285, loss = 0.00473141\n",
      "Iteration 286, loss = 0.00488338\n",
      "Iteration 287, loss = 0.00470799\n",
      "Iteration 288, loss = 0.00482457\n",
      "Iteration 289, loss = 0.00460568\n",
      "Iteration 290, loss = 0.00468450\n",
      "Iteration 291, loss = 0.00445949\n",
      "Iteration 292, loss = 0.00454881\n",
      "Iteration 293, loss = 0.00434905\n",
      "Iteration 294, loss = 0.00460950\n",
      "Iteration 295, loss = 0.00437259\n",
      "Iteration 296, loss = 0.00443076\n",
      "Iteration 297, loss = 0.00423734\n",
      "Iteration 298, loss = 0.00468013\n",
      "Iteration 299, loss = 0.00446325\n",
      "Iteration 300, loss = 0.00416317\n",
      "Iteration 301, loss = 0.00416800\n",
      "Iteration 302, loss = 0.00443143\n",
      "Iteration 303, loss = 0.00425742\n",
      "Iteration 304, loss = 0.00419421\n",
      "Iteration 305, loss = 0.00419810\n",
      "Iteration 306, loss = 0.00409299\n",
      "Iteration 307, loss = 0.00403482\n",
      "Iteration 308, loss = 0.00407428\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46357991\n",
      "Iteration 2, loss = 0.40931644\n",
      "Iteration 3, loss = 0.37109462\n",
      "Iteration 4, loss = 0.33921564\n",
      "Iteration 5, loss = 0.31242043\n",
      "Iteration 6, loss = 0.29294981\n",
      "Iteration 7, loss = 0.27972505\n",
      "Iteration 8, loss = 0.26950919\n",
      "Iteration 9, loss = 0.25580734\n",
      "Iteration 10, loss = 0.24252680\n",
      "Iteration 11, loss = 0.22560320\n",
      "Iteration 12, loss = 0.20844244\n",
      "Iteration 13, loss = 0.19341336\n",
      "Iteration 14, loss = 0.18117181\n",
      "Iteration 15, loss = 0.17215624\n",
      "Iteration 16, loss = 0.16429028\n",
      "Iteration 17, loss = 0.15634620\n",
      "Iteration 18, loss = 0.14721808\n",
      "Iteration 19, loss = 0.14137263\n",
      "Iteration 20, loss = 0.13274210\n",
      "Iteration 21, loss = 0.12650958\n",
      "Iteration 22, loss = 0.12176611\n",
      "Iteration 23, loss = 0.11731058\n",
      "Iteration 24, loss = 0.11324995\n",
      "Iteration 25, loss = 0.10925883\n",
      "Iteration 26, loss = 0.10579649\n",
      "Iteration 27, loss = 0.10296442\n",
      "Iteration 28, loss = 0.10028004\n",
      "Iteration 29, loss = 0.09807632\n",
      "Iteration 30, loss = 0.09528225\n",
      "Iteration 31, loss = 0.09269294\n",
      "Iteration 32, loss = 0.09043094\n",
      "Iteration 33, loss = 0.08852409\n",
      "Iteration 34, loss = 0.08725435\n",
      "Iteration 35, loss = 0.08657664\n",
      "Iteration 36, loss = 0.08454945\n",
      "Iteration 37, loss = 0.08181576\n",
      "Iteration 38, loss = 0.07874326\n",
      "Iteration 39, loss = 0.07722109\n",
      "Iteration 40, loss = 0.07606514\n",
      "Iteration 41, loss = 0.07415438\n",
      "Iteration 42, loss = 0.07278607\n",
      "Iteration 43, loss = 0.07105202\n",
      "Iteration 44, loss = 0.06887711\n",
      "Iteration 45, loss = 0.06721643\n",
      "Iteration 46, loss = 0.06625632\n",
      "Iteration 47, loss = 0.06507689\n",
      "Iteration 48, loss = 0.06365007\n",
      "Iteration 49, loss = 0.06222695\n",
      "Iteration 50, loss = 0.06108978\n",
      "Iteration 51, loss = 0.05971590\n",
      "Iteration 52, loss = 0.05928619\n",
      "Iteration 53, loss = 0.05841869\n",
      "Iteration 54, loss = 0.05735745\n",
      "Iteration 55, loss = 0.05657479\n",
      "Iteration 56, loss = 0.05558659\n",
      "Iteration 57, loss = 0.05524928\n",
      "Iteration 58, loss = 0.05482491\n",
      "Iteration 59, loss = 0.05396917\n",
      "Iteration 60, loss = 0.05231665\n",
      "Iteration 61, loss = 0.05143637\n",
      "Iteration 62, loss = 0.05162875\n",
      "Iteration 63, loss = 0.05063603\n",
      "Iteration 64, loss = 0.04913625\n",
      "Iteration 65, loss = 0.04845730\n",
      "Iteration 66, loss = 0.04860551\n",
      "Iteration 67, loss = 0.04748413\n",
      "Iteration 68, loss = 0.04675765\n",
      "Iteration 69, loss = 0.04601327\n",
      "Iteration 70, loss = 0.04489727\n",
      "Iteration 71, loss = 0.04492848\n",
      "Iteration 72, loss = 0.04463157\n",
      "Iteration 73, loss = 0.04310436\n",
      "Iteration 74, loss = 0.04319249\n",
      "Iteration 75, loss = 0.04330432\n",
      "Iteration 76, loss = 0.04208282\n",
      "Iteration 77, loss = 0.04104590\n",
      "Iteration 78, loss = 0.04035909\n",
      "Iteration 79, loss = 0.03954014\n",
      "Iteration 80, loss = 0.03947859\n",
      "Iteration 81, loss = 0.03852814\n",
      "Iteration 82, loss = 0.03764604\n",
      "Iteration 83, loss = 0.03703899\n",
      "Iteration 84, loss = 0.03644206\n",
      "Iteration 85, loss = 0.03573461\n",
      "Iteration 86, loss = 0.03515844\n",
      "Iteration 87, loss = 0.03487327\n",
      "Iteration 88, loss = 0.03467347\n",
      "Iteration 89, loss = 0.03478281\n",
      "Iteration 90, loss = 0.03418224\n",
      "Iteration 91, loss = 0.03306390\n",
      "Iteration 92, loss = 0.03382317\n",
      "Iteration 93, loss = 0.03267399\n",
      "Iteration 94, loss = 0.03231468\n",
      "Iteration 95, loss = 0.03243040\n",
      "Iteration 96, loss = 0.03165689\n",
      "Iteration 97, loss = 0.03088483\n",
      "Iteration 98, loss = 0.03010655\n",
      "Iteration 99, loss = 0.02948201\n",
      "Iteration 100, loss = 0.03009210\n",
      "Iteration 101, loss = 0.03144486\n",
      "Iteration 102, loss = 0.03100395\n",
      "Iteration 103, loss = 0.02984479\n",
      "Iteration 104, loss = 0.02932188\n",
      "Iteration 105, loss = 0.02856800\n",
      "Iteration 106, loss = 0.02780393\n",
      "Iteration 107, loss = 0.02831790\n",
      "Iteration 108, loss = 0.02818876\n",
      "Iteration 109, loss = 0.02722435\n",
      "Iteration 110, loss = 0.02687089\n",
      "Iteration 111, loss = 0.02649936\n",
      "Iteration 112, loss = 0.02584673\n",
      "Iteration 113, loss = 0.02570313\n",
      "Iteration 114, loss = 0.02562545\n",
      "Iteration 115, loss = 0.02536121\n",
      "Iteration 116, loss = 0.02508402\n",
      "Iteration 117, loss = 0.02484985\n",
      "Iteration 118, loss = 0.02452352\n",
      "Iteration 119, loss = 0.02502681\n",
      "Iteration 120, loss = 0.02376444\n",
      "Iteration 121, loss = 0.02442550\n",
      "Iteration 122, loss = 0.02429672\n",
      "Iteration 123, loss = 0.02319916\n",
      "Iteration 124, loss = 0.02435152\n",
      "Iteration 125, loss = 0.02245530\n",
      "Iteration 126, loss = 0.02371242\n",
      "Iteration 127, loss = 0.02334550\n",
      "Iteration 128, loss = 0.02161311\n",
      "Iteration 129, loss = 0.02236798\n",
      "Iteration 130, loss = 0.02179547\n",
      "Iteration 131, loss = 0.02137016\n",
      "Iteration 132, loss = 0.02087327\n",
      "Iteration 133, loss = 0.02085435\n",
      "Iteration 134, loss = 0.02053644\n",
      "Iteration 135, loss = 0.02021340\n",
      "Iteration 136, loss = 0.02020011\n",
      "Iteration 137, loss = 0.01954573\n",
      "Iteration 138, loss = 0.02032162\n",
      "Iteration 139, loss = 0.02007755\n",
      "Iteration 140, loss = 0.01938822\n",
      "Iteration 141, loss = 0.02099042\n",
      "Iteration 142, loss = 0.01930262\n",
      "Iteration 143, loss = 0.01976923\n",
      "Iteration 144, loss = 0.01964547\n",
      "Iteration 145, loss = 0.01859873\n",
      "Iteration 146, loss = 0.01851059\n",
      "Iteration 147, loss = 0.01821093\n",
      "Iteration 148, loss = 0.01784900\n",
      "Iteration 149, loss = 0.01758391\n",
      "Iteration 150, loss = 0.01905773\n",
      "Iteration 151, loss = 0.01935223\n",
      "Iteration 152, loss = 0.01875722\n",
      "Iteration 153, loss = 0.01817894\n",
      "Iteration 154, loss = 0.01745309\n",
      "Iteration 155, loss = 0.01761573\n",
      "Iteration 156, loss = 0.01790549\n",
      "Iteration 157, loss = 0.01730371\n",
      "Iteration 158, loss = 0.01712901\n",
      "Iteration 159, loss = 0.01690513\n",
      "Iteration 160, loss = 0.01640927\n",
      "Iteration 161, loss = 0.01606307\n",
      "Iteration 162, loss = 0.01576261\n",
      "Iteration 163, loss = 0.01564163\n",
      "Iteration 164, loss = 0.01569624\n",
      "Iteration 165, loss = 0.01554549\n",
      "Iteration 166, loss = 0.01531912\n",
      "Iteration 167, loss = 0.01524874\n",
      "Iteration 168, loss = 0.01530374\n",
      "Iteration 169, loss = 0.01525674\n",
      "Iteration 170, loss = 0.01468475\n",
      "Iteration 171, loss = 0.01494294\n",
      "Iteration 172, loss = 0.01482253\n",
      "Iteration 173, loss = 0.01448413\n",
      "Iteration 174, loss = 0.01427634\n",
      "Iteration 175, loss = 0.01396359\n",
      "Iteration 176, loss = 0.01391546\n",
      "Iteration 177, loss = 0.01368450\n",
      "Iteration 178, loss = 0.01359120\n",
      "Iteration 179, loss = 0.01336736\n",
      "Iteration 180, loss = 0.01314214\n",
      "Iteration 181, loss = 0.01328958\n",
      "Iteration 182, loss = 0.01328142\n",
      "Iteration 183, loss = 0.01334836\n",
      "Iteration 184, loss = 0.01279805\n",
      "Iteration 185, loss = 0.01303505\n",
      "Iteration 186, loss = 0.01379968\n",
      "Iteration 187, loss = 0.01332121\n",
      "Iteration 188, loss = 0.01330955\n",
      "Iteration 189, loss = 0.01236013\n",
      "Iteration 190, loss = 0.01234077\n",
      "Iteration 191, loss = 0.01214391\n",
      "Iteration 192, loss = 0.01176390\n",
      "Iteration 193, loss = 0.01180869\n",
      "Iteration 194, loss = 0.01156655\n",
      "Iteration 195, loss = 0.01154612\n",
      "Iteration 196, loss = 0.01125175\n",
      "Iteration 197, loss = 0.01139783\n",
      "Iteration 198, loss = 0.01148398\n",
      "Iteration 199, loss = 0.01118250\n",
      "Iteration 200, loss = 0.01125747\n",
      "Iteration 201, loss = 0.01106007\n",
      "Iteration 202, loss = 0.01074721\n",
      "Iteration 203, loss = 0.01057621\n",
      "Iteration 204, loss = 0.01049300\n",
      "Iteration 205, loss = 0.01076881\n",
      "Iteration 206, loss = 0.01046152\n",
      "Iteration 207, loss = 0.01037400\n",
      "Iteration 208, loss = 0.01015867\n",
      "Iteration 209, loss = 0.00989364\n",
      "Iteration 210, loss = 0.00992379\n",
      "Iteration 211, loss = 0.00999517\n",
      "Iteration 212, loss = 0.00970690\n",
      "Iteration 213, loss = 0.00952339\n",
      "Iteration 214, loss = 0.00934252\n",
      "Iteration 215, loss = 0.00922054\n",
      "Iteration 216, loss = 0.00917102\n",
      "Iteration 217, loss = 0.00921883\n",
      "Iteration 218, loss = 0.00922568\n",
      "Iteration 219, loss = 0.00897204\n",
      "Iteration 220, loss = 0.00892582\n",
      "Iteration 221, loss = 0.00868717\n",
      "Iteration 222, loss = 0.00907461\n",
      "Iteration 223, loss = 0.00850333\n",
      "Iteration 224, loss = 0.00907073\n",
      "Iteration 225, loss = 0.00870376\n",
      "Iteration 226, loss = 0.00849958\n",
      "Iteration 227, loss = 0.00860471\n",
      "Iteration 228, loss = 0.00806493\n",
      "Iteration 229, loss = 0.00835034\n",
      "Iteration 230, loss = 0.00787920\n",
      "Iteration 231, loss = 0.00876730\n",
      "Iteration 232, loss = 0.00855889\n",
      "Iteration 233, loss = 0.00814342\n",
      "Iteration 234, loss = 0.00826162\n",
      "Iteration 235, loss = 0.00762258\n",
      "Iteration 236, loss = 0.00849276\n",
      "Iteration 237, loss = 0.00752377\n",
      "Iteration 238, loss = 0.00847758\n",
      "Iteration 239, loss = 0.00716293\n",
      "Iteration 240, loss = 0.00892102\n",
      "Iteration 241, loss = 0.00776661\n",
      "Iteration 242, loss = 0.00799764\n",
      "Iteration 243, loss = 0.00732995\n",
      "Iteration 244, loss = 0.00728023\n",
      "Iteration 245, loss = 0.00734337\n",
      "Iteration 246, loss = 0.00743815\n",
      "Iteration 247, loss = 0.00784897\n",
      "Iteration 248, loss = 0.00711893\n",
      "Iteration 249, loss = 0.00737682\n",
      "Iteration 250, loss = 0.00667400\n",
      "Iteration 251, loss = 0.00671216\n",
      "Iteration 252, loss = 0.00638626\n",
      "Iteration 253, loss = 0.00646659\n",
      "Iteration 254, loss = 0.00632347\n",
      "Iteration 255, loss = 0.00680107\n",
      "Iteration 256, loss = 0.00633401\n",
      "Iteration 257, loss = 0.00649335\n",
      "Iteration 258, loss = 0.00627027\n",
      "Iteration 259, loss = 0.00630019\n",
      "Iteration 260, loss = 0.00587451\n",
      "Iteration 261, loss = 0.00640742\n",
      "Iteration 262, loss = 0.00596993\n",
      "Iteration 263, loss = 0.00608964\n",
      "Iteration 264, loss = 0.00622739\n",
      "Iteration 265, loss = 0.00605311\n",
      "Iteration 266, loss = 0.00619427\n",
      "Iteration 267, loss = 0.00586979\n",
      "Iteration 268, loss = 0.00574231\n",
      "Iteration 269, loss = 0.00559329\n",
      "Iteration 270, loss = 0.00569399\n",
      "Iteration 271, loss = 0.00611465\n",
      "Iteration 272, loss = 0.00549531\n",
      "Iteration 273, loss = 0.00548639\n",
      "Iteration 274, loss = 0.00559587\n",
      "Iteration 275, loss = 0.00528665\n",
      "Iteration 276, loss = 0.00541574\n",
      "Iteration 277, loss = 0.00524700\n",
      "Iteration 278, loss = 0.00509201\n",
      "Iteration 279, loss = 0.00515793\n",
      "Iteration 280, loss = 0.00508620\n",
      "Iteration 281, loss = 0.00508526\n",
      "Iteration 282, loss = 0.00489624\n",
      "Iteration 283, loss = 0.00487372\n",
      "Iteration 284, loss = 0.00486211\n",
      "Iteration 285, loss = 0.00473141\n",
      "Iteration 286, loss = 0.00488338\n",
      "Iteration 287, loss = 0.00470799\n",
      "Iteration 288, loss = 0.00482457\n",
      "Iteration 289, loss = 0.00460568\n",
      "Iteration 290, loss = 0.00468450\n",
      "Iteration 291, loss = 0.00445949\n",
      "Iteration 292, loss = 0.00454881\n",
      "Iteration 293, loss = 0.00434905\n",
      "Iteration 294, loss = 0.00460950\n",
      "Iteration 295, loss = 0.00437259\n",
      "Iteration 296, loss = 0.00443076\n",
      "Iteration 297, loss = 0.00423734\n",
      "Iteration 298, loss = 0.00468013\n",
      "Iteration 299, loss = 0.00446325\n",
      "Iteration 300, loss = 0.00416317\n",
      "Iteration 301, loss = 0.00416800\n",
      "Iteration 302, loss = 0.00443143\n",
      "Iteration 303, loss = 0.00425742\n",
      "Iteration 304, loss = 0.00419421\n",
      "Iteration 305, loss = 0.00419810\n",
      "Iteration 306, loss = 0.00409299\n",
      "Iteration 307, loss = 0.00403482\n",
      "Iteration 308, loss = 0.00407428\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46357991\n",
      "Iteration 2, loss = 0.40931644\n",
      "Iteration 3, loss = 0.37109462\n",
      "Iteration 4, loss = 0.33921564\n",
      "Iteration 5, loss = 0.31242043\n",
      "Iteration 6, loss = 0.29294981\n",
      "Iteration 7, loss = 0.27972505\n",
      "Iteration 8, loss = 0.26950919\n",
      "Iteration 9, loss = 0.25580734\n",
      "Iteration 10, loss = 0.24252680\n",
      "Iteration 11, loss = 0.22560320\n",
      "Iteration 12, loss = 0.20844244\n",
      "Iteration 13, loss = 0.19341336\n",
      "Iteration 14, loss = 0.18117181\n",
      "Iteration 15, loss = 0.17215624\n",
      "Iteration 16, loss = 0.16429028\n",
      "Iteration 17, loss = 0.15634620\n",
      "Iteration 18, loss = 0.14721808\n",
      "Iteration 19, loss = 0.14137263\n",
      "Iteration 20, loss = 0.13274210\n",
      "Iteration 21, loss = 0.12650958\n",
      "Iteration 22, loss = 0.12176611\n",
      "Iteration 23, loss = 0.11731058\n",
      "Iteration 24, loss = 0.11324995\n",
      "Iteration 25, loss = 0.10925883\n",
      "Iteration 26, loss = 0.10579649\n",
      "Iteration 27, loss = 0.10296442\n",
      "Iteration 28, loss = 0.10028004\n",
      "Iteration 29, loss = 0.09807632\n",
      "Iteration 30, loss = 0.09528225\n",
      "Iteration 31, loss = 0.09269294\n",
      "Iteration 32, loss = 0.09043094\n",
      "Iteration 33, loss = 0.08852409\n",
      "Iteration 34, loss = 0.08725435\n",
      "Iteration 35, loss = 0.08657664\n",
      "Iteration 36, loss = 0.08454945\n",
      "Iteration 37, loss = 0.08181576\n",
      "Iteration 38, loss = 0.07874326\n",
      "Iteration 39, loss = 0.07722109\n",
      "Iteration 40, loss = 0.07606514\n",
      "Iteration 41, loss = 0.07415438\n",
      "Iteration 42, loss = 0.07278607\n",
      "Iteration 43, loss = 0.07105202\n",
      "Iteration 44, loss = 0.06887711\n",
      "Iteration 45, loss = 0.06721643\n",
      "Iteration 46, loss = 0.06625632\n",
      "Iteration 47, loss = 0.06507689\n",
      "Iteration 48, loss = 0.06365007\n",
      "Iteration 49, loss = 0.06222695\n",
      "Iteration 50, loss = 0.06108978\n",
      "Iteration 51, loss = 0.05971590\n",
      "Iteration 52, loss = 0.05928619\n",
      "Iteration 53, loss = 0.05841869\n",
      "Iteration 54, loss = 0.05735745\n",
      "Iteration 55, loss = 0.05657479\n",
      "Iteration 56, loss = 0.05558659\n",
      "Iteration 57, loss = 0.05524928\n",
      "Iteration 58, loss = 0.05482491\n",
      "Iteration 59, loss = 0.05396917\n",
      "Iteration 60, loss = 0.05231665\n",
      "Iteration 61, loss = 0.05143637\n",
      "Iteration 62, loss = 0.05162875\n",
      "Iteration 63, loss = 0.05063603\n",
      "Iteration 64, loss = 0.04913625\n",
      "Iteration 65, loss = 0.04845730\n",
      "Iteration 66, loss = 0.04860551\n",
      "Iteration 67, loss = 0.04748413\n",
      "Iteration 68, loss = 0.04675765\n",
      "Iteration 69, loss = 0.04601327\n",
      "Iteration 70, loss = 0.04489727\n",
      "Iteration 71, loss = 0.04492848\n",
      "Iteration 72, loss = 0.04463157\n",
      "Iteration 73, loss = 0.04310436\n",
      "Iteration 74, loss = 0.04319249\n",
      "Iteration 75, loss = 0.04330432\n",
      "Iteration 76, loss = 0.04208282\n",
      "Iteration 77, loss = 0.04104590\n",
      "Iteration 78, loss = 0.04035909\n",
      "Iteration 79, loss = 0.03954014\n",
      "Iteration 80, loss = 0.03947859\n",
      "Iteration 81, loss = 0.03852814\n",
      "Iteration 82, loss = 0.03764604\n",
      "Iteration 83, loss = 0.03703899\n",
      "Iteration 84, loss = 0.03644206\n",
      "Iteration 85, loss = 0.03573461\n",
      "Iteration 86, loss = 0.03515844\n",
      "Iteration 87, loss = 0.03487327\n",
      "Iteration 88, loss = 0.03467347\n",
      "Iteration 89, loss = 0.03478281\n",
      "Iteration 90, loss = 0.03418224\n",
      "Iteration 91, loss = 0.03306390\n",
      "Iteration 92, loss = 0.03382317\n",
      "Iteration 93, loss = 0.03267399\n",
      "Iteration 94, loss = 0.03231468\n",
      "Iteration 95, loss = 0.03243040\n",
      "Iteration 96, loss = 0.03165689\n",
      "Iteration 97, loss = 0.03088483\n",
      "Iteration 98, loss = 0.03010655\n",
      "Iteration 99, loss = 0.02948201\n",
      "Iteration 100, loss = 0.03009210\n",
      "Iteration 101, loss = 0.03144486\n",
      "Iteration 102, loss = 0.03100395\n",
      "Iteration 103, loss = 0.02984479\n",
      "Iteration 104, loss = 0.02932188\n",
      "Iteration 105, loss = 0.02856800\n",
      "Iteration 106, loss = 0.02780393\n",
      "Iteration 107, loss = 0.02831790\n",
      "Iteration 108, loss = 0.02818876\n",
      "Iteration 109, loss = 0.02722435\n",
      "Iteration 110, loss = 0.02687089\n",
      "Iteration 111, loss = 0.02649936\n",
      "Iteration 112, loss = 0.02584673\n",
      "Iteration 113, loss = 0.02570313\n",
      "Iteration 114, loss = 0.02562545\n",
      "Iteration 115, loss = 0.02536121\n",
      "Iteration 116, loss = 0.02508402\n",
      "Iteration 117, loss = 0.02484985\n",
      "Iteration 118, loss = 0.02452352\n",
      "Iteration 119, loss = 0.02502681\n",
      "Iteration 120, loss = 0.02376444\n",
      "Iteration 121, loss = 0.02442550\n",
      "Iteration 122, loss = 0.02429672\n",
      "Iteration 123, loss = 0.02319916\n",
      "Iteration 124, loss = 0.02435152\n",
      "Iteration 125, loss = 0.02245530\n",
      "Iteration 126, loss = 0.02371242\n",
      "Iteration 127, loss = 0.02334550\n",
      "Iteration 128, loss = 0.02161311\n",
      "Iteration 129, loss = 0.02236798\n",
      "Iteration 130, loss = 0.02179547\n",
      "Iteration 131, loss = 0.02137016\n",
      "Iteration 132, loss = 0.02087327\n",
      "Iteration 133, loss = 0.02085435\n",
      "Iteration 134, loss = 0.02053644\n",
      "Iteration 135, loss = 0.02021340\n",
      "Iteration 136, loss = 0.02020011\n",
      "Iteration 137, loss = 0.01954573\n",
      "Iteration 138, loss = 0.02032162\n",
      "Iteration 139, loss = 0.02007755\n",
      "Iteration 140, loss = 0.01938822\n",
      "Iteration 141, loss = 0.02099042\n",
      "Iteration 142, loss = 0.01930262\n",
      "Iteration 143, loss = 0.01976923\n",
      "Iteration 144, loss = 0.01964547\n",
      "Iteration 145, loss = 0.01859873\n",
      "Iteration 146, loss = 0.01851059\n",
      "Iteration 147, loss = 0.01821093\n",
      "Iteration 148, loss = 0.01784900\n",
      "Iteration 149, loss = 0.01758391\n",
      "Iteration 150, loss = 0.01905773\n",
      "Iteration 151, loss = 0.01935223\n",
      "Iteration 152, loss = 0.01875722\n",
      "Iteration 153, loss = 0.01817894\n",
      "Iteration 154, loss = 0.01745309\n",
      "Iteration 155, loss = 0.01761573\n",
      "Iteration 156, loss = 0.01790549\n",
      "Iteration 157, loss = 0.01730371\n",
      "Iteration 158, loss = 0.01712901\n",
      "Iteration 159, loss = 0.01690513\n",
      "Iteration 160, loss = 0.01640927\n",
      "Iteration 161, loss = 0.01606307\n",
      "Iteration 162, loss = 0.01576261\n",
      "Iteration 163, loss = 0.01564163\n",
      "Iteration 164, loss = 0.01569624\n",
      "Iteration 165, loss = 0.01554549\n",
      "Iteration 166, loss = 0.01531912\n",
      "Iteration 167, loss = 0.01524874\n",
      "Iteration 168, loss = 0.01530374\n",
      "Iteration 169, loss = 0.01525674\n",
      "Iteration 170, loss = 0.01468475\n",
      "Iteration 171, loss = 0.01494294\n",
      "Iteration 172, loss = 0.01482253\n",
      "Iteration 173, loss = 0.01448413\n",
      "Iteration 174, loss = 0.01427634\n",
      "Iteration 175, loss = 0.01396359\n",
      "Iteration 176, loss = 0.01391546\n",
      "Iteration 177, loss = 0.01368450\n",
      "Iteration 178, loss = 0.01359120\n",
      "Iteration 179, loss = 0.01336736\n",
      "Iteration 180, loss = 0.01314214\n",
      "Iteration 181, loss = 0.01328958\n",
      "Iteration 182, loss = 0.01328142\n",
      "Iteration 183, loss = 0.01334836\n",
      "Iteration 184, loss = 0.01279805\n",
      "Iteration 185, loss = 0.01303505\n",
      "Iteration 186, loss = 0.01379968\n",
      "Iteration 187, loss = 0.01332121\n",
      "Iteration 188, loss = 0.01330955\n",
      "Iteration 189, loss = 0.01236013\n",
      "Iteration 190, loss = 0.01234077\n",
      "Iteration 191, loss = 0.01214391\n",
      "Iteration 192, loss = 0.01176390\n",
      "Iteration 193, loss = 0.01180869\n",
      "Iteration 194, loss = 0.01156655\n",
      "Iteration 195, loss = 0.01154612\n",
      "Iteration 196, loss = 0.01125175\n",
      "Iteration 197, loss = 0.01139783\n",
      "Iteration 198, loss = 0.01148398\n",
      "Iteration 199, loss = 0.01118250\n",
      "Iteration 200, loss = 0.01125747\n",
      "Iteration 201, loss = 0.01106007\n",
      "Iteration 202, loss = 0.01074721\n",
      "Iteration 203, loss = 0.01057621\n",
      "Iteration 204, loss = 0.01049300\n",
      "Iteration 205, loss = 0.01076881\n",
      "Iteration 206, loss = 0.01046152\n",
      "Iteration 207, loss = 0.01037400\n",
      "Iteration 208, loss = 0.01015867\n",
      "Iteration 209, loss = 0.00989364\n",
      "Iteration 210, loss = 0.00992379\n",
      "Iteration 211, loss = 0.00999517\n",
      "Iteration 212, loss = 0.00970690\n",
      "Iteration 213, loss = 0.00952339\n",
      "Iteration 214, loss = 0.00934252\n",
      "Iteration 215, loss = 0.00922054\n",
      "Iteration 216, loss = 0.00917102\n",
      "Iteration 217, loss = 0.00921883\n",
      "Iteration 218, loss = 0.00922568\n",
      "Iteration 219, loss = 0.00897204\n",
      "Iteration 220, loss = 0.00892582\n",
      "Iteration 221, loss = 0.00868717\n",
      "Iteration 222, loss = 0.00907461\n",
      "Iteration 223, loss = 0.00850333\n",
      "Iteration 224, loss = 0.00907073\n",
      "Iteration 225, loss = 0.00870376\n",
      "Iteration 226, loss = 0.00849958\n",
      "Iteration 227, loss = 0.00860471\n",
      "Iteration 228, loss = 0.00806493\n",
      "Iteration 229, loss = 0.00835034\n",
      "Iteration 230, loss = 0.00787920\n",
      "Iteration 231, loss = 0.00876730\n",
      "Iteration 232, loss = 0.00855889\n",
      "Iteration 233, loss = 0.00814342\n",
      "Iteration 234, loss = 0.00826162\n",
      "Iteration 235, loss = 0.00762258\n",
      "Iteration 236, loss = 0.00849276\n",
      "Iteration 237, loss = 0.00752377\n",
      "Iteration 238, loss = 0.00847758\n",
      "Iteration 239, loss = 0.00716293\n",
      "Iteration 240, loss = 0.00892102\n",
      "Iteration 241, loss = 0.00776661\n",
      "Iteration 242, loss = 0.00799764\n",
      "Iteration 243, loss = 0.00732995\n",
      "Iteration 244, loss = 0.00728023\n",
      "Iteration 245, loss = 0.00734337\n",
      "Iteration 246, loss = 0.00743815\n",
      "Iteration 247, loss = 0.00784897\n",
      "Iteration 248, loss = 0.00711893\n",
      "Iteration 249, loss = 0.00737682\n",
      "Iteration 250, loss = 0.00667400\n",
      "Iteration 251, loss = 0.00671216\n",
      "Iteration 252, loss = 0.00638626\n",
      "Iteration 253, loss = 0.00646659\n",
      "Iteration 254, loss = 0.00632347\n",
      "Iteration 255, loss = 0.00680107\n",
      "Iteration 256, loss = 0.00633401\n",
      "Iteration 257, loss = 0.00649335\n",
      "Iteration 258, loss = 0.00627027\n",
      "Iteration 259, loss = 0.00630019\n",
      "Iteration 260, loss = 0.00587451\n",
      "Iteration 261, loss = 0.00640742\n",
      "Iteration 262, loss = 0.00596993\n",
      "Iteration 263, loss = 0.00608964\n",
      "Iteration 264, loss = 0.00622739\n",
      "Iteration 265, loss = 0.00605311\n",
      "Iteration 266, loss = 0.00619427\n",
      "Iteration 267, loss = 0.00586979\n",
      "Iteration 268, loss = 0.00574231\n",
      "Iteration 269, loss = 0.00559329\n",
      "Iteration 270, loss = 0.00569399\n",
      "Iteration 271, loss = 0.00611465\n",
      "Iteration 272, loss = 0.00549531\n",
      "Iteration 273, loss = 0.00548639\n",
      "Iteration 274, loss = 0.00559587\n",
      "Iteration 275, loss = 0.00528665\n",
      "Iteration 276, loss = 0.00541574\n",
      "Iteration 277, loss = 0.00524700\n",
      "Iteration 278, loss = 0.00509201\n",
      "Iteration 279, loss = 0.00515793\n",
      "Iteration 280, loss = 0.00508620\n",
      "Iteration 281, loss = 0.00508526\n",
      "Iteration 282, loss = 0.00489624\n",
      "Iteration 283, loss = 0.00487372\n",
      "Iteration 284, loss = 0.00486211\n",
      "Iteration 285, loss = 0.00473141\n",
      "Iteration 286, loss = 0.00488338\n",
      "Iteration 287, loss = 0.00470799\n",
      "Iteration 288, loss = 0.00482457\n",
      "Iteration 289, loss = 0.00460568\n",
      "Iteration 290, loss = 0.00468450\n",
      "Iteration 291, loss = 0.00445949\n",
      "Iteration 292, loss = 0.00454881\n",
      "Iteration 293, loss = 0.00434905\n",
      "Iteration 294, loss = 0.00460950\n",
      "Iteration 295, loss = 0.00437259\n",
      "Iteration 296, loss = 0.00443076\n",
      "Iteration 297, loss = 0.00423734\n",
      "Iteration 298, loss = 0.00468013\n",
      "Iteration 299, loss = 0.00446325\n",
      "Iteration 300, loss = 0.00416317\n",
      "Iteration 301, loss = 0.00416800\n",
      "Iteration 302, loss = 0.00443143\n",
      "Iteration 303, loss = 0.00425742\n",
      "Iteration 304, loss = 0.00419421\n",
      "Iteration 305, loss = 0.00419810\n",
      "Iteration 306, loss = 0.00409299\n",
      "Iteration 307, loss = 0.00403482\n",
      "Iteration 308, loss = 0.00407428\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45388742\n",
      "Iteration 2, loss = 0.40775030\n",
      "Iteration 3, loss = 0.36470608\n",
      "Iteration 4, loss = 0.33666004\n",
      "Iteration 5, loss = 0.31226590\n",
      "Iteration 6, loss = 0.28898818\n",
      "Iteration 7, loss = 0.27286746\n",
      "Iteration 8, loss = 0.25645262\n",
      "Iteration 9, loss = 0.24217191\n",
      "Iteration 10, loss = 0.22713330\n",
      "Iteration 11, loss = 0.21224367\n",
      "Iteration 12, loss = 0.19880992\n",
      "Iteration 13, loss = 0.18725554\n",
      "Iteration 14, loss = 0.17442093\n",
      "Iteration 15, loss = 0.16520951\n",
      "Iteration 16, loss = 0.15519994\n",
      "Iteration 17, loss = 0.14644393\n",
      "Iteration 18, loss = 0.13831754\n",
      "Iteration 19, loss = 0.13172364\n",
      "Iteration 20, loss = 0.12462424\n",
      "Iteration 21, loss = 0.11872651\n",
      "Iteration 22, loss = 0.11257349\n",
      "Iteration 23, loss = 0.11012403\n",
      "Iteration 24, loss = 0.10397112\n",
      "Iteration 25, loss = 0.10039971\n",
      "Iteration 26, loss = 0.09550937\n",
      "Iteration 27, loss = 0.09265748\n",
      "Iteration 28, loss = 0.08956232\n",
      "Iteration 29, loss = 0.08616460\n",
      "Iteration 30, loss = 0.08304910\n",
      "Iteration 31, loss = 0.08134547\n",
      "Iteration 32, loss = 0.07879938\n",
      "Iteration 33, loss = 0.07654697\n",
      "Iteration 34, loss = 0.07408257\n",
      "Iteration 35, loss = 0.07219528\n",
      "Iteration 36, loss = 0.07039425\n",
      "Iteration 37, loss = 0.06835300\n",
      "Iteration 38, loss = 0.06635793\n",
      "Iteration 39, loss = 0.06453248\n",
      "Iteration 40, loss = 0.06374250\n",
      "Iteration 41, loss = 0.06155533\n",
      "Iteration 42, loss = 0.05987650\n",
      "Iteration 43, loss = 0.05856395\n",
      "Iteration 44, loss = 0.05749487\n",
      "Iteration 45, loss = 0.05576196\n",
      "Iteration 46, loss = 0.05442038\n",
      "Iteration 47, loss = 0.05324761\n",
      "Iteration 48, loss = 0.05237036\n",
      "Iteration 49, loss = 0.05127277\n",
      "Iteration 50, loss = 0.05069749\n",
      "Iteration 51, loss = 0.04919767\n",
      "Iteration 52, loss = 0.04846216\n",
      "Iteration 53, loss = 0.04754615\n",
      "Iteration 54, loss = 0.04664289\n",
      "Iteration 55, loss = 0.04579823\n",
      "Iteration 56, loss = 0.04537036\n",
      "Iteration 57, loss = 0.04408543\n",
      "Iteration 58, loss = 0.04358448\n",
      "Iteration 59, loss = 0.04285776\n",
      "Iteration 60, loss = 0.04199157\n",
      "Iteration 61, loss = 0.04133397\n",
      "Iteration 62, loss = 0.04118276\n",
      "Iteration 63, loss = 0.03979113\n",
      "Iteration 64, loss = 0.03923753\n",
      "Iteration 65, loss = 0.03893701\n",
      "Iteration 66, loss = 0.03808532\n",
      "Iteration 67, loss = 0.03807479\n",
      "Iteration 68, loss = 0.03681906\n",
      "Iteration 69, loss = 0.03685411\n",
      "Iteration 70, loss = 0.03613349\n",
      "Iteration 71, loss = 0.03521028\n",
      "Iteration 72, loss = 0.03462221\n",
      "Iteration 73, loss = 0.03410990\n",
      "Iteration 74, loss = 0.03379402\n",
      "Iteration 75, loss = 0.03300643\n",
      "Iteration 76, loss = 0.03267136\n",
      "Iteration 77, loss = 0.03203990\n",
      "Iteration 78, loss = 0.03192920\n",
      "Iteration 79, loss = 0.03104857\n",
      "Iteration 80, loss = 0.03104872\n",
      "Iteration 81, loss = 0.03040125\n",
      "Iteration 82, loss = 0.02992454\n",
      "Iteration 83, loss = 0.02946639\n",
      "Iteration 84, loss = 0.02907217\n",
      "Iteration 85, loss = 0.02878105\n",
      "Iteration 86, loss = 0.02834048\n",
      "Iteration 87, loss = 0.02798623\n",
      "Iteration 88, loss = 0.02786084\n",
      "Iteration 89, loss = 0.02727197\n",
      "Iteration 90, loss = 0.02689351\n",
      "Iteration 91, loss = 0.02647436\n",
      "Iteration 92, loss = 0.02597570\n",
      "Iteration 93, loss = 0.02579256\n",
      "Iteration 94, loss = 0.02617132\n",
      "Iteration 95, loss = 0.02508738\n",
      "Iteration 96, loss = 0.02472861\n",
      "Iteration 97, loss = 0.02428083\n",
      "Iteration 98, loss = 0.02384081\n",
      "Iteration 99, loss = 0.02376031\n",
      "Iteration 100, loss = 0.02317211\n",
      "Iteration 101, loss = 0.02305204\n",
      "Iteration 102, loss = 0.02275883\n",
      "Iteration 103, loss = 0.02250118\n",
      "Iteration 104, loss = 0.02217171\n",
      "Iteration 105, loss = 0.02208151\n",
      "Iteration 106, loss = 0.02154109\n",
      "Iteration 107, loss = 0.02116073\n",
      "Iteration 108, loss = 0.02095309\n",
      "Iteration 109, loss = 0.02068737\n",
      "Iteration 110, loss = 0.02038467\n",
      "Iteration 111, loss = 0.02001322\n",
      "Iteration 112, loss = 0.02011915\n",
      "Iteration 113, loss = 0.01969574\n",
      "Iteration 114, loss = 0.01926627\n",
      "Iteration 115, loss = 0.01940564\n",
      "Iteration 116, loss = 0.01908960\n",
      "Iteration 117, loss = 0.01883006\n",
      "Iteration 118, loss = 0.01842577\n",
      "Iteration 119, loss = 0.01808542\n",
      "Iteration 120, loss = 0.01800356\n",
      "Iteration 121, loss = 0.01785767\n",
      "Iteration 122, loss = 0.01758836\n",
      "Iteration 123, loss = 0.01743136\n",
      "Iteration 124, loss = 0.01704553\n",
      "Iteration 125, loss = 0.01707886\n",
      "Iteration 126, loss = 0.01669362\n",
      "Iteration 127, loss = 0.01670048\n",
      "Iteration 128, loss = 0.01607869\n",
      "Iteration 129, loss = 0.01587458\n",
      "Iteration 130, loss = 0.01569028\n",
      "Iteration 131, loss = 0.01566678\n",
      "Iteration 132, loss = 0.01552789\n",
      "Iteration 133, loss = 0.01526913\n",
      "Iteration 134, loss = 0.01514659\n",
      "Iteration 135, loss = 0.01474630\n",
      "Iteration 136, loss = 0.01468026\n",
      "Iteration 137, loss = 0.01423257\n",
      "Iteration 138, loss = 0.01435505\n",
      "Iteration 139, loss = 0.01392167\n",
      "Iteration 140, loss = 0.01397675\n",
      "Iteration 141, loss = 0.01379623\n",
      "Iteration 142, loss = 0.01383636\n",
      "Iteration 143, loss = 0.01335127\n",
      "Iteration 144, loss = 0.01311034\n",
      "Iteration 145, loss = 0.01330097\n",
      "Iteration 146, loss = 0.01316766\n",
      "Iteration 147, loss = 0.01275546\n",
      "Iteration 148, loss = 0.01270369\n",
      "Iteration 149, loss = 0.01223730\n",
      "Iteration 150, loss = 0.01240772\n",
      "Iteration 151, loss = 0.01227030\n",
      "Iteration 152, loss = 0.01179270\n",
      "Iteration 153, loss = 0.01175402\n",
      "Iteration 154, loss = 0.01143200\n",
      "Iteration 155, loss = 0.01136589\n",
      "Iteration 156, loss = 0.01121297\n",
      "Iteration 157, loss = 0.01120744\n",
      "Iteration 158, loss = 0.01098440\n",
      "Iteration 159, loss = 0.01072516\n",
      "Iteration 160, loss = 0.01073324\n",
      "Iteration 161, loss = 0.01061515\n",
      "Iteration 162, loss = 0.01034755\n",
      "Iteration 163, loss = 0.01040000\n",
      "Iteration 164, loss = 0.01014931\n",
      "Iteration 165, loss = 0.01001049\n",
      "Iteration 166, loss = 0.00988040\n",
      "Iteration 167, loss = 0.00982485\n",
      "Iteration 168, loss = 0.00966375\n",
      "Iteration 169, loss = 0.00955487\n",
      "Iteration 170, loss = 0.00947376\n",
      "Iteration 171, loss = 0.00931527\n",
      "Iteration 172, loss = 0.00919618\n",
      "Iteration 173, loss = 0.00915627\n",
      "Iteration 174, loss = 0.00900358\n",
      "Iteration 175, loss = 0.00904418\n",
      "Iteration 176, loss = 0.00884266\n",
      "Iteration 177, loss = 0.00876857\n",
      "Iteration 178, loss = 0.00870334\n",
      "Iteration 179, loss = 0.00870293\n",
      "Iteration 180, loss = 0.00853034\n",
      "Iteration 181, loss = 0.00841431\n",
      "Iteration 182, loss = 0.00822682\n",
      "Iteration 183, loss = 0.00817040\n",
      "Iteration 184, loss = 0.00813483\n",
      "Iteration 185, loss = 0.00808392\n",
      "Iteration 186, loss = 0.00795869\n",
      "Iteration 187, loss = 0.00784388\n",
      "Iteration 188, loss = 0.00787799\n",
      "Iteration 189, loss = 0.00778979\n",
      "Iteration 190, loss = 0.00778652\n",
      "Iteration 191, loss = 0.00771421\n",
      "Iteration 192, loss = 0.00764671\n",
      "Iteration 193, loss = 0.00752237\n",
      "Iteration 194, loss = 0.00735708\n",
      "Iteration 195, loss = 0.00736121\n",
      "Iteration 196, loss = 0.00719610\n",
      "Iteration 197, loss = 0.00727310\n",
      "Iteration 198, loss = 0.00701819\n",
      "Iteration 199, loss = 0.00698141\n",
      "Iteration 200, loss = 0.00696429\n",
      "Iteration 201, loss = 0.00684928\n",
      "Iteration 202, loss = 0.00682449\n",
      "Iteration 203, loss = 0.00669077\n",
      "Iteration 204, loss = 0.00661620\n",
      "Iteration 205, loss = 0.00662946\n",
      "Iteration 206, loss = 0.00645596\n",
      "Iteration 207, loss = 0.00644453\n",
      "Iteration 208, loss = 0.00641848\n",
      "Iteration 209, loss = 0.00643183\n",
      "Iteration 210, loss = 0.00624052\n",
      "Iteration 211, loss = 0.00628067\n",
      "Iteration 212, loss = 0.00618405\n",
      "Iteration 213, loss = 0.00609973\n",
      "Iteration 214, loss = 0.00608002\n",
      "Iteration 215, loss = 0.00615919\n",
      "Iteration 216, loss = 0.00594539\n",
      "Iteration 217, loss = 0.00607640\n",
      "Iteration 218, loss = 0.00590389\n",
      "Iteration 219, loss = 0.00583377\n",
      "Iteration 220, loss = 0.00592327\n",
      "Iteration 221, loss = 0.00558934\n",
      "Iteration 222, loss = 0.00583316\n",
      "Iteration 223, loss = 0.00569252\n",
      "Iteration 224, loss = 0.00555260\n",
      "Iteration 225, loss = 0.00580854\n",
      "Iteration 226, loss = 0.00568508\n",
      "Iteration 227, loss = 0.00544534\n",
      "Iteration 228, loss = 0.00577000\n",
      "Iteration 229, loss = 0.00541395\n",
      "Iteration 230, loss = 0.00546052\n",
      "Iteration 231, loss = 0.00534045\n",
      "Iteration 232, loss = 0.00517751\n",
      "Iteration 233, loss = 0.00543101\n",
      "Iteration 234, loss = 0.00521276\n",
      "Iteration 235, loss = 0.00518754\n",
      "Iteration 236, loss = 0.00510676\n",
      "Iteration 237, loss = 0.00498376\n",
      "Iteration 238, loss = 0.00496459\n",
      "Iteration 239, loss = 0.00502699\n",
      "Iteration 240, loss = 0.00502866\n",
      "Iteration 241, loss = 0.00485527\n",
      "Iteration 242, loss = 0.00484334\n",
      "Iteration 243, loss = 0.00464094\n",
      "Iteration 244, loss = 0.00466807\n",
      "Iteration 245, loss = 0.00463989\n",
      "Iteration 246, loss = 0.00457989\n",
      "Iteration 247, loss = 0.00452340\n",
      "Iteration 248, loss = 0.00451198\n",
      "Iteration 249, loss = 0.00442364\n",
      "Iteration 250, loss = 0.00450443\n",
      "Iteration 251, loss = 0.00443378\n",
      "Iteration 252, loss = 0.00432851\n",
      "Iteration 253, loss = 0.00438700\n",
      "Iteration 254, loss = 0.00427666\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45388742\n",
      "Iteration 2, loss = 0.40775030\n",
      "Iteration 3, loss = 0.36470608\n",
      "Iteration 4, loss = 0.33666004\n",
      "Iteration 5, loss = 0.31226590\n",
      "Iteration 6, loss = 0.28898818\n",
      "Iteration 7, loss = 0.27286746\n",
      "Iteration 8, loss = 0.25645262\n",
      "Iteration 9, loss = 0.24217191\n",
      "Iteration 10, loss = 0.22713330\n",
      "Iteration 11, loss = 0.21224367\n",
      "Iteration 12, loss = 0.19880992\n",
      "Iteration 13, loss = 0.18725554\n",
      "Iteration 14, loss = 0.17442093\n",
      "Iteration 15, loss = 0.16520951\n",
      "Iteration 16, loss = 0.15519994\n",
      "Iteration 17, loss = 0.14644393\n",
      "Iteration 18, loss = 0.13831754\n",
      "Iteration 19, loss = 0.13172364\n",
      "Iteration 20, loss = 0.12462424\n",
      "Iteration 21, loss = 0.11872651\n",
      "Iteration 22, loss = 0.11257349\n",
      "Iteration 23, loss = 0.11012403\n",
      "Iteration 24, loss = 0.10397112\n",
      "Iteration 25, loss = 0.10039971\n",
      "Iteration 26, loss = 0.09550937\n",
      "Iteration 27, loss = 0.09265748\n",
      "Iteration 28, loss = 0.08956232\n",
      "Iteration 29, loss = 0.08616460\n",
      "Iteration 30, loss = 0.08304910\n",
      "Iteration 31, loss = 0.08134547\n",
      "Iteration 32, loss = 0.07879938\n",
      "Iteration 33, loss = 0.07654697\n",
      "Iteration 34, loss = 0.07408257\n",
      "Iteration 35, loss = 0.07219528\n",
      "Iteration 36, loss = 0.07039425\n",
      "Iteration 37, loss = 0.06835300\n",
      "Iteration 38, loss = 0.06635793\n",
      "Iteration 39, loss = 0.06453248\n",
      "Iteration 40, loss = 0.06374250\n",
      "Iteration 41, loss = 0.06155533\n",
      "Iteration 42, loss = 0.05987650\n",
      "Iteration 43, loss = 0.05856395\n",
      "Iteration 44, loss = 0.05749487\n",
      "Iteration 45, loss = 0.05576196\n",
      "Iteration 46, loss = 0.05442038\n",
      "Iteration 47, loss = 0.05324761\n",
      "Iteration 48, loss = 0.05237036\n",
      "Iteration 49, loss = 0.05127277\n",
      "Iteration 50, loss = 0.05069749\n",
      "Iteration 51, loss = 0.04919767\n",
      "Iteration 52, loss = 0.04846216\n",
      "Iteration 53, loss = 0.04754615\n",
      "Iteration 54, loss = 0.04664289\n",
      "Iteration 55, loss = 0.04579823\n",
      "Iteration 56, loss = 0.04537036\n",
      "Iteration 57, loss = 0.04408543\n",
      "Iteration 58, loss = 0.04358448\n",
      "Iteration 59, loss = 0.04285776\n",
      "Iteration 60, loss = 0.04199157\n",
      "Iteration 61, loss = 0.04133397\n",
      "Iteration 62, loss = 0.04118276\n",
      "Iteration 63, loss = 0.03979113\n",
      "Iteration 64, loss = 0.03923753\n",
      "Iteration 65, loss = 0.03893701\n",
      "Iteration 66, loss = 0.03808532\n",
      "Iteration 67, loss = 0.03807479\n",
      "Iteration 68, loss = 0.03681906\n",
      "Iteration 69, loss = 0.03685411\n",
      "Iteration 70, loss = 0.03613349\n",
      "Iteration 71, loss = 0.03521028\n",
      "Iteration 72, loss = 0.03462221\n",
      "Iteration 73, loss = 0.03410990\n",
      "Iteration 74, loss = 0.03379402\n",
      "Iteration 75, loss = 0.03300643\n",
      "Iteration 76, loss = 0.03267136\n",
      "Iteration 77, loss = 0.03203990\n",
      "Iteration 78, loss = 0.03192920\n",
      "Iteration 79, loss = 0.03104857\n",
      "Iteration 80, loss = 0.03104872\n",
      "Iteration 81, loss = 0.03040125\n",
      "Iteration 82, loss = 0.02992454\n",
      "Iteration 83, loss = 0.02946639\n",
      "Iteration 84, loss = 0.02907217\n",
      "Iteration 85, loss = 0.02878105\n",
      "Iteration 86, loss = 0.02834048\n",
      "Iteration 87, loss = 0.02798623\n",
      "Iteration 88, loss = 0.02786084\n",
      "Iteration 89, loss = 0.02727197\n",
      "Iteration 90, loss = 0.02689351\n",
      "Iteration 91, loss = 0.02647436\n",
      "Iteration 92, loss = 0.02597570\n",
      "Iteration 93, loss = 0.02579256\n",
      "Iteration 94, loss = 0.02617132\n",
      "Iteration 95, loss = 0.02508738\n",
      "Iteration 96, loss = 0.02472861\n",
      "Iteration 97, loss = 0.02428083\n",
      "Iteration 98, loss = 0.02384081\n",
      "Iteration 99, loss = 0.02376031\n",
      "Iteration 100, loss = 0.02317211\n",
      "Iteration 101, loss = 0.02305204\n",
      "Iteration 102, loss = 0.02275883\n",
      "Iteration 103, loss = 0.02250118\n",
      "Iteration 104, loss = 0.02217171\n",
      "Iteration 105, loss = 0.02208151\n",
      "Iteration 106, loss = 0.02154109\n",
      "Iteration 107, loss = 0.02116073\n",
      "Iteration 108, loss = 0.02095309\n",
      "Iteration 109, loss = 0.02068737\n",
      "Iteration 110, loss = 0.02038467\n",
      "Iteration 111, loss = 0.02001322\n",
      "Iteration 112, loss = 0.02011915\n",
      "Iteration 113, loss = 0.01969574\n",
      "Iteration 114, loss = 0.01926627\n",
      "Iteration 115, loss = 0.01940564\n",
      "Iteration 116, loss = 0.01908960\n",
      "Iteration 117, loss = 0.01883006\n",
      "Iteration 118, loss = 0.01842577\n",
      "Iteration 119, loss = 0.01808542\n",
      "Iteration 120, loss = 0.01800356\n",
      "Iteration 121, loss = 0.01785767\n",
      "Iteration 122, loss = 0.01758836\n",
      "Iteration 123, loss = 0.01743136\n",
      "Iteration 124, loss = 0.01704553\n",
      "Iteration 125, loss = 0.01707886\n",
      "Iteration 126, loss = 0.01669362\n",
      "Iteration 127, loss = 0.01670048\n",
      "Iteration 128, loss = 0.01607869\n",
      "Iteration 129, loss = 0.01587458\n",
      "Iteration 130, loss = 0.01569028\n",
      "Iteration 131, loss = 0.01566678\n",
      "Iteration 132, loss = 0.01552789\n",
      "Iteration 133, loss = 0.01526913\n",
      "Iteration 134, loss = 0.01514659\n",
      "Iteration 135, loss = 0.01474630\n",
      "Iteration 136, loss = 0.01468026\n",
      "Iteration 137, loss = 0.01423257\n",
      "Iteration 138, loss = 0.01435505\n",
      "Iteration 139, loss = 0.01392167\n",
      "Iteration 140, loss = 0.01397675\n",
      "Iteration 141, loss = 0.01379623\n",
      "Iteration 142, loss = 0.01383636\n",
      "Iteration 143, loss = 0.01335127\n",
      "Iteration 144, loss = 0.01311034\n",
      "Iteration 145, loss = 0.01330097\n",
      "Iteration 146, loss = 0.01316766\n",
      "Iteration 147, loss = 0.01275546\n",
      "Iteration 148, loss = 0.01270369\n",
      "Iteration 149, loss = 0.01223730\n",
      "Iteration 150, loss = 0.01240772\n",
      "Iteration 151, loss = 0.01227030\n",
      "Iteration 152, loss = 0.01179270\n",
      "Iteration 153, loss = 0.01175402\n",
      "Iteration 154, loss = 0.01143200\n",
      "Iteration 155, loss = 0.01136589\n",
      "Iteration 156, loss = 0.01121297\n",
      "Iteration 157, loss = 0.01120744\n",
      "Iteration 158, loss = 0.01098440\n",
      "Iteration 159, loss = 0.01072516\n",
      "Iteration 160, loss = 0.01073324\n",
      "Iteration 161, loss = 0.01061515\n",
      "Iteration 162, loss = 0.01034755\n",
      "Iteration 163, loss = 0.01040000\n",
      "Iteration 164, loss = 0.01014931\n",
      "Iteration 165, loss = 0.01001049\n",
      "Iteration 166, loss = 0.00988040\n",
      "Iteration 167, loss = 0.00982485\n",
      "Iteration 168, loss = 0.00966375\n",
      "Iteration 169, loss = 0.00955487\n",
      "Iteration 170, loss = 0.00947376\n",
      "Iteration 171, loss = 0.00931527\n",
      "Iteration 172, loss = 0.00919618\n",
      "Iteration 173, loss = 0.00915627\n",
      "Iteration 174, loss = 0.00900358\n",
      "Iteration 175, loss = 0.00904418\n",
      "Iteration 176, loss = 0.00884266\n",
      "Iteration 177, loss = 0.00876857\n",
      "Iteration 178, loss = 0.00870334\n",
      "Iteration 179, loss = 0.00870293\n",
      "Iteration 180, loss = 0.00853034\n",
      "Iteration 181, loss = 0.00841431\n",
      "Iteration 182, loss = 0.00822682\n",
      "Iteration 183, loss = 0.00817040\n",
      "Iteration 184, loss = 0.00813483\n",
      "Iteration 185, loss = 0.00808392\n",
      "Iteration 186, loss = 0.00795869\n",
      "Iteration 187, loss = 0.00784388\n",
      "Iteration 188, loss = 0.00787799\n",
      "Iteration 189, loss = 0.00778979\n",
      "Iteration 190, loss = 0.00778652\n",
      "Iteration 191, loss = 0.00771421\n",
      "Iteration 192, loss = 0.00764671\n",
      "Iteration 193, loss = 0.00752237\n",
      "Iteration 194, loss = 0.00735708\n",
      "Iteration 195, loss = 0.00736121\n",
      "Iteration 196, loss = 0.00719610\n",
      "Iteration 197, loss = 0.00727310\n",
      "Iteration 198, loss = 0.00701819\n",
      "Iteration 199, loss = 0.00698141\n",
      "Iteration 200, loss = 0.00696429\n",
      "Iteration 201, loss = 0.00684928\n",
      "Iteration 202, loss = 0.00682449\n",
      "Iteration 203, loss = 0.00669077\n",
      "Iteration 204, loss = 0.00661620\n",
      "Iteration 205, loss = 0.00662946\n",
      "Iteration 206, loss = 0.00645596\n",
      "Iteration 207, loss = 0.00644453\n",
      "Iteration 208, loss = 0.00641848\n",
      "Iteration 209, loss = 0.00643183\n",
      "Iteration 210, loss = 0.00624052\n",
      "Iteration 211, loss = 0.00628067\n",
      "Iteration 212, loss = 0.00618405\n",
      "Iteration 213, loss = 0.00609973\n",
      "Iteration 214, loss = 0.00608002\n",
      "Iteration 215, loss = 0.00615919\n",
      "Iteration 216, loss = 0.00594539\n",
      "Iteration 217, loss = 0.00607640\n",
      "Iteration 218, loss = 0.00590389\n",
      "Iteration 219, loss = 0.00583377\n",
      "Iteration 220, loss = 0.00592327\n",
      "Iteration 221, loss = 0.00558934\n",
      "Iteration 222, loss = 0.00583316\n",
      "Iteration 223, loss = 0.00569252\n",
      "Iteration 224, loss = 0.00555260\n",
      "Iteration 225, loss = 0.00580854\n",
      "Iteration 226, loss = 0.00568508\n",
      "Iteration 227, loss = 0.00544534\n",
      "Iteration 228, loss = 0.00577000\n",
      "Iteration 229, loss = 0.00541395\n",
      "Iteration 230, loss = 0.00546052\n",
      "Iteration 231, loss = 0.00534045\n",
      "Iteration 232, loss = 0.00517751\n",
      "Iteration 233, loss = 0.00543101\n",
      "Iteration 234, loss = 0.00521276\n",
      "Iteration 235, loss = 0.00518754\n",
      "Iteration 236, loss = 0.00510676\n",
      "Iteration 237, loss = 0.00498376\n",
      "Iteration 238, loss = 0.00496459\n",
      "Iteration 239, loss = 0.00502699\n",
      "Iteration 240, loss = 0.00502866\n",
      "Iteration 241, loss = 0.00485527\n",
      "Iteration 242, loss = 0.00484334\n",
      "Iteration 243, loss = 0.00464094\n",
      "Iteration 244, loss = 0.00466807\n",
      "Iteration 245, loss = 0.00463989\n",
      "Iteration 246, loss = 0.00457989\n",
      "Iteration 247, loss = 0.00452340\n",
      "Iteration 248, loss = 0.00451198\n",
      "Iteration 249, loss = 0.00442364\n",
      "Iteration 250, loss = 0.00450443\n",
      "Iteration 251, loss = 0.00443378\n",
      "Iteration 252, loss = 0.00432851\n",
      "Iteration 253, loss = 0.00438700\n",
      "Iteration 254, loss = 0.00427666\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45388742\n",
      "Iteration 2, loss = 0.40775030\n",
      "Iteration 3, loss = 0.36470608\n",
      "Iteration 4, loss = 0.33666004\n",
      "Iteration 5, loss = 0.31226590\n",
      "Iteration 6, loss = 0.28898818\n",
      "Iteration 7, loss = 0.27286746\n",
      "Iteration 8, loss = 0.25645262\n",
      "Iteration 9, loss = 0.24217191\n",
      "Iteration 10, loss = 0.22713330\n",
      "Iteration 11, loss = 0.21224367\n",
      "Iteration 12, loss = 0.19880992\n",
      "Iteration 13, loss = 0.18725554\n",
      "Iteration 14, loss = 0.17442093\n",
      "Iteration 15, loss = 0.16520951\n",
      "Iteration 16, loss = 0.15519994\n",
      "Iteration 17, loss = 0.14644393\n",
      "Iteration 18, loss = 0.13831754\n",
      "Iteration 19, loss = 0.13172364\n",
      "Iteration 20, loss = 0.12462424\n",
      "Iteration 21, loss = 0.11872651\n",
      "Iteration 22, loss = 0.11257349\n",
      "Iteration 23, loss = 0.11012403\n",
      "Iteration 24, loss = 0.10397112\n",
      "Iteration 25, loss = 0.10039971\n",
      "Iteration 26, loss = 0.09550937\n",
      "Iteration 27, loss = 0.09265748\n",
      "Iteration 28, loss = 0.08956232\n",
      "Iteration 29, loss = 0.08616460\n",
      "Iteration 30, loss = 0.08304910\n",
      "Iteration 31, loss = 0.08134547\n",
      "Iteration 32, loss = 0.07879938\n",
      "Iteration 33, loss = 0.07654697\n",
      "Iteration 34, loss = 0.07408257\n",
      "Iteration 35, loss = 0.07219528\n",
      "Iteration 36, loss = 0.07039425\n",
      "Iteration 37, loss = 0.06835300\n",
      "Iteration 38, loss = 0.06635793\n",
      "Iteration 39, loss = 0.06453248\n",
      "Iteration 40, loss = 0.06374250\n",
      "Iteration 41, loss = 0.06155533\n",
      "Iteration 42, loss = 0.05987650\n",
      "Iteration 43, loss = 0.05856395\n",
      "Iteration 44, loss = 0.05749487\n",
      "Iteration 45, loss = 0.05576196\n",
      "Iteration 46, loss = 0.05442038\n",
      "Iteration 47, loss = 0.05324761\n",
      "Iteration 48, loss = 0.05237036\n",
      "Iteration 49, loss = 0.05127277\n",
      "Iteration 50, loss = 0.05069749\n",
      "Iteration 51, loss = 0.04919767\n",
      "Iteration 52, loss = 0.04846216\n",
      "Iteration 53, loss = 0.04754615\n",
      "Iteration 54, loss = 0.04664289\n",
      "Iteration 55, loss = 0.04579823\n",
      "Iteration 56, loss = 0.04537036\n",
      "Iteration 57, loss = 0.04408543\n",
      "Iteration 58, loss = 0.04358448\n",
      "Iteration 59, loss = 0.04285776\n",
      "Iteration 60, loss = 0.04199157\n",
      "Iteration 61, loss = 0.04133397\n",
      "Iteration 62, loss = 0.04118276\n",
      "Iteration 63, loss = 0.03979113\n",
      "Iteration 64, loss = 0.03923753\n",
      "Iteration 65, loss = 0.03893701\n",
      "Iteration 66, loss = 0.03808532\n",
      "Iteration 67, loss = 0.03807479\n",
      "Iteration 68, loss = 0.03681906\n",
      "Iteration 69, loss = 0.03685411\n",
      "Iteration 70, loss = 0.03613349\n",
      "Iteration 71, loss = 0.03521028\n",
      "Iteration 72, loss = 0.03462221\n",
      "Iteration 73, loss = 0.03410990\n",
      "Iteration 74, loss = 0.03379402\n",
      "Iteration 75, loss = 0.03300643\n",
      "Iteration 76, loss = 0.03267136\n",
      "Iteration 77, loss = 0.03203990\n",
      "Iteration 78, loss = 0.03192920\n",
      "Iteration 79, loss = 0.03104857\n",
      "Iteration 80, loss = 0.03104872\n",
      "Iteration 81, loss = 0.03040125\n",
      "Iteration 82, loss = 0.02992454\n",
      "Iteration 83, loss = 0.02946639\n",
      "Iteration 84, loss = 0.02907217\n",
      "Iteration 85, loss = 0.02878105\n",
      "Iteration 86, loss = 0.02834048\n",
      "Iteration 87, loss = 0.02798623\n",
      "Iteration 88, loss = 0.02786084\n",
      "Iteration 89, loss = 0.02727197\n",
      "Iteration 90, loss = 0.02689351\n",
      "Iteration 91, loss = 0.02647436\n",
      "Iteration 92, loss = 0.02597570\n",
      "Iteration 93, loss = 0.02579256\n",
      "Iteration 94, loss = 0.02617132\n",
      "Iteration 95, loss = 0.02508738\n",
      "Iteration 96, loss = 0.02472861\n",
      "Iteration 97, loss = 0.02428083\n",
      "Iteration 98, loss = 0.02384081\n",
      "Iteration 99, loss = 0.02376031\n",
      "Iteration 100, loss = 0.02317211\n",
      "Iteration 101, loss = 0.02305204\n",
      "Iteration 102, loss = 0.02275883\n",
      "Iteration 103, loss = 0.02250118\n",
      "Iteration 104, loss = 0.02217171\n",
      "Iteration 105, loss = 0.02208151\n",
      "Iteration 106, loss = 0.02154109\n",
      "Iteration 107, loss = 0.02116073\n",
      "Iteration 108, loss = 0.02095309\n",
      "Iteration 109, loss = 0.02068737\n",
      "Iteration 110, loss = 0.02038467\n",
      "Iteration 111, loss = 0.02001322\n",
      "Iteration 112, loss = 0.02011915\n",
      "Iteration 113, loss = 0.01969574\n",
      "Iteration 114, loss = 0.01926627\n",
      "Iteration 115, loss = 0.01940564\n",
      "Iteration 116, loss = 0.01908960\n",
      "Iteration 117, loss = 0.01883006\n",
      "Iteration 118, loss = 0.01842577\n",
      "Iteration 119, loss = 0.01808542\n",
      "Iteration 120, loss = 0.01800356\n",
      "Iteration 121, loss = 0.01785767\n",
      "Iteration 122, loss = 0.01758836\n",
      "Iteration 123, loss = 0.01743136\n",
      "Iteration 124, loss = 0.01704553\n",
      "Iteration 125, loss = 0.01707886\n",
      "Iteration 126, loss = 0.01669362\n",
      "Iteration 127, loss = 0.01670048\n",
      "Iteration 128, loss = 0.01607869\n",
      "Iteration 129, loss = 0.01587458\n",
      "Iteration 130, loss = 0.01569028\n",
      "Iteration 131, loss = 0.01566678\n",
      "Iteration 132, loss = 0.01552789\n",
      "Iteration 133, loss = 0.01526913\n",
      "Iteration 134, loss = 0.01514659\n",
      "Iteration 135, loss = 0.01474630\n",
      "Iteration 136, loss = 0.01468026\n",
      "Iteration 137, loss = 0.01423257\n",
      "Iteration 138, loss = 0.01435505\n",
      "Iteration 139, loss = 0.01392167\n",
      "Iteration 140, loss = 0.01397675\n",
      "Iteration 141, loss = 0.01379623\n",
      "Iteration 142, loss = 0.01383636\n",
      "Iteration 143, loss = 0.01335127\n",
      "Iteration 144, loss = 0.01311034\n",
      "Iteration 145, loss = 0.01330097\n",
      "Iteration 146, loss = 0.01316766\n",
      "Iteration 147, loss = 0.01275546\n",
      "Iteration 148, loss = 0.01270369\n",
      "Iteration 149, loss = 0.01223730\n",
      "Iteration 150, loss = 0.01240772\n",
      "Iteration 151, loss = 0.01227030\n",
      "Iteration 152, loss = 0.01179270\n",
      "Iteration 153, loss = 0.01175402\n",
      "Iteration 154, loss = 0.01143200\n",
      "Iteration 155, loss = 0.01136589\n",
      "Iteration 156, loss = 0.01121297\n",
      "Iteration 157, loss = 0.01120744\n",
      "Iteration 158, loss = 0.01098440\n",
      "Iteration 159, loss = 0.01072516\n",
      "Iteration 160, loss = 0.01073324\n",
      "Iteration 161, loss = 0.01061515\n",
      "Iteration 162, loss = 0.01034755\n",
      "Iteration 163, loss = 0.01040000\n",
      "Iteration 164, loss = 0.01014931\n",
      "Iteration 165, loss = 0.01001049\n",
      "Iteration 166, loss = 0.00988040\n",
      "Iteration 167, loss = 0.00982485\n",
      "Iteration 168, loss = 0.00966375\n",
      "Iteration 169, loss = 0.00955487\n",
      "Iteration 170, loss = 0.00947376\n",
      "Iteration 171, loss = 0.00931527\n",
      "Iteration 172, loss = 0.00919618\n",
      "Iteration 173, loss = 0.00915627\n",
      "Iteration 174, loss = 0.00900358\n",
      "Iteration 175, loss = 0.00904418\n",
      "Iteration 176, loss = 0.00884266\n",
      "Iteration 177, loss = 0.00876857\n",
      "Iteration 178, loss = 0.00870334\n",
      "Iteration 179, loss = 0.00870293\n",
      "Iteration 180, loss = 0.00853034\n",
      "Iteration 181, loss = 0.00841431\n",
      "Iteration 182, loss = 0.00822682\n",
      "Iteration 183, loss = 0.00817040\n",
      "Iteration 184, loss = 0.00813483\n",
      "Iteration 185, loss = 0.00808392\n",
      "Iteration 186, loss = 0.00795869\n",
      "Iteration 187, loss = 0.00784388\n",
      "Iteration 188, loss = 0.00787799\n",
      "Iteration 189, loss = 0.00778979\n",
      "Iteration 190, loss = 0.00778652\n",
      "Iteration 191, loss = 0.00771421\n",
      "Iteration 192, loss = 0.00764671\n",
      "Iteration 193, loss = 0.00752237\n",
      "Iteration 194, loss = 0.00735708\n",
      "Iteration 195, loss = 0.00736121\n",
      "Iteration 196, loss = 0.00719610\n",
      "Iteration 197, loss = 0.00727310\n",
      "Iteration 198, loss = 0.00701819\n",
      "Iteration 199, loss = 0.00698141\n",
      "Iteration 200, loss = 0.00696429\n",
      "Iteration 201, loss = 0.00684928\n",
      "Iteration 202, loss = 0.00682449\n",
      "Iteration 203, loss = 0.00669077\n",
      "Iteration 204, loss = 0.00661620\n",
      "Iteration 205, loss = 0.00662946\n",
      "Iteration 206, loss = 0.00645596\n",
      "Iteration 207, loss = 0.00644453\n",
      "Iteration 208, loss = 0.00641848\n",
      "Iteration 209, loss = 0.00643183\n",
      "Iteration 210, loss = 0.00624052\n",
      "Iteration 211, loss = 0.00628067\n",
      "Iteration 212, loss = 0.00618405\n",
      "Iteration 213, loss = 0.00609973\n",
      "Iteration 214, loss = 0.00608002\n",
      "Iteration 215, loss = 0.00615919\n",
      "Iteration 216, loss = 0.00594539\n",
      "Iteration 217, loss = 0.00607640\n",
      "Iteration 218, loss = 0.00590389\n",
      "Iteration 219, loss = 0.00583377\n",
      "Iteration 220, loss = 0.00592327\n",
      "Iteration 221, loss = 0.00558934\n",
      "Iteration 222, loss = 0.00583316\n",
      "Iteration 223, loss = 0.00569252\n",
      "Iteration 224, loss = 0.00555260\n",
      "Iteration 225, loss = 0.00580854\n",
      "Iteration 226, loss = 0.00568508\n",
      "Iteration 227, loss = 0.00544534\n",
      "Iteration 228, loss = 0.00577000\n",
      "Iteration 229, loss = 0.00541395\n",
      "Iteration 230, loss = 0.00546052\n",
      "Iteration 231, loss = 0.00534045\n",
      "Iteration 232, loss = 0.00517751\n",
      "Iteration 233, loss = 0.00543101\n",
      "Iteration 234, loss = 0.00521276\n",
      "Iteration 235, loss = 0.00518754\n",
      "Iteration 236, loss = 0.00510676\n",
      "Iteration 237, loss = 0.00498376\n",
      "Iteration 238, loss = 0.00496459\n",
      "Iteration 239, loss = 0.00502699\n",
      "Iteration 240, loss = 0.00502866\n",
      "Iteration 241, loss = 0.00485527\n",
      "Iteration 242, loss = 0.00484334\n",
      "Iteration 243, loss = 0.00464094\n",
      "Iteration 244, loss = 0.00466807\n",
      "Iteration 245, loss = 0.00463989\n",
      "Iteration 246, loss = 0.00457989\n",
      "Iteration 247, loss = 0.00452340\n",
      "Iteration 248, loss = 0.00451198\n",
      "Iteration 249, loss = 0.00442364\n",
      "Iteration 250, loss = 0.00450443\n",
      "Iteration 251, loss = 0.00443378\n",
      "Iteration 252, loss = 0.00432851\n",
      "Iteration 253, loss = 0.00438700\n",
      "Iteration 254, loss = 0.00427666\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46029207\n",
      "Iteration 2, loss = 0.40621980\n",
      "Iteration 3, loss = 0.36057248\n",
      "Iteration 4, loss = 0.33544303\n",
      "Iteration 5, loss = 0.30499096\n",
      "Iteration 6, loss = 0.28613896\n",
      "Iteration 7, loss = 0.26773959\n",
      "Iteration 8, loss = 0.24680414\n",
      "Iteration 9, loss = 0.22980733\n",
      "Iteration 10, loss = 0.21338378\n",
      "Iteration 11, loss = 0.19816346\n",
      "Iteration 12, loss = 0.18188803\n",
      "Iteration 13, loss = 0.16779507\n",
      "Iteration 14, loss = 0.15420539\n",
      "Iteration 15, loss = 0.14281876\n",
      "Iteration 16, loss = 0.13375488\n",
      "Iteration 17, loss = 0.12331975\n",
      "Iteration 18, loss = 0.11290834\n",
      "Iteration 19, loss = 0.10571619\n",
      "Iteration 20, loss = 0.09901523\n",
      "Iteration 21, loss = 0.09306876\n",
      "Iteration 22, loss = 0.08801526\n",
      "Iteration 23, loss = 0.08278968\n",
      "Iteration 24, loss = 0.07875806\n",
      "Iteration 25, loss = 0.07609770\n",
      "Iteration 26, loss = 0.07329222\n",
      "Iteration 27, loss = 0.06924697\n",
      "Iteration 28, loss = 0.06646339\n",
      "Iteration 29, loss = 0.06438655\n",
      "Iteration 30, loss = 0.06161639\n",
      "Iteration 31, loss = 0.05960356\n",
      "Iteration 32, loss = 0.05747623\n",
      "Iteration 33, loss = 0.05584303\n",
      "Iteration 34, loss = 0.05378333\n",
      "Iteration 35, loss = 0.05226510\n",
      "Iteration 36, loss = 0.05082191\n",
      "Iteration 37, loss = 0.04919061\n",
      "Iteration 38, loss = 0.04765444\n",
      "Iteration 39, loss = 0.04608123\n",
      "Iteration 40, loss = 0.04476676\n",
      "Iteration 41, loss = 0.04367447\n",
      "Iteration 42, loss = 0.04223571\n",
      "Iteration 43, loss = 0.04107542\n",
      "Iteration 44, loss = 0.04000401\n",
      "Iteration 45, loss = 0.03918527\n",
      "Iteration 46, loss = 0.03798940\n",
      "Iteration 47, loss = 0.03718344\n",
      "Iteration 48, loss = 0.03619102\n",
      "Iteration 49, loss = 0.03562411\n",
      "Iteration 50, loss = 0.03455165\n",
      "Iteration 51, loss = 0.03372006\n",
      "Iteration 52, loss = 0.03326207\n",
      "Iteration 53, loss = 0.03251867\n",
      "Iteration 54, loss = 0.03184442\n",
      "Iteration 55, loss = 0.03110683\n",
      "Iteration 56, loss = 0.03051927\n",
      "Iteration 57, loss = 0.02991785\n",
      "Iteration 58, loss = 0.02937556\n",
      "Iteration 59, loss = 0.02931958\n",
      "Iteration 60, loss = 0.02842335\n",
      "Iteration 61, loss = 0.02785175\n",
      "Iteration 62, loss = 0.02734353\n",
      "Iteration 63, loss = 0.02710714\n",
      "Iteration 64, loss = 0.02660504\n",
      "Iteration 65, loss = 0.02628728\n",
      "Iteration 66, loss = 0.02563288\n",
      "Iteration 67, loss = 0.02520358\n",
      "Iteration 68, loss = 0.02485763\n",
      "Iteration 69, loss = 0.02446193\n",
      "Iteration 70, loss = 0.02439797\n",
      "Iteration 71, loss = 0.02377223\n",
      "Iteration 72, loss = 0.02340158\n",
      "Iteration 73, loss = 0.02301283\n",
      "Iteration 74, loss = 0.02281406\n",
      "Iteration 75, loss = 0.02238016\n",
      "Iteration 76, loss = 0.02200327\n",
      "Iteration 77, loss = 0.02167235\n",
      "Iteration 78, loss = 0.02140898\n",
      "Iteration 79, loss = 0.02106935\n",
      "Iteration 80, loss = 0.02101251\n",
      "Iteration 81, loss = 0.02058091\n",
      "Iteration 82, loss = 0.02028800\n",
      "Iteration 83, loss = 0.02022757\n",
      "Iteration 84, loss = 0.01985513\n",
      "Iteration 85, loss = 0.01967667\n",
      "Iteration 86, loss = 0.01940342\n",
      "Iteration 87, loss = 0.01899456\n",
      "Iteration 88, loss = 0.01876748\n",
      "Iteration 89, loss = 0.01851530\n",
      "Iteration 90, loss = 0.01847280\n",
      "Iteration 91, loss = 0.01815495\n",
      "Iteration 92, loss = 0.01789089\n",
      "Iteration 93, loss = 0.01771244\n",
      "Iteration 94, loss = 0.01745815\n",
      "Iteration 95, loss = 0.01741252\n",
      "Iteration 96, loss = 0.01701201\n",
      "Iteration 97, loss = 0.01693738\n",
      "Iteration 98, loss = 0.01668105\n",
      "Iteration 99, loss = 0.01672949\n",
      "Iteration 100, loss = 0.01632058\n",
      "Iteration 101, loss = 0.01615791\n",
      "Iteration 102, loss = 0.01623103\n",
      "Iteration 103, loss = 0.01595643\n",
      "Iteration 104, loss = 0.01565620\n",
      "Iteration 105, loss = 0.01543876\n",
      "Iteration 106, loss = 0.01529115\n",
      "Iteration 107, loss = 0.01529758\n",
      "Iteration 108, loss = 0.01505393\n",
      "Iteration 109, loss = 0.01505506\n",
      "Iteration 110, loss = 0.01471517\n",
      "Iteration 111, loss = 0.01455865\n",
      "Iteration 112, loss = 0.01445925\n",
      "Iteration 113, loss = 0.01421891\n",
      "Iteration 114, loss = 0.01427821\n",
      "Iteration 115, loss = 0.01396109\n",
      "Iteration 116, loss = 0.01386184\n",
      "Iteration 117, loss = 0.01374669\n",
      "Iteration 118, loss = 0.01369452\n",
      "Iteration 119, loss = 0.01339776\n",
      "Iteration 120, loss = 0.01334269\n",
      "Iteration 121, loss = 0.01345593\n",
      "Iteration 122, loss = 0.01309384\n",
      "Iteration 123, loss = 0.01299474\n",
      "Iteration 124, loss = 0.01301878\n",
      "Iteration 125, loss = 0.01267001\n",
      "Iteration 126, loss = 0.01266335\n",
      "Iteration 127, loss = 0.01246833\n",
      "Iteration 128, loss = 0.01239485\n",
      "Iteration 129, loss = 0.01214386\n",
      "Iteration 130, loss = 0.01225082\n",
      "Iteration 131, loss = 0.01215752\n",
      "Iteration 132, loss = 0.01193940\n",
      "Iteration 133, loss = 0.01203825\n",
      "Iteration 134, loss = 0.01185169\n",
      "Iteration 135, loss = 0.01167960\n",
      "Iteration 136, loss = 0.01186548\n",
      "Iteration 137, loss = 0.01153622\n",
      "Iteration 138, loss = 0.01141824\n",
      "Iteration 139, loss = 0.01149298\n",
      "Iteration 140, loss = 0.01117815\n",
      "Iteration 141, loss = 0.01111185\n",
      "Iteration 142, loss = 0.01080970\n",
      "Iteration 143, loss = 0.01082943\n",
      "Iteration 144, loss = 0.01062033\n",
      "Iteration 145, loss = 0.01064138\n",
      "Iteration 146, loss = 0.01043953\n",
      "Iteration 147, loss = 0.01051414\n",
      "Iteration 148, loss = 0.01046174\n",
      "Iteration 149, loss = 0.01015883\n",
      "Iteration 150, loss = 0.01026859\n",
      "Iteration 151, loss = 0.01014711\n",
      "Iteration 152, loss = 0.01005991\n",
      "Iteration 153, loss = 0.00979749\n",
      "Iteration 154, loss = 0.00979051\n",
      "Iteration 155, loss = 0.00967237\n",
      "Iteration 156, loss = 0.00956566\n",
      "Iteration 157, loss = 0.00954158\n",
      "Iteration 158, loss = 0.00944011\n",
      "Iteration 159, loss = 0.00941271\n",
      "Iteration 160, loss = 0.00919922\n",
      "Iteration 161, loss = 0.00917873\n",
      "Iteration 162, loss = 0.00932427\n",
      "Iteration 163, loss = 0.00913841\n",
      "Iteration 164, loss = 0.00907751\n",
      "Iteration 165, loss = 0.00888009\n",
      "Iteration 166, loss = 0.00872615\n",
      "Iteration 167, loss = 0.00879121\n",
      "Iteration 168, loss = 0.00865002\n",
      "Iteration 169, loss = 0.00858049\n",
      "Iteration 170, loss = 0.00852860\n",
      "Iteration 171, loss = 0.00850329\n",
      "Iteration 172, loss = 0.00837984\n",
      "Iteration 173, loss = 0.00823882\n",
      "Iteration 174, loss = 0.00827255\n",
      "Iteration 175, loss = 0.00821000\n",
      "Iteration 176, loss = 0.00818075\n",
      "Iteration 177, loss = 0.00806267\n",
      "Iteration 178, loss = 0.00800380\n",
      "Iteration 179, loss = 0.00792634\n",
      "Iteration 180, loss = 0.00786707\n",
      "Iteration 181, loss = 0.00780030\n",
      "Iteration 182, loss = 0.00788102\n",
      "Iteration 183, loss = 0.00768079\n",
      "Iteration 184, loss = 0.00780919\n",
      "Iteration 185, loss = 0.00763658\n",
      "Iteration 186, loss = 0.00759981\n",
      "Iteration 187, loss = 0.00748027\n",
      "Iteration 188, loss = 0.00739296\n",
      "Iteration 189, loss = 0.00745419\n",
      "Iteration 190, loss = 0.00723236\n",
      "Iteration 191, loss = 0.00726643\n",
      "Iteration 192, loss = 0.00714147\n",
      "Iteration 193, loss = 0.00705603\n",
      "Iteration 194, loss = 0.00700621\n",
      "Iteration 195, loss = 0.00695441\n",
      "Iteration 196, loss = 0.00690278\n",
      "Iteration 197, loss = 0.00687670\n",
      "Iteration 198, loss = 0.00675879\n",
      "Iteration 199, loss = 0.00672839\n",
      "Iteration 200, loss = 0.00667513\n",
      "Iteration 201, loss = 0.00665792\n",
      "Iteration 202, loss = 0.00656279\n",
      "Iteration 203, loss = 0.00655949\n",
      "Iteration 204, loss = 0.00646290\n",
      "Iteration 205, loss = 0.00660458\n",
      "Iteration 206, loss = 0.00643679\n",
      "Iteration 207, loss = 0.00639925\n",
      "Iteration 208, loss = 0.00634439\n",
      "Iteration 209, loss = 0.00626198\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46029207\n",
      "Iteration 2, loss = 0.40621980\n",
      "Iteration 3, loss = 0.36057248\n",
      "Iteration 4, loss = 0.33544303\n",
      "Iteration 5, loss = 0.30499096\n",
      "Iteration 6, loss = 0.28613896\n",
      "Iteration 7, loss = 0.26773959\n",
      "Iteration 8, loss = 0.24680414\n",
      "Iteration 9, loss = 0.22980733\n",
      "Iteration 10, loss = 0.21338378\n",
      "Iteration 11, loss = 0.19816346\n",
      "Iteration 12, loss = 0.18188803\n",
      "Iteration 13, loss = 0.16779507\n",
      "Iteration 14, loss = 0.15420539\n",
      "Iteration 15, loss = 0.14281876\n",
      "Iteration 16, loss = 0.13375488\n",
      "Iteration 17, loss = 0.12331975\n",
      "Iteration 18, loss = 0.11290834\n",
      "Iteration 19, loss = 0.10571619\n",
      "Iteration 20, loss = 0.09901523\n",
      "Iteration 21, loss = 0.09306876\n",
      "Iteration 22, loss = 0.08801526\n",
      "Iteration 23, loss = 0.08278968\n",
      "Iteration 24, loss = 0.07875806\n",
      "Iteration 25, loss = 0.07609770\n",
      "Iteration 26, loss = 0.07329222\n",
      "Iteration 27, loss = 0.06924697\n",
      "Iteration 28, loss = 0.06646339\n",
      "Iteration 29, loss = 0.06438655\n",
      "Iteration 30, loss = 0.06161639\n",
      "Iteration 31, loss = 0.05960356\n",
      "Iteration 32, loss = 0.05747623\n",
      "Iteration 33, loss = 0.05584303\n",
      "Iteration 34, loss = 0.05378333\n",
      "Iteration 35, loss = 0.05226510\n",
      "Iteration 36, loss = 0.05082191\n",
      "Iteration 37, loss = 0.04919061\n",
      "Iteration 38, loss = 0.04765444\n",
      "Iteration 39, loss = 0.04608123\n",
      "Iteration 40, loss = 0.04476676\n",
      "Iteration 41, loss = 0.04367447\n",
      "Iteration 42, loss = 0.04223571\n",
      "Iteration 43, loss = 0.04107542\n",
      "Iteration 44, loss = 0.04000401\n",
      "Iteration 45, loss = 0.03918527\n",
      "Iteration 46, loss = 0.03798940\n",
      "Iteration 47, loss = 0.03718344\n",
      "Iteration 48, loss = 0.03619102\n",
      "Iteration 49, loss = 0.03562411\n",
      "Iteration 50, loss = 0.03455165\n",
      "Iteration 51, loss = 0.03372006\n",
      "Iteration 52, loss = 0.03326207\n",
      "Iteration 53, loss = 0.03251867\n",
      "Iteration 54, loss = 0.03184442\n",
      "Iteration 55, loss = 0.03110683\n",
      "Iteration 56, loss = 0.03051927\n",
      "Iteration 57, loss = 0.02991785\n",
      "Iteration 58, loss = 0.02937556\n",
      "Iteration 59, loss = 0.02931958\n",
      "Iteration 60, loss = 0.02842335\n",
      "Iteration 61, loss = 0.02785175\n",
      "Iteration 62, loss = 0.02734353\n",
      "Iteration 63, loss = 0.02710714\n",
      "Iteration 64, loss = 0.02660504\n",
      "Iteration 65, loss = 0.02628728\n",
      "Iteration 66, loss = 0.02563288\n",
      "Iteration 67, loss = 0.02520358\n",
      "Iteration 68, loss = 0.02485763\n",
      "Iteration 69, loss = 0.02446193\n",
      "Iteration 70, loss = 0.02439797\n",
      "Iteration 71, loss = 0.02377223\n",
      "Iteration 72, loss = 0.02340158\n",
      "Iteration 73, loss = 0.02301283\n",
      "Iteration 74, loss = 0.02281406\n",
      "Iteration 75, loss = 0.02238016\n",
      "Iteration 76, loss = 0.02200327\n",
      "Iteration 77, loss = 0.02167235\n",
      "Iteration 78, loss = 0.02140898\n",
      "Iteration 79, loss = 0.02106935\n",
      "Iteration 80, loss = 0.02101251\n",
      "Iteration 81, loss = 0.02058091\n",
      "Iteration 82, loss = 0.02028800\n",
      "Iteration 83, loss = 0.02022757\n",
      "Iteration 84, loss = 0.01985513\n",
      "Iteration 85, loss = 0.01967667\n",
      "Iteration 86, loss = 0.01940342\n",
      "Iteration 87, loss = 0.01899456\n",
      "Iteration 88, loss = 0.01876748\n",
      "Iteration 89, loss = 0.01851530\n",
      "Iteration 90, loss = 0.01847280\n",
      "Iteration 91, loss = 0.01815495\n",
      "Iteration 92, loss = 0.01789089\n",
      "Iteration 93, loss = 0.01771244\n",
      "Iteration 94, loss = 0.01745815\n",
      "Iteration 95, loss = 0.01741252\n",
      "Iteration 96, loss = 0.01701201\n",
      "Iteration 97, loss = 0.01693738\n",
      "Iteration 98, loss = 0.01668105\n",
      "Iteration 99, loss = 0.01672949\n",
      "Iteration 100, loss = 0.01632058\n",
      "Iteration 101, loss = 0.01615791\n",
      "Iteration 102, loss = 0.01623103\n",
      "Iteration 103, loss = 0.01595643\n",
      "Iteration 104, loss = 0.01565620\n",
      "Iteration 105, loss = 0.01543876\n",
      "Iteration 106, loss = 0.01529115\n",
      "Iteration 107, loss = 0.01529758\n",
      "Iteration 108, loss = 0.01505393\n",
      "Iteration 109, loss = 0.01505506\n",
      "Iteration 110, loss = 0.01471517\n",
      "Iteration 111, loss = 0.01455865\n",
      "Iteration 112, loss = 0.01445925\n",
      "Iteration 113, loss = 0.01421891\n",
      "Iteration 114, loss = 0.01427821\n",
      "Iteration 115, loss = 0.01396109\n",
      "Iteration 116, loss = 0.01386184\n",
      "Iteration 117, loss = 0.01374669\n",
      "Iteration 118, loss = 0.01369452\n",
      "Iteration 119, loss = 0.01339776\n",
      "Iteration 120, loss = 0.01334269\n",
      "Iteration 121, loss = 0.01345593\n",
      "Iteration 122, loss = 0.01309384\n",
      "Iteration 123, loss = 0.01299474\n",
      "Iteration 124, loss = 0.01301878\n",
      "Iteration 125, loss = 0.01267001\n",
      "Iteration 126, loss = 0.01266335\n",
      "Iteration 127, loss = 0.01246833\n",
      "Iteration 128, loss = 0.01239485\n",
      "Iteration 129, loss = 0.01214386\n",
      "Iteration 130, loss = 0.01225082\n",
      "Iteration 131, loss = 0.01215752\n",
      "Iteration 132, loss = 0.01193940\n",
      "Iteration 133, loss = 0.01203825\n",
      "Iteration 134, loss = 0.01185169\n",
      "Iteration 135, loss = 0.01167960\n",
      "Iteration 136, loss = 0.01186548\n",
      "Iteration 137, loss = 0.01153622\n",
      "Iteration 138, loss = 0.01141824\n",
      "Iteration 139, loss = 0.01149298\n",
      "Iteration 140, loss = 0.01117815\n",
      "Iteration 141, loss = 0.01111185\n",
      "Iteration 142, loss = 0.01080970\n",
      "Iteration 143, loss = 0.01082943\n",
      "Iteration 144, loss = 0.01062033\n",
      "Iteration 145, loss = 0.01064138\n",
      "Iteration 146, loss = 0.01043953\n",
      "Iteration 147, loss = 0.01051414\n",
      "Iteration 148, loss = 0.01046174\n",
      "Iteration 149, loss = 0.01015883\n",
      "Iteration 150, loss = 0.01026859\n",
      "Iteration 151, loss = 0.01014711\n",
      "Iteration 152, loss = 0.01005991\n",
      "Iteration 153, loss = 0.00979749\n",
      "Iteration 154, loss = 0.00979051\n",
      "Iteration 155, loss = 0.00967237\n",
      "Iteration 156, loss = 0.00956566\n",
      "Iteration 157, loss = 0.00954158\n",
      "Iteration 158, loss = 0.00944011\n",
      "Iteration 159, loss = 0.00941271\n",
      "Iteration 160, loss = 0.00919922\n",
      "Iteration 161, loss = 0.00917873\n",
      "Iteration 162, loss = 0.00932427\n",
      "Iteration 163, loss = 0.00913841\n",
      "Iteration 164, loss = 0.00907751\n",
      "Iteration 165, loss = 0.00888009\n",
      "Iteration 166, loss = 0.00872615\n",
      "Iteration 167, loss = 0.00879121\n",
      "Iteration 168, loss = 0.00865002\n",
      "Iteration 169, loss = 0.00858049\n",
      "Iteration 170, loss = 0.00852860\n",
      "Iteration 171, loss = 0.00850329\n",
      "Iteration 172, loss = 0.00837984\n",
      "Iteration 173, loss = 0.00823882\n",
      "Iteration 174, loss = 0.00827255\n",
      "Iteration 175, loss = 0.00821000\n",
      "Iteration 176, loss = 0.00818075\n",
      "Iteration 177, loss = 0.00806267\n",
      "Iteration 178, loss = 0.00800380\n",
      "Iteration 179, loss = 0.00792634\n",
      "Iteration 180, loss = 0.00786707\n",
      "Iteration 181, loss = 0.00780030\n",
      "Iteration 182, loss = 0.00788102\n",
      "Iteration 183, loss = 0.00768079\n",
      "Iteration 184, loss = 0.00780919\n",
      "Iteration 185, loss = 0.00763658\n",
      "Iteration 186, loss = 0.00759981\n",
      "Iteration 187, loss = 0.00748027\n",
      "Iteration 188, loss = 0.00739296\n",
      "Iteration 189, loss = 0.00745419\n",
      "Iteration 190, loss = 0.00723236\n",
      "Iteration 191, loss = 0.00726643\n",
      "Iteration 192, loss = 0.00714147\n",
      "Iteration 193, loss = 0.00705603\n",
      "Iteration 194, loss = 0.00700621\n",
      "Iteration 195, loss = 0.00695441\n",
      "Iteration 196, loss = 0.00690278\n",
      "Iteration 197, loss = 0.00687670\n",
      "Iteration 198, loss = 0.00675879\n",
      "Iteration 199, loss = 0.00672839\n",
      "Iteration 200, loss = 0.00667513\n",
      "Iteration 201, loss = 0.00665792\n",
      "Iteration 202, loss = 0.00656279\n",
      "Iteration 203, loss = 0.00655949\n",
      "Iteration 204, loss = 0.00646290\n",
      "Iteration 205, loss = 0.00660458\n",
      "Iteration 206, loss = 0.00643679\n",
      "Iteration 207, loss = 0.00639925\n",
      "Iteration 208, loss = 0.00634439\n",
      "Iteration 209, loss = 0.00626198\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46029207\n",
      "Iteration 2, loss = 0.40621980\n",
      "Iteration 3, loss = 0.36057248\n",
      "Iteration 4, loss = 0.33544303\n",
      "Iteration 5, loss = 0.30499096\n",
      "Iteration 6, loss = 0.28613896\n",
      "Iteration 7, loss = 0.26773959\n",
      "Iteration 8, loss = 0.24680414\n",
      "Iteration 9, loss = 0.22980733\n",
      "Iteration 10, loss = 0.21338378\n",
      "Iteration 11, loss = 0.19816346\n",
      "Iteration 12, loss = 0.18188803\n",
      "Iteration 13, loss = 0.16779507\n",
      "Iteration 14, loss = 0.15420539\n",
      "Iteration 15, loss = 0.14281876\n",
      "Iteration 16, loss = 0.13375488\n",
      "Iteration 17, loss = 0.12331975\n",
      "Iteration 18, loss = 0.11290834\n",
      "Iteration 19, loss = 0.10571619\n",
      "Iteration 20, loss = 0.09901523\n",
      "Iteration 21, loss = 0.09306876\n",
      "Iteration 22, loss = 0.08801526\n",
      "Iteration 23, loss = 0.08278968\n",
      "Iteration 24, loss = 0.07875806\n",
      "Iteration 25, loss = 0.07609770\n",
      "Iteration 26, loss = 0.07329222\n",
      "Iteration 27, loss = 0.06924697\n",
      "Iteration 28, loss = 0.06646339\n",
      "Iteration 29, loss = 0.06438655\n",
      "Iteration 30, loss = 0.06161639\n",
      "Iteration 31, loss = 0.05960356\n",
      "Iteration 32, loss = 0.05747623\n",
      "Iteration 33, loss = 0.05584303\n",
      "Iteration 34, loss = 0.05378333\n",
      "Iteration 35, loss = 0.05226510\n",
      "Iteration 36, loss = 0.05082191\n",
      "Iteration 37, loss = 0.04919061\n",
      "Iteration 38, loss = 0.04765444\n",
      "Iteration 39, loss = 0.04608123\n",
      "Iteration 40, loss = 0.04476676\n",
      "Iteration 41, loss = 0.04367447\n",
      "Iteration 42, loss = 0.04223571\n",
      "Iteration 43, loss = 0.04107542\n",
      "Iteration 44, loss = 0.04000401\n",
      "Iteration 45, loss = 0.03918527\n",
      "Iteration 46, loss = 0.03798940\n",
      "Iteration 47, loss = 0.03718344\n",
      "Iteration 48, loss = 0.03619102\n",
      "Iteration 49, loss = 0.03562411\n",
      "Iteration 50, loss = 0.03455165\n",
      "Iteration 51, loss = 0.03372006\n",
      "Iteration 52, loss = 0.03326207\n",
      "Iteration 53, loss = 0.03251867\n",
      "Iteration 54, loss = 0.03184442\n",
      "Iteration 55, loss = 0.03110683\n",
      "Iteration 56, loss = 0.03051927\n",
      "Iteration 57, loss = 0.02991785\n",
      "Iteration 58, loss = 0.02937556\n",
      "Iteration 59, loss = 0.02931958\n",
      "Iteration 60, loss = 0.02842335\n",
      "Iteration 61, loss = 0.02785175\n",
      "Iteration 62, loss = 0.02734353\n",
      "Iteration 63, loss = 0.02710714\n",
      "Iteration 64, loss = 0.02660504\n",
      "Iteration 65, loss = 0.02628728\n",
      "Iteration 66, loss = 0.02563288\n",
      "Iteration 67, loss = 0.02520358\n",
      "Iteration 68, loss = 0.02485763\n",
      "Iteration 69, loss = 0.02446193\n",
      "Iteration 70, loss = 0.02439797\n",
      "Iteration 71, loss = 0.02377223\n",
      "Iteration 72, loss = 0.02340158\n",
      "Iteration 73, loss = 0.02301283\n",
      "Iteration 74, loss = 0.02281406\n",
      "Iteration 75, loss = 0.02238016\n",
      "Iteration 76, loss = 0.02200327\n",
      "Iteration 77, loss = 0.02167235\n",
      "Iteration 78, loss = 0.02140898\n",
      "Iteration 79, loss = 0.02106935\n",
      "Iteration 80, loss = 0.02101251\n",
      "Iteration 81, loss = 0.02058091\n",
      "Iteration 82, loss = 0.02028800\n",
      "Iteration 83, loss = 0.02022757\n",
      "Iteration 84, loss = 0.01985513\n",
      "Iteration 85, loss = 0.01967667\n",
      "Iteration 86, loss = 0.01940342\n",
      "Iteration 87, loss = 0.01899456\n",
      "Iteration 88, loss = 0.01876748\n",
      "Iteration 89, loss = 0.01851530\n",
      "Iteration 90, loss = 0.01847280\n",
      "Iteration 91, loss = 0.01815495\n",
      "Iteration 92, loss = 0.01789089\n",
      "Iteration 93, loss = 0.01771244\n",
      "Iteration 94, loss = 0.01745815\n",
      "Iteration 95, loss = 0.01741252\n",
      "Iteration 96, loss = 0.01701201\n",
      "Iteration 97, loss = 0.01693738\n",
      "Iteration 98, loss = 0.01668105\n",
      "Iteration 99, loss = 0.01672949\n",
      "Iteration 100, loss = 0.01632058\n",
      "Iteration 101, loss = 0.01615791\n",
      "Iteration 102, loss = 0.01623103\n",
      "Iteration 103, loss = 0.01595643\n",
      "Iteration 104, loss = 0.01565620\n",
      "Iteration 105, loss = 0.01543876\n",
      "Iteration 106, loss = 0.01529115\n",
      "Iteration 107, loss = 0.01529758\n",
      "Iteration 108, loss = 0.01505393\n",
      "Iteration 109, loss = 0.01505506\n",
      "Iteration 110, loss = 0.01471517\n",
      "Iteration 111, loss = 0.01455865\n",
      "Iteration 112, loss = 0.01445925\n",
      "Iteration 113, loss = 0.01421891\n",
      "Iteration 114, loss = 0.01427821\n",
      "Iteration 115, loss = 0.01396109\n",
      "Iteration 116, loss = 0.01386184\n",
      "Iteration 117, loss = 0.01374669\n",
      "Iteration 118, loss = 0.01369452\n",
      "Iteration 119, loss = 0.01339776\n",
      "Iteration 120, loss = 0.01334269\n",
      "Iteration 121, loss = 0.01345593\n",
      "Iteration 122, loss = 0.01309384\n",
      "Iteration 123, loss = 0.01299474\n",
      "Iteration 124, loss = 0.01301878\n",
      "Iteration 125, loss = 0.01267001\n",
      "Iteration 126, loss = 0.01266335\n",
      "Iteration 127, loss = 0.01246833\n",
      "Iteration 128, loss = 0.01239485\n",
      "Iteration 129, loss = 0.01214386\n",
      "Iteration 130, loss = 0.01225082\n",
      "Iteration 131, loss = 0.01215752\n",
      "Iteration 132, loss = 0.01193940\n",
      "Iteration 133, loss = 0.01203825\n",
      "Iteration 134, loss = 0.01185169\n",
      "Iteration 135, loss = 0.01167960\n",
      "Iteration 136, loss = 0.01186548\n",
      "Iteration 137, loss = 0.01153622\n",
      "Iteration 138, loss = 0.01141824\n",
      "Iteration 139, loss = 0.01149298\n",
      "Iteration 140, loss = 0.01117815\n",
      "Iteration 141, loss = 0.01111185\n",
      "Iteration 142, loss = 0.01080970\n",
      "Iteration 143, loss = 0.01082943\n",
      "Iteration 144, loss = 0.01062033\n",
      "Iteration 145, loss = 0.01064138\n",
      "Iteration 146, loss = 0.01043953\n",
      "Iteration 147, loss = 0.01051414\n",
      "Iteration 148, loss = 0.01046174\n",
      "Iteration 149, loss = 0.01015883\n",
      "Iteration 150, loss = 0.01026859\n",
      "Iteration 151, loss = 0.01014711\n",
      "Iteration 152, loss = 0.01005991\n",
      "Iteration 153, loss = 0.00979749\n",
      "Iteration 154, loss = 0.00979051\n",
      "Iteration 155, loss = 0.00967237\n",
      "Iteration 156, loss = 0.00956566\n",
      "Iteration 157, loss = 0.00954158\n",
      "Iteration 158, loss = 0.00944011\n",
      "Iteration 159, loss = 0.00941271\n",
      "Iteration 160, loss = 0.00919922\n",
      "Iteration 161, loss = 0.00917873\n",
      "Iteration 162, loss = 0.00932427\n",
      "Iteration 163, loss = 0.00913841\n",
      "Iteration 164, loss = 0.00907751\n",
      "Iteration 165, loss = 0.00888009\n",
      "Iteration 166, loss = 0.00872615\n",
      "Iteration 167, loss = 0.00879121\n",
      "Iteration 168, loss = 0.00865002\n",
      "Iteration 169, loss = 0.00858049\n",
      "Iteration 170, loss = 0.00852860\n",
      "Iteration 171, loss = 0.00850329\n",
      "Iteration 172, loss = 0.00837984\n",
      "Iteration 173, loss = 0.00823882\n",
      "Iteration 174, loss = 0.00827255\n",
      "Iteration 175, loss = 0.00821000\n",
      "Iteration 176, loss = 0.00818075\n",
      "Iteration 177, loss = 0.00806267\n",
      "Iteration 178, loss = 0.00800380\n",
      "Iteration 179, loss = 0.00792634\n",
      "Iteration 180, loss = 0.00786707\n",
      "Iteration 181, loss = 0.00780030\n",
      "Iteration 182, loss = 0.00788102\n",
      "Iteration 183, loss = 0.00768079\n",
      "Iteration 184, loss = 0.00780919\n",
      "Iteration 185, loss = 0.00763658\n",
      "Iteration 186, loss = 0.00759981\n",
      "Iteration 187, loss = 0.00748027\n",
      "Iteration 188, loss = 0.00739296\n",
      "Iteration 189, loss = 0.00745419\n",
      "Iteration 190, loss = 0.00723236\n",
      "Iteration 191, loss = 0.00726643\n",
      "Iteration 192, loss = 0.00714147\n",
      "Iteration 193, loss = 0.00705603\n",
      "Iteration 194, loss = 0.00700621\n",
      "Iteration 195, loss = 0.00695441\n",
      "Iteration 196, loss = 0.00690278\n",
      "Iteration 197, loss = 0.00687670\n",
      "Iteration 198, loss = 0.00675879\n",
      "Iteration 199, loss = 0.00672839\n",
      "Iteration 200, loss = 0.00667513\n",
      "Iteration 201, loss = 0.00665792\n",
      "Iteration 202, loss = 0.00656279\n",
      "Iteration 203, loss = 0.00655949\n",
      "Iteration 204, loss = 0.00646290\n",
      "Iteration 205, loss = 0.00660458\n",
      "Iteration 206, loss = 0.00643679\n",
      "Iteration 207, loss = 0.00639925\n",
      "Iteration 208, loss = 0.00634439\n",
      "Iteration 209, loss = 0.00626198\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.43048721\n",
      "Iteration 2, loss = 0.36109055\n",
      "Iteration 3, loss = 0.32355330\n",
      "Iteration 4, loss = 0.29119066\n",
      "Iteration 5, loss = 0.26942743\n",
      "Iteration 6, loss = 0.24674110\n",
      "Iteration 7, loss = 0.22398220\n",
      "Iteration 8, loss = 0.20087055\n",
      "Iteration 9, loss = 0.18118639\n",
      "Iteration 10, loss = 0.16524084\n",
      "Iteration 11, loss = 0.14924242\n",
      "Iteration 12, loss = 0.13371582\n",
      "Iteration 13, loss = 0.11733614\n",
      "Iteration 14, loss = 0.10576167\n",
      "Iteration 15, loss = 0.09459584\n",
      "Iteration 16, loss = 0.08553221\n",
      "Iteration 17, loss = 0.07759792\n",
      "Iteration 18, loss = 0.07187788\n",
      "Iteration 19, loss = 0.06702825\n",
      "Iteration 20, loss = 0.06234146\n",
      "Iteration 21, loss = 0.05826208\n",
      "Iteration 22, loss = 0.05677761\n",
      "Iteration 23, loss = 0.05364394\n",
      "Iteration 24, loss = 0.05182042\n",
      "Iteration 25, loss = 0.04959008\n",
      "Iteration 26, loss = 0.04862976\n",
      "Iteration 27, loss = 0.04671012\n",
      "Iteration 28, loss = 0.04538081\n",
      "Iteration 29, loss = 0.04455000\n",
      "Iteration 30, loss = 0.04293018\n",
      "Iteration 31, loss = 0.04154061\n",
      "Iteration 32, loss = 0.04038392\n",
      "Iteration 33, loss = 0.03952581\n",
      "Iteration 34, loss = 0.03860155\n",
      "Iteration 35, loss = 0.03786199\n",
      "Iteration 36, loss = 0.03685177\n",
      "Iteration 37, loss = 0.03623576\n",
      "Iteration 38, loss = 0.03537604\n",
      "Iteration 39, loss = 0.03459961\n",
      "Iteration 40, loss = 0.03426476\n",
      "Iteration 41, loss = 0.03340836\n",
      "Iteration 42, loss = 0.03328941\n",
      "Iteration 43, loss = 0.03260458\n",
      "Iteration 44, loss = 0.03179702\n",
      "Iteration 45, loss = 0.03135735\n",
      "Iteration 46, loss = 0.03062545\n",
      "Iteration 47, loss = 0.03052286\n",
      "Iteration 48, loss = 0.03030252\n",
      "Iteration 49, loss = 0.02947922\n",
      "Iteration 50, loss = 0.02907981\n",
      "Iteration 51, loss = 0.02820820\n",
      "Iteration 52, loss = 0.02789345\n",
      "Iteration 53, loss = 0.02739725\n",
      "Iteration 54, loss = 0.02694178\n",
      "Iteration 55, loss = 0.02669106\n",
      "Iteration 56, loss = 0.02593641\n",
      "Iteration 57, loss = 0.02550727\n",
      "Iteration 58, loss = 0.02555456\n",
      "Iteration 59, loss = 0.02478995\n",
      "Iteration 60, loss = 0.02486482\n",
      "Iteration 61, loss = 0.02421162\n",
      "Iteration 62, loss = 0.02437107\n",
      "Iteration 63, loss = 0.02368174\n",
      "Iteration 64, loss = 0.02371201\n",
      "Iteration 65, loss = 0.02305493\n",
      "Iteration 66, loss = 0.02262985\n",
      "Iteration 67, loss = 0.02264314\n",
      "Iteration 68, loss = 0.02213185\n",
      "Iteration 69, loss = 0.02182008\n",
      "Iteration 70, loss = 0.02120658\n",
      "Iteration 71, loss = 0.02113034\n",
      "Iteration 72, loss = 0.02076804\n",
      "Iteration 73, loss = 0.02073030\n",
      "Iteration 74, loss = 0.02065077\n",
      "Iteration 75, loss = 0.02050420\n",
      "Iteration 76, loss = 0.02033557\n",
      "Iteration 77, loss = 0.01958893\n",
      "Iteration 78, loss = 0.01927291\n",
      "Iteration 79, loss = 0.01927556\n",
      "Iteration 80, loss = 0.01906215\n",
      "Iteration 81, loss = 0.01857587\n",
      "Iteration 82, loss = 0.01830226\n",
      "Iteration 83, loss = 0.01854804\n",
      "Iteration 84, loss = 0.01808689\n",
      "Iteration 85, loss = 0.01803398\n",
      "Iteration 86, loss = 0.01809958\n",
      "Iteration 87, loss = 0.01731435\n",
      "Iteration 88, loss = 0.01774118\n",
      "Iteration 89, loss = 0.01775536\n",
      "Iteration 90, loss = 0.01715855\n",
      "Iteration 91, loss = 0.01729349\n",
      "Iteration 92, loss = 0.01685512\n",
      "Iteration 93, loss = 0.01634142\n",
      "Iteration 94, loss = 0.01616200\n",
      "Iteration 95, loss = 0.01616744\n",
      "Iteration 96, loss = 0.01585192\n",
      "Iteration 97, loss = 0.01546520\n",
      "Iteration 98, loss = 0.01568152\n",
      "Iteration 99, loss = 0.01584696\n",
      "Iteration 100, loss = 0.01530532\n",
      "Iteration 101, loss = 0.01494002\n",
      "Iteration 102, loss = 0.01503592\n",
      "Iteration 103, loss = 0.01493276\n",
      "Iteration 104, loss = 0.01434054\n",
      "Iteration 105, loss = 0.01464768\n",
      "Iteration 106, loss = 0.01415149\n",
      "Iteration 107, loss = 0.01420611\n",
      "Iteration 108, loss = 0.01381385\n",
      "Iteration 109, loss = 0.01407475\n",
      "Iteration 110, loss = 0.01407793\n",
      "Iteration 111, loss = 0.01372022\n",
      "Iteration 112, loss = 0.01360899\n",
      "Iteration 113, loss = 0.01353528\n",
      "Iteration 114, loss = 0.01322780\n",
      "Iteration 115, loss = 0.01316768\n",
      "Iteration 116, loss = 0.01339850\n",
      "Iteration 117, loss = 0.01280023\n",
      "Iteration 118, loss = 0.01268961\n",
      "Iteration 119, loss = 0.01266793\n",
      "Iteration 120, loss = 0.01243698\n",
      "Iteration 121, loss = 0.01218626\n",
      "Iteration 122, loss = 0.01212413\n",
      "Iteration 123, loss = 0.01206470\n",
      "Iteration 124, loss = 0.01202111\n",
      "Iteration 125, loss = 0.01179874\n",
      "Iteration 126, loss = 0.01183155\n",
      "Iteration 127, loss = 0.01149025\n",
      "Iteration 128, loss = 0.01144712\n",
      "Iteration 129, loss = 0.01116914\n",
      "Iteration 130, loss = 0.01119439\n",
      "Iteration 131, loss = 0.01100045\n",
      "Iteration 132, loss = 0.01102419\n",
      "Iteration 133, loss = 0.01084107\n",
      "Iteration 134, loss = 0.01082016\n",
      "Iteration 135, loss = 0.01101565\n",
      "Iteration 136, loss = 0.01108905\n",
      "Iteration 137, loss = 0.01047321\n",
      "Iteration 138, loss = 0.01078948\n",
      "Iteration 139, loss = 0.01032171\n",
      "Iteration 140, loss = 0.01002097\n",
      "Iteration 141, loss = 0.01017244\n",
      "Iteration 142, loss = 0.00988665\n",
      "Iteration 143, loss = 0.00981266\n",
      "Iteration 144, loss = 0.00989614\n",
      "Iteration 145, loss = 0.00969941\n",
      "Iteration 146, loss = 0.00944948\n",
      "Iteration 147, loss = 0.00953575\n",
      "Iteration 148, loss = 0.00953454\n",
      "Iteration 149, loss = 0.00925425\n",
      "Iteration 150, loss = 0.00951511\n",
      "Iteration 151, loss = 0.00910654\n",
      "Iteration 152, loss = 0.00899637\n",
      "Iteration 153, loss = 0.00916081\n",
      "Iteration 154, loss = 0.00891642\n",
      "Iteration 155, loss = 0.00925240\n",
      "Iteration 156, loss = 0.00883680\n",
      "Iteration 157, loss = 0.00870518\n",
      "Iteration 158, loss = 0.00871524\n",
      "Iteration 159, loss = 0.00848483\n",
      "Iteration 160, loss = 0.00837690\n",
      "Iteration 161, loss = 0.00830034\n",
      "Iteration 162, loss = 0.00832265\n",
      "Iteration 163, loss = 0.00827043\n",
      "Iteration 164, loss = 0.00816409\n",
      "Iteration 165, loss = 0.00814815\n",
      "Iteration 166, loss = 0.00808471\n",
      "Iteration 167, loss = 0.00786809\n",
      "Iteration 168, loss = 0.00799153\n",
      "Iteration 169, loss = 0.00774073\n",
      "Iteration 170, loss = 0.00777938\n",
      "Iteration 171, loss = 0.00783938\n",
      "Iteration 172, loss = 0.00821987\n",
      "Iteration 173, loss = 0.00814484\n",
      "Iteration 174, loss = 0.00802928\n",
      "Iteration 175, loss = 0.00765368\n",
      "Iteration 176, loss = 0.00769264\n",
      "Iteration 177, loss = 0.00783377\n",
      "Iteration 178, loss = 0.00754750\n",
      "Iteration 179, loss = 0.00753631\n",
      "Iteration 180, loss = 0.00742008\n",
      "Iteration 181, loss = 0.00729296\n",
      "Iteration 182, loss = 0.00710889\n",
      "Iteration 183, loss = 0.00719672\n",
      "Iteration 184, loss = 0.00740892\n",
      "Iteration 185, loss = 0.00730314\n",
      "Iteration 186, loss = 0.00742665\n",
      "Iteration 187, loss = 0.00724305\n",
      "Iteration 188, loss = 0.00706319\n",
      "Iteration 189, loss = 0.00684536\n",
      "Iteration 190, loss = 0.00737205\n",
      "Iteration 191, loss = 0.00724770\n",
      "Iteration 192, loss = 0.00688657\n",
      "Iteration 193, loss = 0.00672126\n",
      "Iteration 194, loss = 0.00665678\n",
      "Iteration 195, loss = 0.00680133\n",
      "Iteration 196, loss = 0.00667435\n",
      "Iteration 197, loss = 0.00645018\n",
      "Iteration 198, loss = 0.00636465\n",
      "Iteration 199, loss = 0.00636166\n",
      "Iteration 200, loss = 0.00662323\n",
      "Iteration 201, loss = 0.00652181\n",
      "Iteration 202, loss = 0.00658084\n",
      "Iteration 203, loss = 0.00644493\n",
      "Iteration 204, loss = 0.00630067\n",
      "Iteration 205, loss = 0.00628550\n",
      "Iteration 206, loss = 0.00609942\n",
      "Iteration 207, loss = 0.00613283\n",
      "Iteration 208, loss = 0.00608221\n",
      "Iteration 209, loss = 0.00596733\n",
      "Iteration 210, loss = 0.00623433\n",
      "Iteration 211, loss = 0.00650523\n",
      "Iteration 212, loss = 0.00654032\n",
      "Iteration 213, loss = 0.00584242\n",
      "Iteration 214, loss = 0.00583277\n",
      "Iteration 215, loss = 0.00662595\n",
      "Iteration 216, loss = 0.00609126\n",
      "Iteration 217, loss = 0.00599429\n",
      "Iteration 218, loss = 0.00553053\n",
      "Iteration 219, loss = 0.00571590\n",
      "Iteration 220, loss = 0.00572392\n",
      "Iteration 221, loss = 0.00565895\n",
      "Iteration 222, loss = 0.00560776\n",
      "Iteration 223, loss = 0.00552584\n",
      "Iteration 224, loss = 0.00558454\n",
      "Iteration 225, loss = 0.00575839\n",
      "Iteration 226, loss = 0.00546903\n",
      "Iteration 227, loss = 0.00550655\n",
      "Iteration 228, loss = 0.00539550\n",
      "Iteration 229, loss = 0.00536780\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.43048721\n",
      "Iteration 2, loss = 0.36109055\n",
      "Iteration 3, loss = 0.32355330\n",
      "Iteration 4, loss = 0.29119066\n",
      "Iteration 5, loss = 0.26942743\n",
      "Iteration 6, loss = 0.24674110\n",
      "Iteration 7, loss = 0.22398220\n",
      "Iteration 8, loss = 0.20087055\n",
      "Iteration 9, loss = 0.18118639\n",
      "Iteration 10, loss = 0.16524084\n",
      "Iteration 11, loss = 0.14924242\n",
      "Iteration 12, loss = 0.13371582\n",
      "Iteration 13, loss = 0.11733614\n",
      "Iteration 14, loss = 0.10576167\n",
      "Iteration 15, loss = 0.09459584\n",
      "Iteration 16, loss = 0.08553221\n",
      "Iteration 17, loss = 0.07759792\n",
      "Iteration 18, loss = 0.07187788\n",
      "Iteration 19, loss = 0.06702825\n",
      "Iteration 20, loss = 0.06234146\n",
      "Iteration 21, loss = 0.05826208\n",
      "Iteration 22, loss = 0.05677761\n",
      "Iteration 23, loss = 0.05364394\n",
      "Iteration 24, loss = 0.05182042\n",
      "Iteration 25, loss = 0.04959008\n",
      "Iteration 26, loss = 0.04862976\n",
      "Iteration 27, loss = 0.04671012\n",
      "Iteration 28, loss = 0.04538081\n",
      "Iteration 29, loss = 0.04455000\n",
      "Iteration 30, loss = 0.04293018\n",
      "Iteration 31, loss = 0.04154061\n",
      "Iteration 32, loss = 0.04038392\n",
      "Iteration 33, loss = 0.03952581\n",
      "Iteration 34, loss = 0.03860155\n",
      "Iteration 35, loss = 0.03786199\n",
      "Iteration 36, loss = 0.03685177\n",
      "Iteration 37, loss = 0.03623576\n",
      "Iteration 38, loss = 0.03537604\n",
      "Iteration 39, loss = 0.03459961\n",
      "Iteration 40, loss = 0.03426476\n",
      "Iteration 41, loss = 0.03340836\n",
      "Iteration 42, loss = 0.03328941\n",
      "Iteration 43, loss = 0.03260458\n",
      "Iteration 44, loss = 0.03179702\n",
      "Iteration 45, loss = 0.03135735\n",
      "Iteration 46, loss = 0.03062545\n",
      "Iteration 47, loss = 0.03052286\n",
      "Iteration 48, loss = 0.03030252\n",
      "Iteration 49, loss = 0.02947922\n",
      "Iteration 50, loss = 0.02907981\n",
      "Iteration 51, loss = 0.02820820\n",
      "Iteration 52, loss = 0.02789345\n",
      "Iteration 53, loss = 0.02739725\n",
      "Iteration 54, loss = 0.02694178\n",
      "Iteration 55, loss = 0.02669106\n",
      "Iteration 56, loss = 0.02593641\n",
      "Iteration 57, loss = 0.02550727\n",
      "Iteration 58, loss = 0.02555456\n",
      "Iteration 59, loss = 0.02478995\n",
      "Iteration 60, loss = 0.02486482\n",
      "Iteration 61, loss = 0.02421162\n",
      "Iteration 62, loss = 0.02437107\n",
      "Iteration 63, loss = 0.02368174\n",
      "Iteration 64, loss = 0.02371201\n",
      "Iteration 65, loss = 0.02305493\n",
      "Iteration 66, loss = 0.02262985\n",
      "Iteration 67, loss = 0.02264314\n",
      "Iteration 68, loss = 0.02213185\n",
      "Iteration 69, loss = 0.02182008\n",
      "Iteration 70, loss = 0.02120658\n",
      "Iteration 71, loss = 0.02113034\n",
      "Iteration 72, loss = 0.02076804\n",
      "Iteration 73, loss = 0.02073030\n",
      "Iteration 74, loss = 0.02065077\n",
      "Iteration 75, loss = 0.02050420\n",
      "Iteration 76, loss = 0.02033557\n",
      "Iteration 77, loss = 0.01958893\n",
      "Iteration 78, loss = 0.01927291\n",
      "Iteration 79, loss = 0.01927556\n",
      "Iteration 80, loss = 0.01906215\n",
      "Iteration 81, loss = 0.01857587\n",
      "Iteration 82, loss = 0.01830226\n",
      "Iteration 83, loss = 0.01854804\n",
      "Iteration 84, loss = 0.01808689\n",
      "Iteration 85, loss = 0.01803398\n",
      "Iteration 86, loss = 0.01809958\n",
      "Iteration 87, loss = 0.01731435\n",
      "Iteration 88, loss = 0.01774118\n",
      "Iteration 89, loss = 0.01775536\n",
      "Iteration 90, loss = 0.01715855\n",
      "Iteration 91, loss = 0.01729349\n",
      "Iteration 92, loss = 0.01685512\n",
      "Iteration 93, loss = 0.01634142\n",
      "Iteration 94, loss = 0.01616200\n",
      "Iteration 95, loss = 0.01616744\n",
      "Iteration 96, loss = 0.01585192\n",
      "Iteration 97, loss = 0.01546520\n",
      "Iteration 98, loss = 0.01568152\n",
      "Iteration 99, loss = 0.01584696\n",
      "Iteration 100, loss = 0.01530532\n",
      "Iteration 101, loss = 0.01494002\n",
      "Iteration 102, loss = 0.01503592\n",
      "Iteration 103, loss = 0.01493276\n",
      "Iteration 104, loss = 0.01434054\n",
      "Iteration 105, loss = 0.01464768\n",
      "Iteration 106, loss = 0.01415149\n",
      "Iteration 107, loss = 0.01420611\n",
      "Iteration 108, loss = 0.01381385\n",
      "Iteration 109, loss = 0.01407475\n",
      "Iteration 110, loss = 0.01407793\n",
      "Iteration 111, loss = 0.01372022\n",
      "Iteration 112, loss = 0.01360899\n",
      "Iteration 113, loss = 0.01353528\n",
      "Iteration 114, loss = 0.01322780\n",
      "Iteration 115, loss = 0.01316768\n",
      "Iteration 116, loss = 0.01339850\n",
      "Iteration 117, loss = 0.01280023\n",
      "Iteration 118, loss = 0.01268961\n",
      "Iteration 119, loss = 0.01266793\n",
      "Iteration 120, loss = 0.01243698\n",
      "Iteration 121, loss = 0.01218626\n",
      "Iteration 122, loss = 0.01212413\n",
      "Iteration 123, loss = 0.01206470\n",
      "Iteration 124, loss = 0.01202111\n",
      "Iteration 125, loss = 0.01179874\n",
      "Iteration 126, loss = 0.01183155\n",
      "Iteration 127, loss = 0.01149025\n",
      "Iteration 128, loss = 0.01144712\n",
      "Iteration 129, loss = 0.01116914\n",
      "Iteration 130, loss = 0.01119439\n",
      "Iteration 131, loss = 0.01100045\n",
      "Iteration 132, loss = 0.01102419\n",
      "Iteration 133, loss = 0.01084107\n",
      "Iteration 134, loss = 0.01082016\n",
      "Iteration 135, loss = 0.01101565\n",
      "Iteration 136, loss = 0.01108905\n",
      "Iteration 137, loss = 0.01047321\n",
      "Iteration 138, loss = 0.01078948\n",
      "Iteration 139, loss = 0.01032171\n",
      "Iteration 140, loss = 0.01002097\n",
      "Iteration 141, loss = 0.01017244\n",
      "Iteration 142, loss = 0.00988665\n",
      "Iteration 143, loss = 0.00981266\n",
      "Iteration 144, loss = 0.00989614\n",
      "Iteration 145, loss = 0.00969941\n",
      "Iteration 146, loss = 0.00944948\n",
      "Iteration 147, loss = 0.00953575\n",
      "Iteration 148, loss = 0.00953454\n",
      "Iteration 149, loss = 0.00925425\n",
      "Iteration 150, loss = 0.00951511\n",
      "Iteration 151, loss = 0.00910654\n",
      "Iteration 152, loss = 0.00899637\n",
      "Iteration 153, loss = 0.00916081\n",
      "Iteration 154, loss = 0.00891642\n",
      "Iteration 155, loss = 0.00925240\n",
      "Iteration 156, loss = 0.00883680\n",
      "Iteration 157, loss = 0.00870518\n",
      "Iteration 158, loss = 0.00871524\n",
      "Iteration 159, loss = 0.00848483\n",
      "Iteration 160, loss = 0.00837690\n",
      "Iteration 161, loss = 0.00830034\n",
      "Iteration 162, loss = 0.00832265\n",
      "Iteration 163, loss = 0.00827043\n",
      "Iteration 164, loss = 0.00816409\n",
      "Iteration 165, loss = 0.00814815\n",
      "Iteration 166, loss = 0.00808471\n",
      "Iteration 167, loss = 0.00786809\n",
      "Iteration 168, loss = 0.00799153\n",
      "Iteration 169, loss = 0.00774073\n",
      "Iteration 170, loss = 0.00777938\n",
      "Iteration 171, loss = 0.00783938\n",
      "Iteration 172, loss = 0.00821987\n",
      "Iteration 173, loss = 0.00814484\n",
      "Iteration 174, loss = 0.00802928\n",
      "Iteration 175, loss = 0.00765368\n",
      "Iteration 176, loss = 0.00769264\n",
      "Iteration 177, loss = 0.00783377\n",
      "Iteration 178, loss = 0.00754750\n",
      "Iteration 179, loss = 0.00753631\n",
      "Iteration 180, loss = 0.00742008\n",
      "Iteration 181, loss = 0.00729296\n",
      "Iteration 182, loss = 0.00710889\n",
      "Iteration 183, loss = 0.00719672\n",
      "Iteration 184, loss = 0.00740892\n",
      "Iteration 185, loss = 0.00730314\n",
      "Iteration 186, loss = 0.00742665\n",
      "Iteration 187, loss = 0.00724305\n",
      "Iteration 188, loss = 0.00706319\n",
      "Iteration 189, loss = 0.00684536\n",
      "Iteration 190, loss = 0.00737205\n",
      "Iteration 191, loss = 0.00724770\n",
      "Iteration 192, loss = 0.00688657\n",
      "Iteration 193, loss = 0.00672126\n",
      "Iteration 194, loss = 0.00665678\n",
      "Iteration 195, loss = 0.00680133\n",
      "Iteration 196, loss = 0.00667435\n",
      "Iteration 197, loss = 0.00645018\n",
      "Iteration 198, loss = 0.00636465\n",
      "Iteration 199, loss = 0.00636166\n",
      "Iteration 200, loss = 0.00662323\n",
      "Iteration 201, loss = 0.00652181\n",
      "Iteration 202, loss = 0.00658084\n",
      "Iteration 203, loss = 0.00644493\n",
      "Iteration 204, loss = 0.00630067\n",
      "Iteration 205, loss = 0.00628550\n",
      "Iteration 206, loss = 0.00609942\n",
      "Iteration 207, loss = 0.00613283\n",
      "Iteration 208, loss = 0.00608221\n",
      "Iteration 209, loss = 0.00596733\n",
      "Iteration 210, loss = 0.00623433\n",
      "Iteration 211, loss = 0.00650523\n",
      "Iteration 212, loss = 0.00654032\n",
      "Iteration 213, loss = 0.00584242\n",
      "Iteration 214, loss = 0.00583277\n",
      "Iteration 215, loss = 0.00662595\n",
      "Iteration 216, loss = 0.00609126\n",
      "Iteration 217, loss = 0.00599429\n",
      "Iteration 218, loss = 0.00553053\n",
      "Iteration 219, loss = 0.00571590\n",
      "Iteration 220, loss = 0.00572392\n",
      "Iteration 221, loss = 0.00565895\n",
      "Iteration 222, loss = 0.00560776\n",
      "Iteration 223, loss = 0.00552584\n",
      "Iteration 224, loss = 0.00558454\n",
      "Iteration 225, loss = 0.00575839\n",
      "Iteration 226, loss = 0.00546903\n",
      "Iteration 227, loss = 0.00550655\n",
      "Iteration 228, loss = 0.00539550\n",
      "Iteration 229, loss = 0.00536780\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.43048721\n",
      "Iteration 2, loss = 0.36109055\n",
      "Iteration 3, loss = 0.32355330\n",
      "Iteration 4, loss = 0.29119066\n",
      "Iteration 5, loss = 0.26942743\n",
      "Iteration 6, loss = 0.24674110\n",
      "Iteration 7, loss = 0.22398220\n",
      "Iteration 8, loss = 0.20087055\n",
      "Iteration 9, loss = 0.18118639\n",
      "Iteration 10, loss = 0.16524084\n",
      "Iteration 11, loss = 0.14924242\n",
      "Iteration 12, loss = 0.13371582\n",
      "Iteration 13, loss = 0.11733614\n",
      "Iteration 14, loss = 0.10576167\n",
      "Iteration 15, loss = 0.09459584\n",
      "Iteration 16, loss = 0.08553221\n",
      "Iteration 17, loss = 0.07759792\n",
      "Iteration 18, loss = 0.07187788\n",
      "Iteration 19, loss = 0.06702825\n",
      "Iteration 20, loss = 0.06234146\n",
      "Iteration 21, loss = 0.05826208\n",
      "Iteration 22, loss = 0.05677761\n",
      "Iteration 23, loss = 0.05364394\n",
      "Iteration 24, loss = 0.05182042\n",
      "Iteration 25, loss = 0.04959008\n",
      "Iteration 26, loss = 0.04862976\n",
      "Iteration 27, loss = 0.04671012\n",
      "Iteration 28, loss = 0.04538081\n",
      "Iteration 29, loss = 0.04455000\n",
      "Iteration 30, loss = 0.04293018\n",
      "Iteration 31, loss = 0.04154061\n",
      "Iteration 32, loss = 0.04038392\n",
      "Iteration 33, loss = 0.03952581\n",
      "Iteration 34, loss = 0.03860155\n",
      "Iteration 35, loss = 0.03786199\n",
      "Iteration 36, loss = 0.03685177\n",
      "Iteration 37, loss = 0.03623576\n",
      "Iteration 38, loss = 0.03537604\n",
      "Iteration 39, loss = 0.03459961\n",
      "Iteration 40, loss = 0.03426476\n",
      "Iteration 41, loss = 0.03340836\n",
      "Iteration 42, loss = 0.03328941\n",
      "Iteration 43, loss = 0.03260458\n",
      "Iteration 44, loss = 0.03179702\n",
      "Iteration 45, loss = 0.03135735\n",
      "Iteration 46, loss = 0.03062545\n",
      "Iteration 47, loss = 0.03052286\n",
      "Iteration 48, loss = 0.03030252\n",
      "Iteration 49, loss = 0.02947922\n",
      "Iteration 50, loss = 0.02907981\n",
      "Iteration 51, loss = 0.02820820\n",
      "Iteration 52, loss = 0.02789345\n",
      "Iteration 53, loss = 0.02739725\n",
      "Iteration 54, loss = 0.02694178\n",
      "Iteration 55, loss = 0.02669106\n",
      "Iteration 56, loss = 0.02593641\n",
      "Iteration 57, loss = 0.02550727\n",
      "Iteration 58, loss = 0.02555456\n",
      "Iteration 59, loss = 0.02478995\n",
      "Iteration 60, loss = 0.02486482\n",
      "Iteration 61, loss = 0.02421162\n",
      "Iteration 62, loss = 0.02437107\n",
      "Iteration 63, loss = 0.02368174\n",
      "Iteration 64, loss = 0.02371201\n",
      "Iteration 65, loss = 0.02305493\n",
      "Iteration 66, loss = 0.02262985\n",
      "Iteration 67, loss = 0.02264314\n",
      "Iteration 68, loss = 0.02213185\n",
      "Iteration 69, loss = 0.02182008\n",
      "Iteration 70, loss = 0.02120658\n",
      "Iteration 71, loss = 0.02113034\n",
      "Iteration 72, loss = 0.02076804\n",
      "Iteration 73, loss = 0.02073030\n",
      "Iteration 74, loss = 0.02065077\n",
      "Iteration 75, loss = 0.02050420\n",
      "Iteration 76, loss = 0.02033557\n",
      "Iteration 77, loss = 0.01958893\n",
      "Iteration 78, loss = 0.01927291\n",
      "Iteration 79, loss = 0.01927556\n",
      "Iteration 80, loss = 0.01906215\n",
      "Iteration 81, loss = 0.01857587\n",
      "Iteration 82, loss = 0.01830226\n",
      "Iteration 83, loss = 0.01854804\n",
      "Iteration 84, loss = 0.01808689\n",
      "Iteration 85, loss = 0.01803398\n",
      "Iteration 86, loss = 0.01809958\n",
      "Iteration 87, loss = 0.01731435\n",
      "Iteration 88, loss = 0.01774118\n",
      "Iteration 89, loss = 0.01775536\n",
      "Iteration 90, loss = 0.01715855\n",
      "Iteration 91, loss = 0.01729349\n",
      "Iteration 92, loss = 0.01685512\n",
      "Iteration 93, loss = 0.01634142\n",
      "Iteration 94, loss = 0.01616200\n",
      "Iteration 95, loss = 0.01616744\n",
      "Iteration 96, loss = 0.01585192\n",
      "Iteration 97, loss = 0.01546520\n",
      "Iteration 98, loss = 0.01568152\n",
      "Iteration 99, loss = 0.01584696\n",
      "Iteration 100, loss = 0.01530532\n",
      "Iteration 101, loss = 0.01494002\n",
      "Iteration 102, loss = 0.01503592\n",
      "Iteration 103, loss = 0.01493276\n",
      "Iteration 104, loss = 0.01434054\n",
      "Iteration 105, loss = 0.01464768\n",
      "Iteration 106, loss = 0.01415149\n",
      "Iteration 107, loss = 0.01420611\n",
      "Iteration 108, loss = 0.01381385\n",
      "Iteration 109, loss = 0.01407475\n",
      "Iteration 110, loss = 0.01407793\n",
      "Iteration 111, loss = 0.01372022\n",
      "Iteration 112, loss = 0.01360899\n",
      "Iteration 113, loss = 0.01353528\n",
      "Iteration 114, loss = 0.01322780\n",
      "Iteration 115, loss = 0.01316768\n",
      "Iteration 116, loss = 0.01339850\n",
      "Iteration 117, loss = 0.01280023\n",
      "Iteration 118, loss = 0.01268961\n",
      "Iteration 119, loss = 0.01266793\n",
      "Iteration 120, loss = 0.01243698\n",
      "Iteration 121, loss = 0.01218626\n",
      "Iteration 122, loss = 0.01212413\n",
      "Iteration 123, loss = 0.01206470\n",
      "Iteration 124, loss = 0.01202111\n",
      "Iteration 125, loss = 0.01179874\n",
      "Iteration 126, loss = 0.01183155\n",
      "Iteration 127, loss = 0.01149025\n",
      "Iteration 128, loss = 0.01144712\n",
      "Iteration 129, loss = 0.01116914\n",
      "Iteration 130, loss = 0.01119439\n",
      "Iteration 131, loss = 0.01100045\n",
      "Iteration 132, loss = 0.01102419\n",
      "Iteration 133, loss = 0.01084107\n",
      "Iteration 134, loss = 0.01082016\n",
      "Iteration 135, loss = 0.01101565\n",
      "Iteration 136, loss = 0.01108905\n",
      "Iteration 137, loss = 0.01047321\n",
      "Iteration 138, loss = 0.01078948\n",
      "Iteration 139, loss = 0.01032171\n",
      "Iteration 140, loss = 0.01002097\n",
      "Iteration 141, loss = 0.01017244\n",
      "Iteration 142, loss = 0.00988665\n",
      "Iteration 143, loss = 0.00981266\n",
      "Iteration 144, loss = 0.00989614\n",
      "Iteration 145, loss = 0.00969941\n",
      "Iteration 146, loss = 0.00944948\n",
      "Iteration 147, loss = 0.00953575\n",
      "Iteration 148, loss = 0.00953454\n",
      "Iteration 149, loss = 0.00925425\n",
      "Iteration 150, loss = 0.00951511\n",
      "Iteration 151, loss = 0.00910654\n",
      "Iteration 152, loss = 0.00899637\n",
      "Iteration 153, loss = 0.00916081\n",
      "Iteration 154, loss = 0.00891642\n",
      "Iteration 155, loss = 0.00925240\n",
      "Iteration 156, loss = 0.00883680\n",
      "Iteration 157, loss = 0.00870518\n",
      "Iteration 158, loss = 0.00871524\n",
      "Iteration 159, loss = 0.00848483\n",
      "Iteration 160, loss = 0.00837690\n",
      "Iteration 161, loss = 0.00830034\n",
      "Iteration 162, loss = 0.00832265\n",
      "Iteration 163, loss = 0.00827043\n",
      "Iteration 164, loss = 0.00816409\n",
      "Iteration 165, loss = 0.00814815\n",
      "Iteration 166, loss = 0.00808471\n",
      "Iteration 167, loss = 0.00786809\n",
      "Iteration 168, loss = 0.00799153\n",
      "Iteration 169, loss = 0.00774073\n",
      "Iteration 170, loss = 0.00777938\n",
      "Iteration 171, loss = 0.00783938\n",
      "Iteration 172, loss = 0.00821987\n",
      "Iteration 173, loss = 0.00814484\n",
      "Iteration 174, loss = 0.00802928\n",
      "Iteration 175, loss = 0.00765368\n",
      "Iteration 176, loss = 0.00769264\n",
      "Iteration 177, loss = 0.00783377\n",
      "Iteration 178, loss = 0.00754750\n",
      "Iteration 179, loss = 0.00753631\n",
      "Iteration 180, loss = 0.00742008\n",
      "Iteration 181, loss = 0.00729296\n",
      "Iteration 182, loss = 0.00710889\n",
      "Iteration 183, loss = 0.00719672\n",
      "Iteration 184, loss = 0.00740892\n",
      "Iteration 185, loss = 0.00730314\n",
      "Iteration 186, loss = 0.00742665\n",
      "Iteration 187, loss = 0.00724305\n",
      "Iteration 188, loss = 0.00706319\n",
      "Iteration 189, loss = 0.00684536\n",
      "Iteration 190, loss = 0.00737205\n",
      "Iteration 191, loss = 0.00724770\n",
      "Iteration 192, loss = 0.00688657\n",
      "Iteration 193, loss = 0.00672126\n",
      "Iteration 194, loss = 0.00665678\n",
      "Iteration 195, loss = 0.00680133\n",
      "Iteration 196, loss = 0.00667435\n",
      "Iteration 197, loss = 0.00645018\n",
      "Iteration 198, loss = 0.00636465\n",
      "Iteration 199, loss = 0.00636166\n",
      "Iteration 200, loss = 0.00662323\n",
      "Iteration 201, loss = 0.00652181\n",
      "Iteration 202, loss = 0.00658084\n",
      "Iteration 203, loss = 0.00644493\n",
      "Iteration 204, loss = 0.00630067\n",
      "Iteration 205, loss = 0.00628550\n",
      "Iteration 206, loss = 0.00609942\n",
      "Iteration 207, loss = 0.00613283\n",
      "Iteration 208, loss = 0.00608221\n",
      "Iteration 209, loss = 0.00596733\n",
      "Iteration 210, loss = 0.00623433\n",
      "Iteration 211, loss = 0.00650523\n",
      "Iteration 212, loss = 0.00654032\n",
      "Iteration 213, loss = 0.00584242\n",
      "Iteration 214, loss = 0.00583277\n",
      "Iteration 215, loss = 0.00662595\n",
      "Iteration 216, loss = 0.00609126\n",
      "Iteration 217, loss = 0.00599429\n",
      "Iteration 218, loss = 0.00553053\n",
      "Iteration 219, loss = 0.00571590\n",
      "Iteration 220, loss = 0.00572392\n",
      "Iteration 221, loss = 0.00565895\n",
      "Iteration 222, loss = 0.00560776\n",
      "Iteration 223, loss = 0.00552584\n",
      "Iteration 224, loss = 0.00558454\n",
      "Iteration 225, loss = 0.00575839\n",
      "Iteration 226, loss = 0.00546903\n",
      "Iteration 227, loss = 0.00550655\n",
      "Iteration 228, loss = 0.00539550\n",
      "Iteration 229, loss = 0.00536780\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.43168298\n",
      "Iteration 2, loss = 0.36052413\n",
      "Iteration 3, loss = 0.31436426\n",
      "Iteration 4, loss = 0.27815430\n",
      "Iteration 5, loss = 0.25897659\n",
      "Iteration 6, loss = 0.23345266\n",
      "Iteration 7, loss = 0.21012096\n",
      "Iteration 8, loss = 0.19030113\n",
      "Iteration 9, loss = 0.16842060\n",
      "Iteration 10, loss = 0.15191353\n",
      "Iteration 11, loss = 0.13610732\n",
      "Iteration 12, loss = 0.12036716\n",
      "Iteration 13, loss = 0.10956760\n",
      "Iteration 14, loss = 0.09825659\n",
      "Iteration 15, loss = 0.08886668\n",
      "Iteration 16, loss = 0.08123913\n",
      "Iteration 17, loss = 0.07597120\n",
      "Iteration 18, loss = 0.07156552\n",
      "Iteration 19, loss = 0.06813930\n",
      "Iteration 20, loss = 0.06312796\n",
      "Iteration 21, loss = 0.06083203\n",
      "Iteration 22, loss = 0.05728367\n",
      "Iteration 23, loss = 0.05495691\n",
      "Iteration 24, loss = 0.05275817\n",
      "Iteration 25, loss = 0.05098278\n",
      "Iteration 26, loss = 0.04894737\n",
      "Iteration 27, loss = 0.04705199\n",
      "Iteration 28, loss = 0.04507940\n",
      "Iteration 29, loss = 0.04392774\n",
      "Iteration 30, loss = 0.04227658\n",
      "Iteration 31, loss = 0.04111365\n",
      "Iteration 32, loss = 0.03988434\n",
      "Iteration 33, loss = 0.03870370\n",
      "Iteration 34, loss = 0.03827122\n",
      "Iteration 35, loss = 0.03714917\n",
      "Iteration 36, loss = 0.03582185\n",
      "Iteration 37, loss = 0.03496025\n",
      "Iteration 38, loss = 0.03400059\n",
      "Iteration 39, loss = 0.03333800\n",
      "Iteration 40, loss = 0.03255406\n",
      "Iteration 41, loss = 0.03199355\n",
      "Iteration 42, loss = 0.03121274\n",
      "Iteration 43, loss = 0.03054475\n",
      "Iteration 44, loss = 0.02999562\n",
      "Iteration 45, loss = 0.02942214\n",
      "Iteration 46, loss = 0.02938845\n",
      "Iteration 47, loss = 0.02843780\n",
      "Iteration 48, loss = 0.02784419\n",
      "Iteration 49, loss = 0.02779766\n",
      "Iteration 50, loss = 0.02713156\n",
      "Iteration 51, loss = 0.02669204\n",
      "Iteration 52, loss = 0.02644062\n",
      "Iteration 53, loss = 0.02596614\n",
      "Iteration 54, loss = 0.02567495\n",
      "Iteration 55, loss = 0.02511672\n",
      "Iteration 56, loss = 0.02535058\n",
      "Iteration 57, loss = 0.02433304\n",
      "Iteration 58, loss = 0.02399421\n",
      "Iteration 59, loss = 0.02417508\n",
      "Iteration 60, loss = 0.02347181\n",
      "Iteration 61, loss = 0.02324557\n",
      "Iteration 62, loss = 0.02297042\n",
      "Iteration 63, loss = 0.02253560\n",
      "Iteration 64, loss = 0.02224464\n",
      "Iteration 65, loss = 0.02200164\n",
      "Iteration 66, loss = 0.02184876\n",
      "Iteration 67, loss = 0.02152931\n",
      "Iteration 68, loss = 0.02152218\n",
      "Iteration 69, loss = 0.02102253\n",
      "Iteration 70, loss = 0.02079791\n",
      "Iteration 71, loss = 0.02080047\n",
      "Iteration 72, loss = 0.02023340\n",
      "Iteration 73, loss = 0.02014817\n",
      "Iteration 74, loss = 0.01984204\n",
      "Iteration 75, loss = 0.01959810\n",
      "Iteration 76, loss = 0.01942458\n",
      "Iteration 77, loss = 0.01935932\n",
      "Iteration 78, loss = 0.01906331\n",
      "Iteration 79, loss = 0.01895763\n",
      "Iteration 80, loss = 0.01876791\n",
      "Iteration 81, loss = 0.01842165\n",
      "Iteration 82, loss = 0.01832082\n",
      "Iteration 83, loss = 0.01804926\n",
      "Iteration 84, loss = 0.01805959\n",
      "Iteration 85, loss = 0.01779711\n",
      "Iteration 86, loss = 0.01771959\n",
      "Iteration 87, loss = 0.01737430\n",
      "Iteration 88, loss = 0.01725890\n",
      "Iteration 89, loss = 0.01703936\n",
      "Iteration 90, loss = 0.01696998\n",
      "Iteration 91, loss = 0.01687968\n",
      "Iteration 92, loss = 0.01653032\n",
      "Iteration 93, loss = 0.01648110\n",
      "Iteration 94, loss = 0.01619928\n",
      "Iteration 95, loss = 0.01610716\n",
      "Iteration 96, loss = 0.01624432\n",
      "Iteration 97, loss = 0.01572291\n",
      "Iteration 98, loss = 0.01576153\n",
      "Iteration 99, loss = 0.01532434\n",
      "Iteration 100, loss = 0.01530283\n",
      "Iteration 101, loss = 0.01524756\n",
      "Iteration 102, loss = 0.01502781\n",
      "Iteration 103, loss = 0.01473212\n",
      "Iteration 104, loss = 0.01471428\n",
      "Iteration 105, loss = 0.01447812\n",
      "Iteration 106, loss = 0.01465087\n",
      "Iteration 107, loss = 0.01427161\n",
      "Iteration 108, loss = 0.01442274\n",
      "Iteration 109, loss = 0.01428984\n",
      "Iteration 110, loss = 0.01382646\n",
      "Iteration 111, loss = 0.01378216\n",
      "Iteration 112, loss = 0.01364079\n",
      "Iteration 113, loss = 0.01339667\n",
      "Iteration 114, loss = 0.01338568\n",
      "Iteration 115, loss = 0.01320251\n",
      "Iteration 116, loss = 0.01312039\n",
      "Iteration 117, loss = 0.01292097\n",
      "Iteration 118, loss = 0.01299827\n",
      "Iteration 119, loss = 0.01265232\n",
      "Iteration 120, loss = 0.01267979\n",
      "Iteration 121, loss = 0.01259464\n",
      "Iteration 122, loss = 0.01242948\n",
      "Iteration 123, loss = 0.01228055\n",
      "Iteration 124, loss = 0.01212893\n",
      "Iteration 125, loss = 0.01210113\n",
      "Iteration 126, loss = 0.01204301\n",
      "Iteration 127, loss = 0.01185572\n",
      "Iteration 128, loss = 0.01175791\n",
      "Iteration 129, loss = 0.01190846\n",
      "Iteration 130, loss = 0.01200159\n",
      "Iteration 131, loss = 0.01187432\n",
      "Iteration 132, loss = 0.01162216\n",
      "Iteration 133, loss = 0.01156662\n",
      "Iteration 134, loss = 0.01166957\n",
      "Iteration 135, loss = 0.01142656\n",
      "Iteration 136, loss = 0.01109849\n",
      "Iteration 137, loss = 0.01143940\n",
      "Iteration 138, loss = 0.01129759\n",
      "Iteration 139, loss = 0.01129039\n",
      "Iteration 140, loss = 0.01141832\n",
      "Iteration 141, loss = 0.01088094\n",
      "Iteration 142, loss = 0.01098623\n",
      "Iteration 143, loss = 0.01054220\n",
      "Iteration 144, loss = 0.01058157\n",
      "Iteration 145, loss = 0.01033534\n",
      "Iteration 146, loss = 0.01042499\n",
      "Iteration 147, loss = 0.01055647\n",
      "Iteration 148, loss = 0.01028835\n",
      "Iteration 149, loss = 0.01027746\n",
      "Iteration 150, loss = 0.01010250\n",
      "Iteration 151, loss = 0.01018218\n",
      "Iteration 152, loss = 0.01024130\n",
      "Iteration 153, loss = 0.00973548\n",
      "Iteration 154, loss = 0.00967418\n",
      "Iteration 155, loss = 0.00956658\n",
      "Iteration 156, loss = 0.00959252\n",
      "Iteration 157, loss = 0.00955948\n",
      "Iteration 158, loss = 0.00948864\n",
      "Iteration 159, loss = 0.00939169\n",
      "Iteration 160, loss = 0.00927510\n",
      "Iteration 161, loss = 0.00918076\n",
      "Iteration 162, loss = 0.00912627\n",
      "Iteration 163, loss = 0.00899544\n",
      "Iteration 164, loss = 0.00893633\n",
      "Iteration 165, loss = 0.00884927\n",
      "Iteration 166, loss = 0.00881952\n",
      "Iteration 167, loss = 0.00892351\n",
      "Iteration 168, loss = 0.00920759\n",
      "Iteration 169, loss = 0.00887860\n",
      "Iteration 170, loss = 0.00911147\n",
      "Iteration 171, loss = 0.00873130\n",
      "Iteration 172, loss = 0.00865433\n",
      "Iteration 173, loss = 0.00870216\n",
      "Iteration 174, loss = 0.00868253\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.43168298\n",
      "Iteration 2, loss = 0.36052413\n",
      "Iteration 3, loss = 0.31436426\n",
      "Iteration 4, loss = 0.27815430\n",
      "Iteration 5, loss = 0.25897659\n",
      "Iteration 6, loss = 0.23345266\n",
      "Iteration 7, loss = 0.21012096\n",
      "Iteration 8, loss = 0.19030113\n",
      "Iteration 9, loss = 0.16842060\n",
      "Iteration 10, loss = 0.15191353\n",
      "Iteration 11, loss = 0.13610732\n",
      "Iteration 12, loss = 0.12036716\n",
      "Iteration 13, loss = 0.10956760\n",
      "Iteration 14, loss = 0.09825659\n",
      "Iteration 15, loss = 0.08886668\n",
      "Iteration 16, loss = 0.08123913\n",
      "Iteration 17, loss = 0.07597120\n",
      "Iteration 18, loss = 0.07156552\n",
      "Iteration 19, loss = 0.06813930\n",
      "Iteration 20, loss = 0.06312796\n",
      "Iteration 21, loss = 0.06083203\n",
      "Iteration 22, loss = 0.05728367\n",
      "Iteration 23, loss = 0.05495691\n",
      "Iteration 24, loss = 0.05275817\n",
      "Iteration 25, loss = 0.05098278\n",
      "Iteration 26, loss = 0.04894737\n",
      "Iteration 27, loss = 0.04705199\n",
      "Iteration 28, loss = 0.04507940\n",
      "Iteration 29, loss = 0.04392774\n",
      "Iteration 30, loss = 0.04227658\n",
      "Iteration 31, loss = 0.04111365\n",
      "Iteration 32, loss = 0.03988434\n",
      "Iteration 33, loss = 0.03870370\n",
      "Iteration 34, loss = 0.03827122\n",
      "Iteration 35, loss = 0.03714917\n",
      "Iteration 36, loss = 0.03582185\n",
      "Iteration 37, loss = 0.03496025\n",
      "Iteration 38, loss = 0.03400059\n",
      "Iteration 39, loss = 0.03333800\n",
      "Iteration 40, loss = 0.03255406\n",
      "Iteration 41, loss = 0.03199355\n",
      "Iteration 42, loss = 0.03121274\n",
      "Iteration 43, loss = 0.03054475\n",
      "Iteration 44, loss = 0.02999562\n",
      "Iteration 45, loss = 0.02942214\n",
      "Iteration 46, loss = 0.02938845\n",
      "Iteration 47, loss = 0.02843780\n",
      "Iteration 48, loss = 0.02784419\n",
      "Iteration 49, loss = 0.02779766\n",
      "Iteration 50, loss = 0.02713156\n",
      "Iteration 51, loss = 0.02669204\n",
      "Iteration 52, loss = 0.02644062\n",
      "Iteration 53, loss = 0.02596614\n",
      "Iteration 54, loss = 0.02567495\n",
      "Iteration 55, loss = 0.02511672\n",
      "Iteration 56, loss = 0.02535058\n",
      "Iteration 57, loss = 0.02433304\n",
      "Iteration 58, loss = 0.02399421\n",
      "Iteration 59, loss = 0.02417508\n",
      "Iteration 60, loss = 0.02347181\n",
      "Iteration 61, loss = 0.02324557\n",
      "Iteration 62, loss = 0.02297042\n",
      "Iteration 63, loss = 0.02253560\n",
      "Iteration 64, loss = 0.02224464\n",
      "Iteration 65, loss = 0.02200164\n",
      "Iteration 66, loss = 0.02184876\n",
      "Iteration 67, loss = 0.02152931\n",
      "Iteration 68, loss = 0.02152218\n",
      "Iteration 69, loss = 0.02102253\n",
      "Iteration 70, loss = 0.02079791\n",
      "Iteration 71, loss = 0.02080047\n",
      "Iteration 72, loss = 0.02023340\n",
      "Iteration 73, loss = 0.02014817\n",
      "Iteration 74, loss = 0.01984204\n",
      "Iteration 75, loss = 0.01959810\n",
      "Iteration 76, loss = 0.01942458\n",
      "Iteration 77, loss = 0.01935932\n",
      "Iteration 78, loss = 0.01906331\n",
      "Iteration 79, loss = 0.01895763\n",
      "Iteration 80, loss = 0.01876791\n",
      "Iteration 81, loss = 0.01842165\n",
      "Iteration 82, loss = 0.01832082\n",
      "Iteration 83, loss = 0.01804926\n",
      "Iteration 84, loss = 0.01805959\n",
      "Iteration 85, loss = 0.01779711\n",
      "Iteration 86, loss = 0.01771959\n",
      "Iteration 87, loss = 0.01737430\n",
      "Iteration 88, loss = 0.01725890\n",
      "Iteration 89, loss = 0.01703936\n",
      "Iteration 90, loss = 0.01696998\n",
      "Iteration 91, loss = 0.01687968\n",
      "Iteration 92, loss = 0.01653032\n",
      "Iteration 93, loss = 0.01648110\n",
      "Iteration 94, loss = 0.01619928\n",
      "Iteration 95, loss = 0.01610716\n",
      "Iteration 96, loss = 0.01624432\n",
      "Iteration 97, loss = 0.01572291\n",
      "Iteration 98, loss = 0.01576153\n",
      "Iteration 99, loss = 0.01532434\n",
      "Iteration 100, loss = 0.01530283\n",
      "Iteration 101, loss = 0.01524756\n",
      "Iteration 102, loss = 0.01502781\n",
      "Iteration 103, loss = 0.01473212\n",
      "Iteration 104, loss = 0.01471428\n",
      "Iteration 105, loss = 0.01447812\n",
      "Iteration 106, loss = 0.01465087\n",
      "Iteration 107, loss = 0.01427161\n",
      "Iteration 108, loss = 0.01442274\n",
      "Iteration 109, loss = 0.01428984\n",
      "Iteration 110, loss = 0.01382646\n",
      "Iteration 111, loss = 0.01378216\n",
      "Iteration 112, loss = 0.01364079\n",
      "Iteration 113, loss = 0.01339667\n",
      "Iteration 114, loss = 0.01338568\n",
      "Iteration 115, loss = 0.01320251\n",
      "Iteration 116, loss = 0.01312039\n",
      "Iteration 117, loss = 0.01292097\n",
      "Iteration 118, loss = 0.01299827\n",
      "Iteration 119, loss = 0.01265232\n",
      "Iteration 120, loss = 0.01267979\n",
      "Iteration 121, loss = 0.01259464\n",
      "Iteration 122, loss = 0.01242948\n",
      "Iteration 123, loss = 0.01228055\n",
      "Iteration 124, loss = 0.01212893\n",
      "Iteration 125, loss = 0.01210113\n",
      "Iteration 126, loss = 0.01204301\n",
      "Iteration 127, loss = 0.01185572\n",
      "Iteration 128, loss = 0.01175791\n",
      "Iteration 129, loss = 0.01190846\n",
      "Iteration 130, loss = 0.01200159\n",
      "Iteration 131, loss = 0.01187432\n",
      "Iteration 132, loss = 0.01162216\n",
      "Iteration 133, loss = 0.01156662\n",
      "Iteration 134, loss = 0.01166957\n",
      "Iteration 135, loss = 0.01142656\n",
      "Iteration 136, loss = 0.01109849\n",
      "Iteration 137, loss = 0.01143940\n",
      "Iteration 138, loss = 0.01129759\n",
      "Iteration 139, loss = 0.01129039\n",
      "Iteration 140, loss = 0.01141832\n",
      "Iteration 141, loss = 0.01088094\n",
      "Iteration 142, loss = 0.01098623\n",
      "Iteration 143, loss = 0.01054220\n",
      "Iteration 144, loss = 0.01058157\n",
      "Iteration 145, loss = 0.01033534\n",
      "Iteration 146, loss = 0.01042499\n",
      "Iteration 147, loss = 0.01055647\n",
      "Iteration 148, loss = 0.01028835\n",
      "Iteration 149, loss = 0.01027746\n",
      "Iteration 150, loss = 0.01010250\n",
      "Iteration 151, loss = 0.01018218\n",
      "Iteration 152, loss = 0.01024130\n",
      "Iteration 153, loss = 0.00973548\n",
      "Iteration 154, loss = 0.00967418\n",
      "Iteration 155, loss = 0.00956658\n",
      "Iteration 156, loss = 0.00959252\n",
      "Iteration 157, loss = 0.00955948\n",
      "Iteration 158, loss = 0.00948864\n",
      "Iteration 159, loss = 0.00939169\n",
      "Iteration 160, loss = 0.00927510\n",
      "Iteration 161, loss = 0.00918076\n",
      "Iteration 162, loss = 0.00912627\n",
      "Iteration 163, loss = 0.00899544\n",
      "Iteration 164, loss = 0.00893633\n",
      "Iteration 165, loss = 0.00884927\n",
      "Iteration 166, loss = 0.00881952\n",
      "Iteration 167, loss = 0.00892351\n",
      "Iteration 168, loss = 0.00920759\n",
      "Iteration 169, loss = 0.00887860\n",
      "Iteration 170, loss = 0.00911147\n",
      "Iteration 171, loss = 0.00873130\n",
      "Iteration 172, loss = 0.00865433\n",
      "Iteration 173, loss = 0.00870216\n",
      "Iteration 174, loss = 0.00868253\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.43168298\n",
      "Iteration 2, loss = 0.36052413\n",
      "Iteration 3, loss = 0.31436426\n",
      "Iteration 4, loss = 0.27815430\n",
      "Iteration 5, loss = 0.25897659\n",
      "Iteration 6, loss = 0.23345266\n",
      "Iteration 7, loss = 0.21012096\n",
      "Iteration 8, loss = 0.19030113\n",
      "Iteration 9, loss = 0.16842060\n",
      "Iteration 10, loss = 0.15191353\n",
      "Iteration 11, loss = 0.13610732\n",
      "Iteration 12, loss = 0.12036716\n",
      "Iteration 13, loss = 0.10956760\n",
      "Iteration 14, loss = 0.09825659\n",
      "Iteration 15, loss = 0.08886668\n",
      "Iteration 16, loss = 0.08123913\n",
      "Iteration 17, loss = 0.07597120\n",
      "Iteration 18, loss = 0.07156552\n",
      "Iteration 19, loss = 0.06813930\n",
      "Iteration 20, loss = 0.06312796\n",
      "Iteration 21, loss = 0.06083203\n",
      "Iteration 22, loss = 0.05728367\n",
      "Iteration 23, loss = 0.05495691\n",
      "Iteration 24, loss = 0.05275817\n",
      "Iteration 25, loss = 0.05098278\n",
      "Iteration 26, loss = 0.04894737\n",
      "Iteration 27, loss = 0.04705199\n",
      "Iteration 28, loss = 0.04507940\n",
      "Iteration 29, loss = 0.04392774\n",
      "Iteration 30, loss = 0.04227658\n",
      "Iteration 31, loss = 0.04111365\n",
      "Iteration 32, loss = 0.03988434\n",
      "Iteration 33, loss = 0.03870370\n",
      "Iteration 34, loss = 0.03827122\n",
      "Iteration 35, loss = 0.03714917\n",
      "Iteration 36, loss = 0.03582185\n",
      "Iteration 37, loss = 0.03496025\n",
      "Iteration 38, loss = 0.03400059\n",
      "Iteration 39, loss = 0.03333800\n",
      "Iteration 40, loss = 0.03255406\n",
      "Iteration 41, loss = 0.03199355\n",
      "Iteration 42, loss = 0.03121274\n",
      "Iteration 43, loss = 0.03054475\n",
      "Iteration 44, loss = 0.02999562\n",
      "Iteration 45, loss = 0.02942214\n",
      "Iteration 46, loss = 0.02938845\n",
      "Iteration 47, loss = 0.02843780\n",
      "Iteration 48, loss = 0.02784419\n",
      "Iteration 49, loss = 0.02779766\n",
      "Iteration 50, loss = 0.02713156\n",
      "Iteration 51, loss = 0.02669204\n",
      "Iteration 52, loss = 0.02644062\n",
      "Iteration 53, loss = 0.02596614\n",
      "Iteration 54, loss = 0.02567495\n",
      "Iteration 55, loss = 0.02511672\n",
      "Iteration 56, loss = 0.02535058\n",
      "Iteration 57, loss = 0.02433304\n",
      "Iteration 58, loss = 0.02399421\n",
      "Iteration 59, loss = 0.02417508\n",
      "Iteration 60, loss = 0.02347181\n",
      "Iteration 61, loss = 0.02324557\n",
      "Iteration 62, loss = 0.02297042\n",
      "Iteration 63, loss = 0.02253560\n",
      "Iteration 64, loss = 0.02224464\n",
      "Iteration 65, loss = 0.02200164\n",
      "Iteration 66, loss = 0.02184876\n",
      "Iteration 67, loss = 0.02152931\n",
      "Iteration 68, loss = 0.02152218\n",
      "Iteration 69, loss = 0.02102253\n",
      "Iteration 70, loss = 0.02079791\n",
      "Iteration 71, loss = 0.02080047\n",
      "Iteration 72, loss = 0.02023340\n",
      "Iteration 73, loss = 0.02014817\n",
      "Iteration 74, loss = 0.01984204\n",
      "Iteration 75, loss = 0.01959810\n",
      "Iteration 76, loss = 0.01942458\n",
      "Iteration 77, loss = 0.01935932\n",
      "Iteration 78, loss = 0.01906331\n",
      "Iteration 79, loss = 0.01895763\n",
      "Iteration 80, loss = 0.01876791\n",
      "Iteration 81, loss = 0.01842165\n",
      "Iteration 82, loss = 0.01832082\n",
      "Iteration 83, loss = 0.01804926\n",
      "Iteration 84, loss = 0.01805959\n",
      "Iteration 85, loss = 0.01779711\n",
      "Iteration 86, loss = 0.01771959\n",
      "Iteration 87, loss = 0.01737430\n",
      "Iteration 88, loss = 0.01725890\n",
      "Iteration 89, loss = 0.01703936\n",
      "Iteration 90, loss = 0.01696998\n",
      "Iteration 91, loss = 0.01687968\n",
      "Iteration 92, loss = 0.01653032\n",
      "Iteration 93, loss = 0.01648110\n",
      "Iteration 94, loss = 0.01619928\n",
      "Iteration 95, loss = 0.01610716\n",
      "Iteration 96, loss = 0.01624432\n",
      "Iteration 97, loss = 0.01572291\n",
      "Iteration 98, loss = 0.01576153\n",
      "Iteration 99, loss = 0.01532434\n",
      "Iteration 100, loss = 0.01530283\n",
      "Iteration 101, loss = 0.01524756\n",
      "Iteration 102, loss = 0.01502781\n",
      "Iteration 103, loss = 0.01473212\n",
      "Iteration 104, loss = 0.01471428\n",
      "Iteration 105, loss = 0.01447812\n",
      "Iteration 106, loss = 0.01465087\n",
      "Iteration 107, loss = 0.01427161\n",
      "Iteration 108, loss = 0.01442274\n",
      "Iteration 109, loss = 0.01428984\n",
      "Iteration 110, loss = 0.01382646\n",
      "Iteration 111, loss = 0.01378216\n",
      "Iteration 112, loss = 0.01364079\n",
      "Iteration 113, loss = 0.01339667\n",
      "Iteration 114, loss = 0.01338568\n",
      "Iteration 115, loss = 0.01320251\n",
      "Iteration 116, loss = 0.01312039\n",
      "Iteration 117, loss = 0.01292097\n",
      "Iteration 118, loss = 0.01299827\n",
      "Iteration 119, loss = 0.01265232\n",
      "Iteration 120, loss = 0.01267979\n",
      "Iteration 121, loss = 0.01259464\n",
      "Iteration 122, loss = 0.01242948\n",
      "Iteration 123, loss = 0.01228055\n",
      "Iteration 124, loss = 0.01212893\n",
      "Iteration 125, loss = 0.01210113\n",
      "Iteration 126, loss = 0.01204301\n",
      "Iteration 127, loss = 0.01185572\n",
      "Iteration 128, loss = 0.01175791\n",
      "Iteration 129, loss = 0.01190846\n",
      "Iteration 130, loss = 0.01200159\n",
      "Iteration 131, loss = 0.01187432\n",
      "Iteration 132, loss = 0.01162216\n",
      "Iteration 133, loss = 0.01156662\n",
      "Iteration 134, loss = 0.01166957\n",
      "Iteration 135, loss = 0.01142656\n",
      "Iteration 136, loss = 0.01109849\n",
      "Iteration 137, loss = 0.01143940\n",
      "Iteration 138, loss = 0.01129759\n",
      "Iteration 139, loss = 0.01129039\n",
      "Iteration 140, loss = 0.01141832\n",
      "Iteration 141, loss = 0.01088094\n",
      "Iteration 142, loss = 0.01098623\n",
      "Iteration 143, loss = 0.01054220\n",
      "Iteration 144, loss = 0.01058157\n",
      "Iteration 145, loss = 0.01033534\n",
      "Iteration 146, loss = 0.01042499\n",
      "Iteration 147, loss = 0.01055647\n",
      "Iteration 148, loss = 0.01028835\n",
      "Iteration 149, loss = 0.01027746\n",
      "Iteration 150, loss = 0.01010250\n",
      "Iteration 151, loss = 0.01018218\n",
      "Iteration 152, loss = 0.01024130\n",
      "Iteration 153, loss = 0.00973548\n",
      "Iteration 154, loss = 0.00967418\n",
      "Iteration 155, loss = 0.00956658\n",
      "Iteration 156, loss = 0.00959252\n",
      "Iteration 157, loss = 0.00955948\n",
      "Iteration 158, loss = 0.00948864\n",
      "Iteration 159, loss = 0.00939169\n",
      "Iteration 160, loss = 0.00927510\n",
      "Iteration 161, loss = 0.00918076\n",
      "Iteration 162, loss = 0.00912627\n",
      "Iteration 163, loss = 0.00899544\n",
      "Iteration 164, loss = 0.00893633\n",
      "Iteration 165, loss = 0.00884927\n",
      "Iteration 166, loss = 0.00881952\n",
      "Iteration 167, loss = 0.00892351\n",
      "Iteration 168, loss = 0.00920759\n",
      "Iteration 169, loss = 0.00887860\n",
      "Iteration 170, loss = 0.00911147\n",
      "Iteration 171, loss = 0.00873130\n",
      "Iteration 172, loss = 0.00865433\n",
      "Iteration 173, loss = 0.00870216\n",
      "Iteration 174, loss = 0.00868253\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.43088778\n",
      "Iteration 2, loss = 0.32790756\n",
      "Iteration 3, loss = 0.28067116\n",
      "Iteration 4, loss = 0.25404994\n",
      "Iteration 5, loss = 0.23681211\n",
      "Iteration 6, loss = 0.20934998\n",
      "Iteration 7, loss = 0.18058516\n",
      "Iteration 8, loss = 0.16349490\n",
      "Iteration 9, loss = 0.14897087\n",
      "Iteration 10, loss = 0.13357471\n",
      "Iteration 11, loss = 0.11919219\n",
      "Iteration 12, loss = 0.11015655\n",
      "Iteration 13, loss = 0.10178638\n",
      "Iteration 14, loss = 0.09535269\n",
      "Iteration 15, loss = 0.08982177\n",
      "Iteration 16, loss = 0.08392137\n",
      "Iteration 17, loss = 0.08059614\n",
      "Iteration 18, loss = 0.07788389\n",
      "Iteration 19, loss = 0.07425795\n",
      "Iteration 20, loss = 0.07139824\n",
      "Iteration 21, loss = 0.07225749\n",
      "Iteration 22, loss = 0.06941200\n",
      "Iteration 23, loss = 0.06641865\n",
      "Iteration 24, loss = 0.06533153\n",
      "Iteration 25, loss = 0.06403306\n",
      "Iteration 26, loss = 0.06076751\n",
      "Iteration 27, loss = 0.05947792\n",
      "Iteration 28, loss = 0.05753627\n",
      "Iteration 29, loss = 0.05521678\n",
      "Iteration 30, loss = 0.05653495\n",
      "Iteration 31, loss = 0.05369496\n",
      "Iteration 32, loss = 0.05327070\n",
      "Iteration 33, loss = 0.05196351\n",
      "Iteration 34, loss = 0.05142632\n",
      "Iteration 35, loss = 0.05084243\n",
      "Iteration 36, loss = 0.04928650\n",
      "Iteration 37, loss = 0.04726657\n",
      "Iteration 38, loss = 0.04692406\n",
      "Iteration 39, loss = 0.04614205\n",
      "Iteration 40, loss = 0.04487674\n",
      "Iteration 41, loss = 0.04399568\n",
      "Iteration 42, loss = 0.04296619\n",
      "Iteration 43, loss = 0.04228629\n",
      "Iteration 44, loss = 0.04206269\n",
      "Iteration 45, loss = 0.04091950\n",
      "Iteration 46, loss = 0.03999126\n",
      "Iteration 47, loss = 0.04045823\n",
      "Iteration 48, loss = 0.03924988\n",
      "Iteration 49, loss = 0.03917264\n",
      "Iteration 50, loss = 0.03759819\n",
      "Iteration 51, loss = 0.03679962\n",
      "Iteration 52, loss = 0.03669265\n",
      "Iteration 53, loss = 0.03589537\n",
      "Iteration 54, loss = 0.03520095\n",
      "Iteration 55, loss = 0.03450089\n",
      "Iteration 56, loss = 0.03416686\n",
      "Iteration 57, loss = 0.03360784\n",
      "Iteration 58, loss = 0.03289061\n",
      "Iteration 59, loss = 0.03285006\n",
      "Iteration 60, loss = 0.03215944\n",
      "Iteration 61, loss = 0.03170055\n",
      "Iteration 62, loss = 0.03077928\n",
      "Iteration 63, loss = 0.03007909\n",
      "Iteration 64, loss = 0.02999687\n",
      "Iteration 65, loss = 0.02918664\n",
      "Iteration 66, loss = 0.02916520\n",
      "Iteration 67, loss = 0.02773878\n",
      "Iteration 68, loss = 0.02830294\n",
      "Iteration 69, loss = 0.02759126\n",
      "Iteration 70, loss = 0.02687376\n",
      "Iteration 71, loss = 0.02726298\n",
      "Iteration 72, loss = 0.02690852\n",
      "Iteration 73, loss = 0.02572283\n",
      "Iteration 74, loss = 0.02569393\n",
      "Iteration 75, loss = 0.02633783\n",
      "Iteration 76, loss = 0.02546693\n",
      "Iteration 77, loss = 0.02464016\n",
      "Iteration 78, loss = 0.02396710\n",
      "Iteration 79, loss = 0.02300161\n",
      "Iteration 80, loss = 0.02317950\n",
      "Iteration 81, loss = 0.02237036\n",
      "Iteration 82, loss = 0.02199242\n",
      "Iteration 83, loss = 0.02173149\n",
      "Iteration 84, loss = 0.02215087\n",
      "Iteration 85, loss = 0.02079934\n",
      "Iteration 86, loss = 0.02098949\n",
      "Iteration 87, loss = 0.02086310\n",
      "Iteration 88, loss = 0.02071509\n",
      "Iteration 89, loss = 0.02011807\n",
      "Iteration 90, loss = 0.01939732\n",
      "Iteration 91, loss = 0.01909619\n",
      "Iteration 92, loss = 0.01964997\n",
      "Iteration 93, loss = 0.01897537\n",
      "Iteration 94, loss = 0.01900084\n",
      "Iteration 95, loss = 0.01836464\n",
      "Iteration 96, loss = 0.01789485\n",
      "Iteration 97, loss = 0.01785082\n",
      "Iteration 98, loss = 0.01770174\n",
      "Iteration 99, loss = 0.01665804\n",
      "Iteration 100, loss = 0.01681621\n",
      "Iteration 101, loss = 0.01591587\n",
      "Iteration 102, loss = 0.01515401\n",
      "Iteration 103, loss = 0.01492944\n",
      "Iteration 104, loss = 0.01503226\n",
      "Iteration 105, loss = 0.01464848\n",
      "Iteration 106, loss = 0.01434592\n",
      "Iteration 107, loss = 0.01421307\n",
      "Iteration 108, loss = 0.01401335\n",
      "Iteration 109, loss = 0.01360044\n",
      "Iteration 110, loss = 0.01329625\n",
      "Iteration 111, loss = 0.01346174\n",
      "Iteration 112, loss = 0.01307875\n",
      "Iteration 113, loss = 0.01279666\n",
      "Iteration 114, loss = 0.01270615\n",
      "Iteration 115, loss = 0.01214911\n",
      "Iteration 116, loss = 0.01203206\n",
      "Iteration 117, loss = 0.01236791\n",
      "Iteration 118, loss = 0.01222177\n",
      "Iteration 119, loss = 0.01254903\n",
      "Iteration 120, loss = 0.01274141\n",
      "Iteration 121, loss = 0.01211817\n",
      "Iteration 122, loss = 0.01182193\n",
      "Iteration 123, loss = 0.01179629\n",
      "Iteration 124, loss = 0.01137443\n",
      "Iteration 125, loss = 0.01135196\n",
      "Iteration 126, loss = 0.01058745\n",
      "Iteration 127, loss = 0.01080344\n",
      "Iteration 128, loss = 0.01077737\n",
      "Iteration 129, loss = 0.01118138\n",
      "Iteration 130, loss = 0.01077993\n",
      "Iteration 131, loss = 0.01032867\n",
      "Iteration 132, loss = 0.01047957\n",
      "Iteration 133, loss = 0.01065185\n",
      "Iteration 134, loss = 0.01075207\n",
      "Iteration 135, loss = 0.01019501\n",
      "Iteration 136, loss = 0.01100663\n",
      "Iteration 137, loss = 0.00984811\n",
      "Iteration 138, loss = 0.00987047\n",
      "Iteration 139, loss = 0.00985015\n",
      "Iteration 140, loss = 0.00933515\n",
      "Iteration 141, loss = 0.00909398\n",
      "Iteration 142, loss = 0.00902333\n",
      "Iteration 143, loss = 0.00940358\n",
      "Iteration 144, loss = 0.00874018\n",
      "Iteration 145, loss = 0.00868835\n",
      "Iteration 146, loss = 0.00868123\n",
      "Iteration 147, loss = 0.00854360\n",
      "Iteration 148, loss = 0.00863292\n",
      "Iteration 149, loss = 0.00851419\n",
      "Iteration 150, loss = 0.00853590\n",
      "Iteration 151, loss = 0.00842266\n",
      "Iteration 152, loss = 0.00825399\n",
      "Iteration 153, loss = 0.00803092\n",
      "Iteration 154, loss = 0.00813101\n",
      "Iteration 155, loss = 0.00795985\n",
      "Iteration 156, loss = 0.00800755\n",
      "Iteration 157, loss = 0.00837608\n",
      "Iteration 158, loss = 0.00809401\n",
      "Iteration 159, loss = 0.00774382\n",
      "Iteration 160, loss = 0.00832090\n",
      "Iteration 161, loss = 0.00846497\n",
      "Iteration 162, loss = 0.00813600\n",
      "Iteration 163, loss = 0.00764138\n",
      "Iteration 164, loss = 0.00780591\n",
      "Iteration 165, loss = 0.00846919\n",
      "Iteration 166, loss = 0.00769757\n",
      "Iteration 167, loss = 0.00768239\n",
      "Iteration 168, loss = 0.00753989\n",
      "Iteration 169, loss = 0.00721131\n",
      "Iteration 170, loss = 0.00824218\n",
      "Iteration 171, loss = 0.00854814\n",
      "Iteration 172, loss = 0.00810637\n",
      "Iteration 173, loss = 0.00860914\n",
      "Iteration 174, loss = 0.00815712\n",
      "Iteration 175, loss = 0.00758685\n",
      "Iteration 176, loss = 0.00695628\n",
      "Iteration 177, loss = 0.00667035\n",
      "Iteration 178, loss = 0.00658640\n",
      "Iteration 179, loss = 0.00655279\n",
      "Iteration 180, loss = 0.00692731\n",
      "Iteration 181, loss = 0.00682643\n",
      "Iteration 182, loss = 0.00675220\n",
      "Iteration 183, loss = 0.00636665\n",
      "Iteration 184, loss = 0.00638419\n",
      "Iteration 185, loss = 0.00635121\n",
      "Iteration 186, loss = 0.00665311\n",
      "Iteration 187, loss = 0.00660115\n",
      "Iteration 188, loss = 0.00694836\n",
      "Iteration 189, loss = 0.00639594\n",
      "Iteration 190, loss = 0.00616290\n",
      "Iteration 191, loss = 0.00607813\n",
      "Iteration 192, loss = 0.00592930\n",
      "Iteration 193, loss = 0.00601021\n",
      "Iteration 194, loss = 0.00608616\n",
      "Iteration 195, loss = 0.00620320\n",
      "Iteration 196, loss = 0.00627319\n",
      "Iteration 197, loss = 0.00644568\n",
      "Iteration 198, loss = 0.00701434\n",
      "Iteration 199, loss = 0.00669381\n",
      "Iteration 200, loss = 0.00646862\n",
      "Iteration 201, loss = 0.00653322\n",
      "Iteration 202, loss = 0.00579504\n",
      "Iteration 203, loss = 0.00610709\n",
      "Iteration 204, loss = 0.00574117\n",
      "Iteration 205, loss = 0.00581617\n",
      "Iteration 206, loss = 0.00609422\n",
      "Iteration 207, loss = 0.00655091\n",
      "Iteration 208, loss = 0.00689049\n",
      "Iteration 209, loss = 0.00561937\n",
      "Iteration 210, loss = 0.00581935\n",
      "Iteration 211, loss = 0.00572556\n",
      "Iteration 212, loss = 0.00569517\n",
      "Iteration 213, loss = 0.00548623\n",
      "Iteration 214, loss = 0.00539967\n",
      "Iteration 215, loss = 0.00542603\n",
      "Iteration 216, loss = 0.00518735\n",
      "Iteration 217, loss = 0.00537861\n",
      "Iteration 218, loss = 0.00507046\n",
      "Iteration 219, loss = 0.00519325\n",
      "Iteration 220, loss = 0.00633677\n",
      "Iteration 221, loss = 0.00678038\n",
      "Iteration 222, loss = 0.00567283\n",
      "Iteration 223, loss = 0.00517926\n",
      "Iteration 224, loss = 0.00510357\n",
      "Iteration 225, loss = 0.00493578\n",
      "Iteration 226, loss = 0.00494424\n",
      "Iteration 227, loss = 0.00516414\n",
      "Iteration 228, loss = 0.00505658\n",
      "Iteration 229, loss = 0.00494138\n",
      "Iteration 230, loss = 0.00534693\n",
      "Iteration 231, loss = 0.00510149\n",
      "Iteration 232, loss = 0.00472257\n",
      "Iteration 233, loss = 0.00505550\n",
      "Iteration 234, loss = 0.00487829\n",
      "Iteration 235, loss = 0.00472120\n",
      "Iteration 236, loss = 0.00459179\n",
      "Iteration 237, loss = 0.00475557\n",
      "Iteration 238, loss = 0.00458216\n",
      "Iteration 239, loss = 0.00476016\n",
      "Iteration 240, loss = 0.00496532\n",
      "Iteration 241, loss = 0.00466168\n",
      "Iteration 242, loss = 0.00462042\n",
      "Iteration 243, loss = 0.00457381\n",
      "Iteration 244, loss = 0.00478761\n",
      "Iteration 245, loss = 0.00449066\n",
      "Iteration 246, loss = 0.00478139\n",
      "Iteration 247, loss = 0.00456288\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.43088778\n",
      "Iteration 2, loss = 0.32790756\n",
      "Iteration 3, loss = 0.28067116\n",
      "Iteration 4, loss = 0.25404994\n",
      "Iteration 5, loss = 0.23681211\n",
      "Iteration 6, loss = 0.20934998\n",
      "Iteration 7, loss = 0.18058516\n",
      "Iteration 8, loss = 0.16349490\n",
      "Iteration 9, loss = 0.14897087\n",
      "Iteration 10, loss = 0.13357471\n",
      "Iteration 11, loss = 0.11919219\n",
      "Iteration 12, loss = 0.11015655\n",
      "Iteration 13, loss = 0.10178638\n",
      "Iteration 14, loss = 0.09535269\n",
      "Iteration 15, loss = 0.08982177\n",
      "Iteration 16, loss = 0.08392137\n",
      "Iteration 17, loss = 0.08059614\n",
      "Iteration 18, loss = 0.07788389\n",
      "Iteration 19, loss = 0.07425795\n",
      "Iteration 20, loss = 0.07139824\n",
      "Iteration 21, loss = 0.07225749\n",
      "Iteration 22, loss = 0.06941200\n",
      "Iteration 23, loss = 0.06641865\n",
      "Iteration 24, loss = 0.06533153\n",
      "Iteration 25, loss = 0.06403306\n",
      "Iteration 26, loss = 0.06076751\n",
      "Iteration 27, loss = 0.05947792\n",
      "Iteration 28, loss = 0.05753627\n",
      "Iteration 29, loss = 0.05521678\n",
      "Iteration 30, loss = 0.05653495\n",
      "Iteration 31, loss = 0.05369496\n",
      "Iteration 32, loss = 0.05327070\n",
      "Iteration 33, loss = 0.05196351\n",
      "Iteration 34, loss = 0.05142632\n",
      "Iteration 35, loss = 0.05084243\n",
      "Iteration 36, loss = 0.04928650\n",
      "Iteration 37, loss = 0.04726657\n",
      "Iteration 38, loss = 0.04692406\n",
      "Iteration 39, loss = 0.04614205\n",
      "Iteration 40, loss = 0.04487674\n",
      "Iteration 41, loss = 0.04399568\n",
      "Iteration 42, loss = 0.04296619\n",
      "Iteration 43, loss = 0.04228629\n",
      "Iteration 44, loss = 0.04206269\n",
      "Iteration 45, loss = 0.04091950\n",
      "Iteration 46, loss = 0.03999126\n",
      "Iteration 47, loss = 0.04045823\n",
      "Iteration 48, loss = 0.03924988\n",
      "Iteration 49, loss = 0.03917264\n",
      "Iteration 50, loss = 0.03759819\n",
      "Iteration 51, loss = 0.03679962\n",
      "Iteration 52, loss = 0.03669265\n",
      "Iteration 53, loss = 0.03589537\n",
      "Iteration 54, loss = 0.03520095\n",
      "Iteration 55, loss = 0.03450089\n",
      "Iteration 56, loss = 0.03416686\n",
      "Iteration 57, loss = 0.03360784\n",
      "Iteration 58, loss = 0.03289061\n",
      "Iteration 59, loss = 0.03285006\n",
      "Iteration 60, loss = 0.03215944\n",
      "Iteration 61, loss = 0.03170055\n",
      "Iteration 62, loss = 0.03077928\n",
      "Iteration 63, loss = 0.03007909\n",
      "Iteration 64, loss = 0.02999687\n",
      "Iteration 65, loss = 0.02918664\n",
      "Iteration 66, loss = 0.02916520\n",
      "Iteration 67, loss = 0.02773878\n",
      "Iteration 68, loss = 0.02830294\n",
      "Iteration 69, loss = 0.02759126\n",
      "Iteration 70, loss = 0.02687376\n",
      "Iteration 71, loss = 0.02726298\n",
      "Iteration 72, loss = 0.02690852\n",
      "Iteration 73, loss = 0.02572283\n",
      "Iteration 74, loss = 0.02569393\n",
      "Iteration 75, loss = 0.02633783\n",
      "Iteration 76, loss = 0.02546693\n",
      "Iteration 77, loss = 0.02464016\n",
      "Iteration 78, loss = 0.02396710\n",
      "Iteration 79, loss = 0.02300161\n",
      "Iteration 80, loss = 0.02317950\n",
      "Iteration 81, loss = 0.02237036\n",
      "Iteration 82, loss = 0.02199242\n",
      "Iteration 83, loss = 0.02173149\n",
      "Iteration 84, loss = 0.02215087\n",
      "Iteration 85, loss = 0.02079934\n",
      "Iteration 86, loss = 0.02098949\n",
      "Iteration 87, loss = 0.02086310\n",
      "Iteration 88, loss = 0.02071509\n",
      "Iteration 89, loss = 0.02011807\n",
      "Iteration 90, loss = 0.01939732\n",
      "Iteration 91, loss = 0.01909619\n",
      "Iteration 92, loss = 0.01964997\n",
      "Iteration 93, loss = 0.01897537\n",
      "Iteration 94, loss = 0.01900084\n",
      "Iteration 95, loss = 0.01836464\n",
      "Iteration 96, loss = 0.01789485\n",
      "Iteration 97, loss = 0.01785082\n",
      "Iteration 98, loss = 0.01770174\n",
      "Iteration 99, loss = 0.01665804\n",
      "Iteration 100, loss = 0.01681621\n",
      "Iteration 101, loss = 0.01591587\n",
      "Iteration 102, loss = 0.01515401\n",
      "Iteration 103, loss = 0.01492944\n",
      "Iteration 104, loss = 0.01503226\n",
      "Iteration 105, loss = 0.01464848\n",
      "Iteration 106, loss = 0.01434592\n",
      "Iteration 107, loss = 0.01421307\n",
      "Iteration 108, loss = 0.01401335\n",
      "Iteration 109, loss = 0.01360044\n",
      "Iteration 110, loss = 0.01329625\n",
      "Iteration 111, loss = 0.01346174\n",
      "Iteration 112, loss = 0.01307875\n",
      "Iteration 113, loss = 0.01279666\n",
      "Iteration 114, loss = 0.01270615\n",
      "Iteration 115, loss = 0.01214911\n",
      "Iteration 116, loss = 0.01203206\n",
      "Iteration 117, loss = 0.01236791\n",
      "Iteration 118, loss = 0.01222177\n",
      "Iteration 119, loss = 0.01254903\n",
      "Iteration 120, loss = 0.01274141\n",
      "Iteration 121, loss = 0.01211817\n",
      "Iteration 122, loss = 0.01182193\n",
      "Iteration 123, loss = 0.01179629\n",
      "Iteration 124, loss = 0.01137443\n",
      "Iteration 125, loss = 0.01135196\n",
      "Iteration 126, loss = 0.01058745\n",
      "Iteration 127, loss = 0.01080344\n",
      "Iteration 128, loss = 0.01077737\n",
      "Iteration 129, loss = 0.01118138\n",
      "Iteration 130, loss = 0.01077993\n",
      "Iteration 131, loss = 0.01032867\n",
      "Iteration 132, loss = 0.01047957\n",
      "Iteration 133, loss = 0.01065185\n",
      "Iteration 134, loss = 0.01075207\n",
      "Iteration 135, loss = 0.01019501\n",
      "Iteration 136, loss = 0.01100663\n",
      "Iteration 137, loss = 0.00984811\n",
      "Iteration 138, loss = 0.00987047\n",
      "Iteration 139, loss = 0.00985015\n",
      "Iteration 140, loss = 0.00933515\n",
      "Iteration 141, loss = 0.00909398\n",
      "Iteration 142, loss = 0.00902333\n",
      "Iteration 143, loss = 0.00940358\n",
      "Iteration 144, loss = 0.00874018\n",
      "Iteration 145, loss = 0.00868835\n",
      "Iteration 146, loss = 0.00868123\n",
      "Iteration 147, loss = 0.00854360\n",
      "Iteration 148, loss = 0.00863292\n",
      "Iteration 149, loss = 0.00851419\n",
      "Iteration 150, loss = 0.00853590\n",
      "Iteration 151, loss = 0.00842266\n",
      "Iteration 152, loss = 0.00825399\n",
      "Iteration 153, loss = 0.00803092\n",
      "Iteration 154, loss = 0.00813101\n",
      "Iteration 155, loss = 0.00795985\n",
      "Iteration 156, loss = 0.00800755\n",
      "Iteration 157, loss = 0.00837608\n",
      "Iteration 158, loss = 0.00809401\n",
      "Iteration 159, loss = 0.00774382\n",
      "Iteration 160, loss = 0.00832090\n",
      "Iteration 161, loss = 0.00846497\n",
      "Iteration 162, loss = 0.00813600\n",
      "Iteration 163, loss = 0.00764138\n",
      "Iteration 164, loss = 0.00780591\n",
      "Iteration 165, loss = 0.00846919\n",
      "Iteration 166, loss = 0.00769757\n",
      "Iteration 167, loss = 0.00768239\n",
      "Iteration 168, loss = 0.00753989\n",
      "Iteration 169, loss = 0.00721131\n",
      "Iteration 170, loss = 0.00824218\n",
      "Iteration 171, loss = 0.00854814\n",
      "Iteration 172, loss = 0.00810637\n",
      "Iteration 173, loss = 0.00860914\n",
      "Iteration 174, loss = 0.00815712\n",
      "Iteration 175, loss = 0.00758685\n",
      "Iteration 176, loss = 0.00695628\n",
      "Iteration 177, loss = 0.00667035\n",
      "Iteration 178, loss = 0.00658640\n",
      "Iteration 179, loss = 0.00655279\n",
      "Iteration 180, loss = 0.00692731\n",
      "Iteration 181, loss = 0.00682643\n",
      "Iteration 182, loss = 0.00675220\n",
      "Iteration 183, loss = 0.00636665\n",
      "Iteration 184, loss = 0.00638419\n",
      "Iteration 185, loss = 0.00635121\n",
      "Iteration 186, loss = 0.00665311\n",
      "Iteration 187, loss = 0.00660115\n",
      "Iteration 188, loss = 0.00694836\n",
      "Iteration 189, loss = 0.00639594\n",
      "Iteration 190, loss = 0.00616290\n",
      "Iteration 191, loss = 0.00607813\n",
      "Iteration 192, loss = 0.00592930\n",
      "Iteration 193, loss = 0.00601021\n",
      "Iteration 194, loss = 0.00608616\n",
      "Iteration 195, loss = 0.00620320\n",
      "Iteration 196, loss = 0.00627319\n",
      "Iteration 197, loss = 0.00644568\n",
      "Iteration 198, loss = 0.00701434\n",
      "Iteration 199, loss = 0.00669381\n",
      "Iteration 200, loss = 0.00646862\n",
      "Iteration 201, loss = 0.00653322\n",
      "Iteration 202, loss = 0.00579504\n",
      "Iteration 203, loss = 0.00610709\n",
      "Iteration 204, loss = 0.00574117\n",
      "Iteration 205, loss = 0.00581617\n",
      "Iteration 206, loss = 0.00609422\n",
      "Iteration 207, loss = 0.00655091\n",
      "Iteration 208, loss = 0.00689049\n",
      "Iteration 209, loss = 0.00561937\n",
      "Iteration 210, loss = 0.00581935\n",
      "Iteration 211, loss = 0.00572556\n",
      "Iteration 212, loss = 0.00569517\n",
      "Iteration 213, loss = 0.00548623\n",
      "Iteration 214, loss = 0.00539967\n",
      "Iteration 215, loss = 0.00542603\n",
      "Iteration 216, loss = 0.00518735\n",
      "Iteration 217, loss = 0.00537861\n",
      "Iteration 218, loss = 0.00507046\n",
      "Iteration 219, loss = 0.00519325\n",
      "Iteration 220, loss = 0.00633677\n",
      "Iteration 221, loss = 0.00678038\n",
      "Iteration 222, loss = 0.00567283\n",
      "Iteration 223, loss = 0.00517926\n",
      "Iteration 224, loss = 0.00510357\n",
      "Iteration 225, loss = 0.00493578\n",
      "Iteration 226, loss = 0.00494424\n",
      "Iteration 227, loss = 0.00516414\n",
      "Iteration 228, loss = 0.00505658\n",
      "Iteration 229, loss = 0.00494138\n",
      "Iteration 230, loss = 0.00534693\n",
      "Iteration 231, loss = 0.00510149\n",
      "Iteration 232, loss = 0.00472257\n",
      "Iteration 233, loss = 0.00505550\n",
      "Iteration 234, loss = 0.00487829\n",
      "Iteration 235, loss = 0.00472120\n",
      "Iteration 236, loss = 0.00459179\n",
      "Iteration 237, loss = 0.00475557\n",
      "Iteration 238, loss = 0.00458216\n",
      "Iteration 239, loss = 0.00476016\n",
      "Iteration 240, loss = 0.00496532\n",
      "Iteration 241, loss = 0.00466168\n",
      "Iteration 242, loss = 0.00462042\n",
      "Iteration 243, loss = 0.00457381\n",
      "Iteration 244, loss = 0.00478761\n",
      "Iteration 245, loss = 0.00449066\n",
      "Iteration 246, loss = 0.00478139\n",
      "Iteration 247, loss = 0.00456288\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.43088778\n",
      "Iteration 2, loss = 0.32790756\n",
      "Iteration 3, loss = 0.28067116\n",
      "Iteration 4, loss = 0.25404994\n",
      "Iteration 5, loss = 0.23681211\n",
      "Iteration 6, loss = 0.20934998\n",
      "Iteration 7, loss = 0.18058516\n",
      "Iteration 8, loss = 0.16349490\n",
      "Iteration 9, loss = 0.14897087\n",
      "Iteration 10, loss = 0.13357471\n",
      "Iteration 11, loss = 0.11919219\n",
      "Iteration 12, loss = 0.11015655\n",
      "Iteration 13, loss = 0.10178638\n",
      "Iteration 14, loss = 0.09535269\n",
      "Iteration 15, loss = 0.08982177\n",
      "Iteration 16, loss = 0.08392137\n",
      "Iteration 17, loss = 0.08059614\n",
      "Iteration 18, loss = 0.07788389\n",
      "Iteration 19, loss = 0.07425795\n",
      "Iteration 20, loss = 0.07139824\n",
      "Iteration 21, loss = 0.07225749\n",
      "Iteration 22, loss = 0.06941200\n",
      "Iteration 23, loss = 0.06641865\n",
      "Iteration 24, loss = 0.06533153\n",
      "Iteration 25, loss = 0.06403306\n",
      "Iteration 26, loss = 0.06076751\n",
      "Iteration 27, loss = 0.05947792\n",
      "Iteration 28, loss = 0.05753627\n",
      "Iteration 29, loss = 0.05521678\n",
      "Iteration 30, loss = 0.05653495\n",
      "Iteration 31, loss = 0.05369496\n",
      "Iteration 32, loss = 0.05327070\n",
      "Iteration 33, loss = 0.05196351\n",
      "Iteration 34, loss = 0.05142632\n",
      "Iteration 35, loss = 0.05084243\n",
      "Iteration 36, loss = 0.04928650\n",
      "Iteration 37, loss = 0.04726657\n",
      "Iteration 38, loss = 0.04692406\n",
      "Iteration 39, loss = 0.04614205\n",
      "Iteration 40, loss = 0.04487674\n",
      "Iteration 41, loss = 0.04399568\n",
      "Iteration 42, loss = 0.04296619\n",
      "Iteration 43, loss = 0.04228629\n",
      "Iteration 44, loss = 0.04206269\n",
      "Iteration 45, loss = 0.04091950\n",
      "Iteration 46, loss = 0.03999126\n",
      "Iteration 47, loss = 0.04045823\n",
      "Iteration 48, loss = 0.03924988\n",
      "Iteration 49, loss = 0.03917264\n",
      "Iteration 50, loss = 0.03759819\n",
      "Iteration 51, loss = 0.03679962\n",
      "Iteration 52, loss = 0.03669265\n",
      "Iteration 53, loss = 0.03589537\n",
      "Iteration 54, loss = 0.03520095\n",
      "Iteration 55, loss = 0.03450089\n",
      "Iteration 56, loss = 0.03416686\n",
      "Iteration 57, loss = 0.03360784\n",
      "Iteration 58, loss = 0.03289061\n",
      "Iteration 59, loss = 0.03285006\n",
      "Iteration 60, loss = 0.03215944\n",
      "Iteration 61, loss = 0.03170055\n",
      "Iteration 62, loss = 0.03077928\n",
      "Iteration 63, loss = 0.03007909\n",
      "Iteration 64, loss = 0.02999687\n",
      "Iteration 65, loss = 0.02918664\n",
      "Iteration 66, loss = 0.02916520\n",
      "Iteration 67, loss = 0.02773878\n",
      "Iteration 68, loss = 0.02830294\n",
      "Iteration 69, loss = 0.02759126\n",
      "Iteration 70, loss = 0.02687376\n",
      "Iteration 71, loss = 0.02726298\n",
      "Iteration 72, loss = 0.02690852\n",
      "Iteration 73, loss = 0.02572283\n",
      "Iteration 74, loss = 0.02569393\n",
      "Iteration 75, loss = 0.02633783\n",
      "Iteration 76, loss = 0.02546693\n",
      "Iteration 77, loss = 0.02464016\n",
      "Iteration 78, loss = 0.02396710\n",
      "Iteration 79, loss = 0.02300161\n",
      "Iteration 80, loss = 0.02317950\n",
      "Iteration 81, loss = 0.02237036\n",
      "Iteration 82, loss = 0.02199242\n",
      "Iteration 83, loss = 0.02173149\n",
      "Iteration 84, loss = 0.02215087\n",
      "Iteration 85, loss = 0.02079934\n",
      "Iteration 86, loss = 0.02098949\n",
      "Iteration 87, loss = 0.02086310\n",
      "Iteration 88, loss = 0.02071509\n",
      "Iteration 89, loss = 0.02011807\n",
      "Iteration 90, loss = 0.01939732\n",
      "Iteration 91, loss = 0.01909619\n",
      "Iteration 92, loss = 0.01964997\n",
      "Iteration 93, loss = 0.01897537\n",
      "Iteration 94, loss = 0.01900084\n",
      "Iteration 95, loss = 0.01836464\n",
      "Iteration 96, loss = 0.01789485\n",
      "Iteration 97, loss = 0.01785082\n",
      "Iteration 98, loss = 0.01770174\n",
      "Iteration 99, loss = 0.01665804\n",
      "Iteration 100, loss = 0.01681621\n",
      "Iteration 101, loss = 0.01591587\n",
      "Iteration 102, loss = 0.01515401\n",
      "Iteration 103, loss = 0.01492944\n",
      "Iteration 104, loss = 0.01503226\n",
      "Iteration 105, loss = 0.01464848\n",
      "Iteration 106, loss = 0.01434592\n",
      "Iteration 107, loss = 0.01421307\n",
      "Iteration 108, loss = 0.01401335\n",
      "Iteration 109, loss = 0.01360044\n",
      "Iteration 110, loss = 0.01329625\n",
      "Iteration 111, loss = 0.01346174\n",
      "Iteration 112, loss = 0.01307875\n",
      "Iteration 113, loss = 0.01279666\n",
      "Iteration 114, loss = 0.01270615\n",
      "Iteration 115, loss = 0.01214911\n",
      "Iteration 116, loss = 0.01203206\n",
      "Iteration 117, loss = 0.01236791\n",
      "Iteration 118, loss = 0.01222177\n",
      "Iteration 119, loss = 0.01254903\n",
      "Iteration 120, loss = 0.01274141\n",
      "Iteration 121, loss = 0.01211817\n",
      "Iteration 122, loss = 0.01182193\n",
      "Iteration 123, loss = 0.01179629\n",
      "Iteration 124, loss = 0.01137443\n",
      "Iteration 125, loss = 0.01135196\n",
      "Iteration 126, loss = 0.01058745\n",
      "Iteration 127, loss = 0.01080344\n",
      "Iteration 128, loss = 0.01077737\n",
      "Iteration 129, loss = 0.01118138\n",
      "Iteration 130, loss = 0.01077993\n",
      "Iteration 131, loss = 0.01032867\n",
      "Iteration 132, loss = 0.01047957\n",
      "Iteration 133, loss = 0.01065185\n",
      "Iteration 134, loss = 0.01075207\n",
      "Iteration 135, loss = 0.01019501\n",
      "Iteration 136, loss = 0.01100663\n",
      "Iteration 137, loss = 0.00984811\n",
      "Iteration 138, loss = 0.00987047\n",
      "Iteration 139, loss = 0.00985015\n",
      "Iteration 140, loss = 0.00933515\n",
      "Iteration 141, loss = 0.00909398\n",
      "Iteration 142, loss = 0.00902333\n",
      "Iteration 143, loss = 0.00940358\n",
      "Iteration 144, loss = 0.00874018\n",
      "Iteration 145, loss = 0.00868835\n",
      "Iteration 146, loss = 0.00868123\n",
      "Iteration 147, loss = 0.00854360\n",
      "Iteration 148, loss = 0.00863292\n",
      "Iteration 149, loss = 0.00851419\n",
      "Iteration 150, loss = 0.00853590\n",
      "Iteration 151, loss = 0.00842266\n",
      "Iteration 152, loss = 0.00825399\n",
      "Iteration 153, loss = 0.00803092\n",
      "Iteration 154, loss = 0.00813101\n",
      "Iteration 155, loss = 0.00795985\n",
      "Iteration 156, loss = 0.00800755\n",
      "Iteration 157, loss = 0.00837608\n",
      "Iteration 158, loss = 0.00809401\n",
      "Iteration 159, loss = 0.00774382\n",
      "Iteration 160, loss = 0.00832090\n",
      "Iteration 161, loss = 0.00846497\n",
      "Iteration 162, loss = 0.00813600\n",
      "Iteration 163, loss = 0.00764138\n",
      "Iteration 164, loss = 0.00780591\n",
      "Iteration 165, loss = 0.00846919\n",
      "Iteration 166, loss = 0.00769757\n",
      "Iteration 167, loss = 0.00768239\n",
      "Iteration 168, loss = 0.00753989\n",
      "Iteration 169, loss = 0.00721131\n",
      "Iteration 170, loss = 0.00824218\n",
      "Iteration 171, loss = 0.00854814\n",
      "Iteration 172, loss = 0.00810637\n",
      "Iteration 173, loss = 0.00860914\n",
      "Iteration 174, loss = 0.00815712\n",
      "Iteration 175, loss = 0.00758685\n",
      "Iteration 176, loss = 0.00695628\n",
      "Iteration 177, loss = 0.00667035\n",
      "Iteration 178, loss = 0.00658640\n",
      "Iteration 179, loss = 0.00655279\n",
      "Iteration 180, loss = 0.00692731\n",
      "Iteration 181, loss = 0.00682643\n",
      "Iteration 182, loss = 0.00675220\n",
      "Iteration 183, loss = 0.00636665\n",
      "Iteration 184, loss = 0.00638419\n",
      "Iteration 185, loss = 0.00635121\n",
      "Iteration 186, loss = 0.00665311\n",
      "Iteration 187, loss = 0.00660115\n",
      "Iteration 188, loss = 0.00694836\n",
      "Iteration 189, loss = 0.00639594\n",
      "Iteration 190, loss = 0.00616290\n",
      "Iteration 191, loss = 0.00607813\n",
      "Iteration 192, loss = 0.00592930\n",
      "Iteration 193, loss = 0.00601021\n",
      "Iteration 194, loss = 0.00608616\n",
      "Iteration 195, loss = 0.00620320\n",
      "Iteration 196, loss = 0.00627319\n",
      "Iteration 197, loss = 0.00644568\n",
      "Iteration 198, loss = 0.00701434\n",
      "Iteration 199, loss = 0.00669381\n",
      "Iteration 200, loss = 0.00646862\n",
      "Iteration 201, loss = 0.00653322\n",
      "Iteration 202, loss = 0.00579504\n",
      "Iteration 203, loss = 0.00610709\n",
      "Iteration 204, loss = 0.00574117\n",
      "Iteration 205, loss = 0.00581617\n",
      "Iteration 206, loss = 0.00609422\n",
      "Iteration 207, loss = 0.00655091\n",
      "Iteration 208, loss = 0.00689049\n",
      "Iteration 209, loss = 0.00561937\n",
      "Iteration 210, loss = 0.00581935\n",
      "Iteration 211, loss = 0.00572556\n",
      "Iteration 212, loss = 0.00569517\n",
      "Iteration 213, loss = 0.00548623\n",
      "Iteration 214, loss = 0.00539967\n",
      "Iteration 215, loss = 0.00542603\n",
      "Iteration 216, loss = 0.00518735\n",
      "Iteration 217, loss = 0.00537861\n",
      "Iteration 218, loss = 0.00507046\n",
      "Iteration 219, loss = 0.00519325\n",
      "Iteration 220, loss = 0.00633677\n",
      "Iteration 221, loss = 0.00678038\n",
      "Iteration 222, loss = 0.00567283\n",
      "Iteration 223, loss = 0.00517926\n",
      "Iteration 224, loss = 0.00510357\n",
      "Iteration 225, loss = 0.00493578\n",
      "Iteration 226, loss = 0.00494424\n",
      "Iteration 227, loss = 0.00516414\n",
      "Iteration 228, loss = 0.00505658\n",
      "Iteration 229, loss = 0.00494138\n",
      "Iteration 230, loss = 0.00534693\n",
      "Iteration 231, loss = 0.00510149\n",
      "Iteration 232, loss = 0.00472257\n",
      "Iteration 233, loss = 0.00505550\n",
      "Iteration 234, loss = 0.00487829\n",
      "Iteration 235, loss = 0.00472120\n",
      "Iteration 236, loss = 0.00459179\n",
      "Iteration 237, loss = 0.00475557\n",
      "Iteration 238, loss = 0.00458216\n",
      "Iteration 239, loss = 0.00476016\n",
      "Iteration 240, loss = 0.00496532\n",
      "Iteration 241, loss = 0.00466168\n",
      "Iteration 242, loss = 0.00462042\n",
      "Iteration 243, loss = 0.00457381\n",
      "Iteration 244, loss = 0.00478761\n",
      "Iteration 245, loss = 0.00449066\n",
      "Iteration 246, loss = 0.00478139\n",
      "Iteration 247, loss = 0.00456288\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.40698515\n",
      "Iteration 2, loss = 0.29952219\n",
      "Iteration 3, loss = 0.26000358\n",
      "Iteration 4, loss = 0.22312230\n",
      "Iteration 5, loss = 0.19825517\n",
      "Iteration 6, loss = 0.17624303\n",
      "Iteration 7, loss = 0.15395664\n",
      "Iteration 8, loss = 0.13621121\n",
      "Iteration 9, loss = 0.12022624\n",
      "Iteration 10, loss = 0.10845334\n",
      "Iteration 11, loss = 0.09632854\n",
      "Iteration 12, loss = 0.08799728\n",
      "Iteration 13, loss = 0.08050629\n",
      "Iteration 14, loss = 0.07486450\n",
      "Iteration 15, loss = 0.06963585\n",
      "Iteration 16, loss = 0.06610835\n",
      "Iteration 17, loss = 0.06340914\n",
      "Iteration 18, loss = 0.05980768\n",
      "Iteration 19, loss = 0.05741534\n",
      "Iteration 20, loss = 0.05534602\n",
      "Iteration 21, loss = 0.05355368\n",
      "Iteration 22, loss = 0.05135019\n",
      "Iteration 23, loss = 0.04965888\n",
      "Iteration 24, loss = 0.04833089\n",
      "Iteration 25, loss = 0.04640989\n",
      "Iteration 26, loss = 0.04457686\n",
      "Iteration 27, loss = 0.04279120\n",
      "Iteration 28, loss = 0.04173915\n",
      "Iteration 29, loss = 0.04061220\n",
      "Iteration 30, loss = 0.03908469\n",
      "Iteration 31, loss = 0.03875706\n",
      "Iteration 32, loss = 0.03728887\n",
      "Iteration 33, loss = 0.03619129\n",
      "Iteration 34, loss = 0.03540628\n",
      "Iteration 35, loss = 0.03436204\n",
      "Iteration 36, loss = 0.03340948\n",
      "Iteration 37, loss = 0.03248634\n",
      "Iteration 38, loss = 0.03180024\n",
      "Iteration 39, loss = 0.03077907\n",
      "Iteration 40, loss = 0.03080859\n",
      "Iteration 41, loss = 0.02970171\n",
      "Iteration 42, loss = 0.02871766\n",
      "Iteration 43, loss = 0.02830559\n",
      "Iteration 44, loss = 0.02778104\n",
      "Iteration 45, loss = 0.02685035\n",
      "Iteration 46, loss = 0.02685345\n",
      "Iteration 47, loss = 0.02683534\n",
      "Iteration 48, loss = 0.02574397\n",
      "Iteration 49, loss = 0.02478901\n",
      "Iteration 50, loss = 0.02456007\n",
      "Iteration 51, loss = 0.02361694\n",
      "Iteration 52, loss = 0.02316028\n",
      "Iteration 53, loss = 0.02253283\n",
      "Iteration 54, loss = 0.02228084\n",
      "Iteration 55, loss = 0.02162991\n",
      "Iteration 56, loss = 0.02110833\n",
      "Iteration 57, loss = 0.02062183\n",
      "Iteration 58, loss = 0.02021588\n",
      "Iteration 59, loss = 0.01976469\n",
      "Iteration 60, loss = 0.01931145\n",
      "Iteration 61, loss = 0.01895955\n",
      "Iteration 62, loss = 0.01838263\n",
      "Iteration 63, loss = 0.01841635\n",
      "Iteration 64, loss = 0.01790479\n",
      "Iteration 65, loss = 0.01763050\n",
      "Iteration 66, loss = 0.01715897\n",
      "Iteration 67, loss = 0.01713285\n",
      "Iteration 68, loss = 0.01663225\n",
      "Iteration 69, loss = 0.01638377\n",
      "Iteration 70, loss = 0.01602685\n",
      "Iteration 71, loss = 0.01598255\n",
      "Iteration 72, loss = 0.01568633\n",
      "Iteration 73, loss = 0.01548726\n",
      "Iteration 74, loss = 0.01554200\n",
      "Iteration 75, loss = 0.01565490\n",
      "Iteration 76, loss = 0.01495324\n",
      "Iteration 77, loss = 0.01474040\n",
      "Iteration 78, loss = 0.01443204\n",
      "Iteration 79, loss = 0.01414809\n",
      "Iteration 80, loss = 0.01410025\n",
      "Iteration 81, loss = 0.01378777\n",
      "Iteration 82, loss = 0.01371512\n",
      "Iteration 83, loss = 0.01348491\n",
      "Iteration 84, loss = 0.01345294\n",
      "Iteration 85, loss = 0.01334147\n",
      "Iteration 86, loss = 0.01287810\n",
      "Iteration 87, loss = 0.01296180\n",
      "Iteration 88, loss = 0.01272707\n",
      "Iteration 89, loss = 0.01344040\n",
      "Iteration 90, loss = 0.01304566\n",
      "Iteration 91, loss = 0.01264214\n",
      "Iteration 92, loss = 0.01265108\n",
      "Iteration 93, loss = 0.01216575\n",
      "Iteration 94, loss = 0.01219788\n",
      "Iteration 95, loss = 0.01208476\n",
      "Iteration 96, loss = 0.01239686\n",
      "Iteration 97, loss = 0.01220129\n",
      "Iteration 98, loss = 0.01210388\n",
      "Iteration 99, loss = 0.01169321\n",
      "Iteration 100, loss = 0.01161523\n",
      "Iteration 101, loss = 0.01127771\n",
      "Iteration 102, loss = 0.01130709\n",
      "Iteration 103, loss = 0.01104295\n",
      "Iteration 104, loss = 0.01130975\n",
      "Iteration 105, loss = 0.01129806\n",
      "Iteration 106, loss = 0.01117117\n",
      "Iteration 107, loss = 0.01079166\n",
      "Iteration 108, loss = 0.01098205\n",
      "Iteration 109, loss = 0.01062836\n",
      "Iteration 110, loss = 0.01071289\n",
      "Iteration 111, loss = 0.01073517\n",
      "Iteration 112, loss = 0.01052547\n",
      "Iteration 113, loss = 0.01047156\n",
      "Iteration 114, loss = 0.01060070\n",
      "Iteration 115, loss = 0.01050663\n",
      "Iteration 116, loss = 0.01050654\n",
      "Iteration 117, loss = 0.01019425\n",
      "Iteration 118, loss = 0.01053727\n",
      "Iteration 119, loss = 0.01061764\n",
      "Iteration 120, loss = 0.01054813\n",
      "Iteration 121, loss = 0.01024810\n",
      "Iteration 122, loss = 0.01027402\n",
      "Iteration 123, loss = 0.01011179\n",
      "Iteration 124, loss = 0.00972416\n",
      "Iteration 125, loss = 0.00992948\n",
      "Iteration 126, loss = 0.00980792\n",
      "Iteration 127, loss = 0.01015094\n",
      "Iteration 128, loss = 0.00993504\n",
      "Iteration 129, loss = 0.00960915\n",
      "Iteration 130, loss = 0.00953205\n",
      "Iteration 131, loss = 0.00951497\n",
      "Iteration 132, loss = 0.00958786\n",
      "Iteration 133, loss = 0.00945576\n",
      "Iteration 134, loss = 0.00955103\n",
      "Iteration 135, loss = 0.00950384\n",
      "Iteration 136, loss = 0.00937868\n",
      "Iteration 137, loss = 0.00913423\n",
      "Iteration 138, loss = 0.00925115\n",
      "Iteration 139, loss = 0.00920785\n",
      "Iteration 140, loss = 0.00904788\n",
      "Iteration 141, loss = 0.00889516\n",
      "Iteration 142, loss = 0.00904741\n",
      "Iteration 143, loss = 0.00893909\n",
      "Iteration 144, loss = 0.00914001\n",
      "Iteration 145, loss = 0.00879972\n",
      "Iteration 146, loss = 0.00872399\n",
      "Iteration 147, loss = 0.00856391\n",
      "Iteration 148, loss = 0.00903515\n",
      "Iteration 149, loss = 0.00932039\n",
      "Iteration 150, loss = 0.00886325\n",
      "Iteration 151, loss = 0.00888675\n",
      "Iteration 152, loss = 0.00896515\n",
      "Iteration 153, loss = 0.00895265\n",
      "Iteration 154, loss = 0.00884713\n",
      "Iteration 155, loss = 0.00832727\n",
      "Iteration 156, loss = 0.00847774\n",
      "Iteration 157, loss = 0.00825418\n",
      "Iteration 158, loss = 0.00841349\n",
      "Iteration 159, loss = 0.00864860\n",
      "Iteration 160, loss = 0.00842180\n",
      "Iteration 161, loss = 0.00886050\n",
      "Iteration 162, loss = 0.00905408\n",
      "Iteration 163, loss = 0.00891603\n",
      "Iteration 164, loss = 0.00851154\n",
      "Iteration 165, loss = 0.00823451\n",
      "Iteration 166, loss = 0.00817040\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.40698515\n",
      "Iteration 2, loss = 0.29952219\n",
      "Iteration 3, loss = 0.26000358\n",
      "Iteration 4, loss = 0.22312230\n",
      "Iteration 5, loss = 0.19825517\n",
      "Iteration 6, loss = 0.17624303\n",
      "Iteration 7, loss = 0.15395664\n",
      "Iteration 8, loss = 0.13621121\n",
      "Iteration 9, loss = 0.12022624\n",
      "Iteration 10, loss = 0.10845334\n",
      "Iteration 11, loss = 0.09632854\n",
      "Iteration 12, loss = 0.08799728\n",
      "Iteration 13, loss = 0.08050629\n",
      "Iteration 14, loss = 0.07486450\n",
      "Iteration 15, loss = 0.06963585\n",
      "Iteration 16, loss = 0.06610835\n",
      "Iteration 17, loss = 0.06340914\n",
      "Iteration 18, loss = 0.05980768\n",
      "Iteration 19, loss = 0.05741534\n",
      "Iteration 20, loss = 0.05534602\n",
      "Iteration 21, loss = 0.05355368\n",
      "Iteration 22, loss = 0.05135019\n",
      "Iteration 23, loss = 0.04965888\n",
      "Iteration 24, loss = 0.04833089\n",
      "Iteration 25, loss = 0.04640989\n",
      "Iteration 26, loss = 0.04457686\n",
      "Iteration 27, loss = 0.04279120\n",
      "Iteration 28, loss = 0.04173915\n",
      "Iteration 29, loss = 0.04061220\n",
      "Iteration 30, loss = 0.03908469\n",
      "Iteration 31, loss = 0.03875706\n",
      "Iteration 32, loss = 0.03728887\n",
      "Iteration 33, loss = 0.03619129\n",
      "Iteration 34, loss = 0.03540628\n",
      "Iteration 35, loss = 0.03436204\n",
      "Iteration 36, loss = 0.03340948\n",
      "Iteration 37, loss = 0.03248634\n",
      "Iteration 38, loss = 0.03180024\n",
      "Iteration 39, loss = 0.03077907\n",
      "Iteration 40, loss = 0.03080859\n",
      "Iteration 41, loss = 0.02970171\n",
      "Iteration 42, loss = 0.02871766\n",
      "Iteration 43, loss = 0.02830559\n",
      "Iteration 44, loss = 0.02778104\n",
      "Iteration 45, loss = 0.02685035\n",
      "Iteration 46, loss = 0.02685345\n",
      "Iteration 47, loss = 0.02683534\n",
      "Iteration 48, loss = 0.02574397\n",
      "Iteration 49, loss = 0.02478901\n",
      "Iteration 50, loss = 0.02456007\n",
      "Iteration 51, loss = 0.02361694\n",
      "Iteration 52, loss = 0.02316028\n",
      "Iteration 53, loss = 0.02253283\n",
      "Iteration 54, loss = 0.02228084\n",
      "Iteration 55, loss = 0.02162991\n",
      "Iteration 56, loss = 0.02110833\n",
      "Iteration 57, loss = 0.02062183\n",
      "Iteration 58, loss = 0.02021588\n",
      "Iteration 59, loss = 0.01976469\n",
      "Iteration 60, loss = 0.01931145\n",
      "Iteration 61, loss = 0.01895955\n",
      "Iteration 62, loss = 0.01838263\n",
      "Iteration 63, loss = 0.01841635\n",
      "Iteration 64, loss = 0.01790479\n",
      "Iteration 65, loss = 0.01763050\n",
      "Iteration 66, loss = 0.01715897\n",
      "Iteration 67, loss = 0.01713285\n",
      "Iteration 68, loss = 0.01663225\n",
      "Iteration 69, loss = 0.01638377\n",
      "Iteration 70, loss = 0.01602685\n",
      "Iteration 71, loss = 0.01598255\n",
      "Iteration 72, loss = 0.01568633\n",
      "Iteration 73, loss = 0.01548726\n",
      "Iteration 74, loss = 0.01554200\n",
      "Iteration 75, loss = 0.01565490\n",
      "Iteration 76, loss = 0.01495324\n",
      "Iteration 77, loss = 0.01474040\n",
      "Iteration 78, loss = 0.01443204\n",
      "Iteration 79, loss = 0.01414809\n",
      "Iteration 80, loss = 0.01410025\n",
      "Iteration 81, loss = 0.01378777\n",
      "Iteration 82, loss = 0.01371512\n",
      "Iteration 83, loss = 0.01348491\n",
      "Iteration 84, loss = 0.01345294\n",
      "Iteration 85, loss = 0.01334147\n",
      "Iteration 86, loss = 0.01287810\n",
      "Iteration 87, loss = 0.01296180\n",
      "Iteration 88, loss = 0.01272707\n",
      "Iteration 89, loss = 0.01344040\n",
      "Iteration 90, loss = 0.01304566\n",
      "Iteration 91, loss = 0.01264214\n",
      "Iteration 92, loss = 0.01265108\n",
      "Iteration 93, loss = 0.01216575\n",
      "Iteration 94, loss = 0.01219788\n",
      "Iteration 95, loss = 0.01208476\n",
      "Iteration 96, loss = 0.01239686\n",
      "Iteration 97, loss = 0.01220129\n",
      "Iteration 98, loss = 0.01210388\n",
      "Iteration 99, loss = 0.01169321\n",
      "Iteration 100, loss = 0.01161523\n",
      "Iteration 101, loss = 0.01127771\n",
      "Iteration 102, loss = 0.01130709\n",
      "Iteration 103, loss = 0.01104295\n",
      "Iteration 104, loss = 0.01130975\n",
      "Iteration 105, loss = 0.01129806\n",
      "Iteration 106, loss = 0.01117117\n",
      "Iteration 107, loss = 0.01079166\n",
      "Iteration 108, loss = 0.01098205\n",
      "Iteration 109, loss = 0.01062836\n",
      "Iteration 110, loss = 0.01071289\n",
      "Iteration 111, loss = 0.01073517\n",
      "Iteration 112, loss = 0.01052547\n",
      "Iteration 113, loss = 0.01047156\n",
      "Iteration 114, loss = 0.01060070\n",
      "Iteration 115, loss = 0.01050663\n",
      "Iteration 116, loss = 0.01050654\n",
      "Iteration 117, loss = 0.01019425\n",
      "Iteration 118, loss = 0.01053727\n",
      "Iteration 119, loss = 0.01061764\n",
      "Iteration 120, loss = 0.01054813\n",
      "Iteration 121, loss = 0.01024810\n",
      "Iteration 122, loss = 0.01027402\n",
      "Iteration 123, loss = 0.01011179\n",
      "Iteration 124, loss = 0.00972416\n",
      "Iteration 125, loss = 0.00992948\n",
      "Iteration 126, loss = 0.00980792\n",
      "Iteration 127, loss = 0.01015094\n",
      "Iteration 128, loss = 0.00993504\n",
      "Iteration 129, loss = 0.00960915\n",
      "Iteration 130, loss = 0.00953205\n",
      "Iteration 131, loss = 0.00951497\n",
      "Iteration 132, loss = 0.00958786\n",
      "Iteration 133, loss = 0.00945576\n",
      "Iteration 134, loss = 0.00955103\n",
      "Iteration 135, loss = 0.00950384\n",
      "Iteration 136, loss = 0.00937868\n",
      "Iteration 137, loss = 0.00913423\n",
      "Iteration 138, loss = 0.00925115\n",
      "Iteration 139, loss = 0.00920785\n",
      "Iteration 140, loss = 0.00904788\n",
      "Iteration 141, loss = 0.00889516\n",
      "Iteration 142, loss = 0.00904741\n",
      "Iteration 143, loss = 0.00893909\n",
      "Iteration 144, loss = 0.00914001\n",
      "Iteration 145, loss = 0.00879972\n",
      "Iteration 146, loss = 0.00872399\n",
      "Iteration 147, loss = 0.00856391\n",
      "Iteration 148, loss = 0.00903515\n",
      "Iteration 149, loss = 0.00932039\n",
      "Iteration 150, loss = 0.00886325\n",
      "Iteration 151, loss = 0.00888675\n",
      "Iteration 152, loss = 0.00896515\n",
      "Iteration 153, loss = 0.00895265\n",
      "Iteration 154, loss = 0.00884713\n",
      "Iteration 155, loss = 0.00832727\n",
      "Iteration 156, loss = 0.00847774\n",
      "Iteration 157, loss = 0.00825418\n",
      "Iteration 158, loss = 0.00841349\n",
      "Iteration 159, loss = 0.00864860\n",
      "Iteration 160, loss = 0.00842180\n",
      "Iteration 161, loss = 0.00886050\n",
      "Iteration 162, loss = 0.00905408\n",
      "Iteration 163, loss = 0.00891603\n",
      "Iteration 164, loss = 0.00851154\n",
      "Iteration 165, loss = 0.00823451\n",
      "Iteration 166, loss = 0.00817040\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.40698515\n",
      "Iteration 2, loss = 0.29952219\n",
      "Iteration 3, loss = 0.26000358\n",
      "Iteration 4, loss = 0.22312230\n",
      "Iteration 5, loss = 0.19825517\n",
      "Iteration 6, loss = 0.17624303\n",
      "Iteration 7, loss = 0.15395664\n",
      "Iteration 8, loss = 0.13621121\n",
      "Iteration 9, loss = 0.12022624\n",
      "Iteration 10, loss = 0.10845334\n",
      "Iteration 11, loss = 0.09632854\n",
      "Iteration 12, loss = 0.08799728\n",
      "Iteration 13, loss = 0.08050629\n",
      "Iteration 14, loss = 0.07486450\n",
      "Iteration 15, loss = 0.06963585\n",
      "Iteration 16, loss = 0.06610835\n",
      "Iteration 17, loss = 0.06340914\n",
      "Iteration 18, loss = 0.05980768\n",
      "Iteration 19, loss = 0.05741534\n",
      "Iteration 20, loss = 0.05534602\n",
      "Iteration 21, loss = 0.05355368\n",
      "Iteration 22, loss = 0.05135019\n",
      "Iteration 23, loss = 0.04965888\n",
      "Iteration 24, loss = 0.04833089\n",
      "Iteration 25, loss = 0.04640989\n",
      "Iteration 26, loss = 0.04457686\n",
      "Iteration 27, loss = 0.04279120\n",
      "Iteration 28, loss = 0.04173915\n",
      "Iteration 29, loss = 0.04061220\n",
      "Iteration 30, loss = 0.03908469\n",
      "Iteration 31, loss = 0.03875706\n",
      "Iteration 32, loss = 0.03728887\n",
      "Iteration 33, loss = 0.03619129\n",
      "Iteration 34, loss = 0.03540628\n",
      "Iteration 35, loss = 0.03436204\n",
      "Iteration 36, loss = 0.03340948\n",
      "Iteration 37, loss = 0.03248634\n",
      "Iteration 38, loss = 0.03180024\n",
      "Iteration 39, loss = 0.03077907\n",
      "Iteration 40, loss = 0.03080859\n",
      "Iteration 41, loss = 0.02970171\n",
      "Iteration 42, loss = 0.02871766\n",
      "Iteration 43, loss = 0.02830559\n",
      "Iteration 44, loss = 0.02778104\n",
      "Iteration 45, loss = 0.02685035\n",
      "Iteration 46, loss = 0.02685345\n",
      "Iteration 47, loss = 0.02683534\n",
      "Iteration 48, loss = 0.02574397\n",
      "Iteration 49, loss = 0.02478901\n",
      "Iteration 50, loss = 0.02456007\n",
      "Iteration 51, loss = 0.02361694\n",
      "Iteration 52, loss = 0.02316028\n",
      "Iteration 53, loss = 0.02253283\n",
      "Iteration 54, loss = 0.02228084\n",
      "Iteration 55, loss = 0.02162991\n",
      "Iteration 56, loss = 0.02110833\n",
      "Iteration 57, loss = 0.02062183\n",
      "Iteration 58, loss = 0.02021588\n",
      "Iteration 59, loss = 0.01976469\n",
      "Iteration 60, loss = 0.01931145\n",
      "Iteration 61, loss = 0.01895955\n",
      "Iteration 62, loss = 0.01838263\n",
      "Iteration 63, loss = 0.01841635\n",
      "Iteration 64, loss = 0.01790479\n",
      "Iteration 65, loss = 0.01763050\n",
      "Iteration 66, loss = 0.01715897\n",
      "Iteration 67, loss = 0.01713285\n",
      "Iteration 68, loss = 0.01663225\n",
      "Iteration 69, loss = 0.01638377\n",
      "Iteration 70, loss = 0.01602685\n",
      "Iteration 71, loss = 0.01598255\n",
      "Iteration 72, loss = 0.01568633\n",
      "Iteration 73, loss = 0.01548726\n",
      "Iteration 74, loss = 0.01554200\n",
      "Iteration 75, loss = 0.01565490\n",
      "Iteration 76, loss = 0.01495324\n",
      "Iteration 77, loss = 0.01474040\n",
      "Iteration 78, loss = 0.01443204\n",
      "Iteration 79, loss = 0.01414809\n",
      "Iteration 80, loss = 0.01410025\n",
      "Iteration 81, loss = 0.01378777\n",
      "Iteration 82, loss = 0.01371512\n",
      "Iteration 83, loss = 0.01348491\n",
      "Iteration 84, loss = 0.01345294\n",
      "Iteration 85, loss = 0.01334147\n",
      "Iteration 86, loss = 0.01287810\n",
      "Iteration 87, loss = 0.01296180\n",
      "Iteration 88, loss = 0.01272707\n",
      "Iteration 89, loss = 0.01344040\n",
      "Iteration 90, loss = 0.01304566\n",
      "Iteration 91, loss = 0.01264214\n",
      "Iteration 92, loss = 0.01265108\n",
      "Iteration 93, loss = 0.01216575\n",
      "Iteration 94, loss = 0.01219788\n",
      "Iteration 95, loss = 0.01208476\n",
      "Iteration 96, loss = 0.01239686\n",
      "Iteration 97, loss = 0.01220129\n",
      "Iteration 98, loss = 0.01210388\n",
      "Iteration 99, loss = 0.01169321\n",
      "Iteration 100, loss = 0.01161523\n",
      "Iteration 101, loss = 0.01127771\n",
      "Iteration 102, loss = 0.01130709\n",
      "Iteration 103, loss = 0.01104295\n",
      "Iteration 104, loss = 0.01130975\n",
      "Iteration 105, loss = 0.01129806\n",
      "Iteration 106, loss = 0.01117117\n",
      "Iteration 107, loss = 0.01079166\n",
      "Iteration 108, loss = 0.01098205\n",
      "Iteration 109, loss = 0.01062836\n",
      "Iteration 110, loss = 0.01071289\n",
      "Iteration 111, loss = 0.01073517\n",
      "Iteration 112, loss = 0.01052547\n",
      "Iteration 113, loss = 0.01047156\n",
      "Iteration 114, loss = 0.01060070\n",
      "Iteration 115, loss = 0.01050663\n",
      "Iteration 116, loss = 0.01050654\n",
      "Iteration 117, loss = 0.01019425\n",
      "Iteration 118, loss = 0.01053727\n",
      "Iteration 119, loss = 0.01061764\n",
      "Iteration 120, loss = 0.01054813\n",
      "Iteration 121, loss = 0.01024810\n",
      "Iteration 122, loss = 0.01027402\n",
      "Iteration 123, loss = 0.01011179\n",
      "Iteration 124, loss = 0.00972416\n",
      "Iteration 125, loss = 0.00992948\n",
      "Iteration 126, loss = 0.00980792\n",
      "Iteration 127, loss = 0.01015094\n",
      "Iteration 128, loss = 0.00993504\n",
      "Iteration 129, loss = 0.00960915\n",
      "Iteration 130, loss = 0.00953205\n",
      "Iteration 131, loss = 0.00951497\n",
      "Iteration 132, loss = 0.00958786\n",
      "Iteration 133, loss = 0.00945576\n",
      "Iteration 134, loss = 0.00955103\n",
      "Iteration 135, loss = 0.00950384\n",
      "Iteration 136, loss = 0.00937868\n",
      "Iteration 137, loss = 0.00913423\n",
      "Iteration 138, loss = 0.00925115\n",
      "Iteration 139, loss = 0.00920785\n",
      "Iteration 140, loss = 0.00904788\n",
      "Iteration 141, loss = 0.00889516\n",
      "Iteration 142, loss = 0.00904741\n",
      "Iteration 143, loss = 0.00893909\n",
      "Iteration 144, loss = 0.00914001\n",
      "Iteration 145, loss = 0.00879972\n",
      "Iteration 146, loss = 0.00872399\n",
      "Iteration 147, loss = 0.00856391\n",
      "Iteration 148, loss = 0.00903515\n",
      "Iteration 149, loss = 0.00932039\n",
      "Iteration 150, loss = 0.00886325\n",
      "Iteration 151, loss = 0.00888675\n",
      "Iteration 152, loss = 0.00896515\n",
      "Iteration 153, loss = 0.00895265\n",
      "Iteration 154, loss = 0.00884713\n",
      "Iteration 155, loss = 0.00832727\n",
      "Iteration 156, loss = 0.00847774\n",
      "Iteration 157, loss = 0.00825418\n",
      "Iteration 158, loss = 0.00841349\n",
      "Iteration 159, loss = 0.00864860\n",
      "Iteration 160, loss = 0.00842180\n",
      "Iteration 161, loss = 0.00886050\n",
      "Iteration 162, loss = 0.00905408\n",
      "Iteration 163, loss = 0.00891603\n",
      "Iteration 164, loss = 0.00851154\n",
      "Iteration 165, loss = 0.00823451\n",
      "Iteration 166, loss = 0.00817040\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.41603049\n",
      "Iteration 2, loss = 0.33327896\n",
      "Iteration 3, loss = 0.29571976\n",
      "Iteration 4, loss = 0.26498146\n",
      "Iteration 5, loss = 0.23984071\n",
      "Iteration 6, loss = 0.21235728\n",
      "Iteration 7, loss = 0.19031268\n",
      "Iteration 8, loss = 0.16865000\n",
      "Iteration 9, loss = 0.14916147\n",
      "Iteration 10, loss = 0.13226910\n",
      "Iteration 11, loss = 0.11664769\n",
      "Iteration 12, loss = 0.10424684\n",
      "Iteration 13, loss = 0.09535827\n",
      "Iteration 14, loss = 0.08618732\n",
      "Iteration 15, loss = 0.07924203\n",
      "Iteration 16, loss = 0.07391710\n",
      "Iteration 17, loss = 0.06941200\n",
      "Iteration 18, loss = 0.06618390\n",
      "Iteration 19, loss = 0.06328363\n",
      "Iteration 20, loss = 0.05956393\n",
      "Iteration 21, loss = 0.05836112\n",
      "Iteration 22, loss = 0.05475058\n",
      "Iteration 23, loss = 0.05284616\n",
      "Iteration 24, loss = 0.05106433\n",
      "Iteration 25, loss = 0.04975709\n",
      "Iteration 26, loss = 0.04689322\n",
      "Iteration 27, loss = 0.04607378\n",
      "Iteration 28, loss = 0.04442021\n",
      "Iteration 29, loss = 0.04307542\n",
      "Iteration 30, loss = 0.04185707\n",
      "Iteration 31, loss = 0.04036519\n",
      "Iteration 32, loss = 0.03951509\n",
      "Iteration 33, loss = 0.03794383\n",
      "Iteration 34, loss = 0.03726112\n",
      "Iteration 35, loss = 0.03594755\n",
      "Iteration 36, loss = 0.03533874\n",
      "Iteration 37, loss = 0.03409139\n",
      "Iteration 38, loss = 0.03310318\n",
      "Iteration 39, loss = 0.03229249\n",
      "Iteration 40, loss = 0.03142646\n",
      "Iteration 41, loss = 0.03086407\n",
      "Iteration 42, loss = 0.03018104\n",
      "Iteration 43, loss = 0.02922321\n",
      "Iteration 44, loss = 0.02847509\n",
      "Iteration 45, loss = 0.02807441\n",
      "Iteration 46, loss = 0.02747584\n",
      "Iteration 47, loss = 0.02665211\n",
      "Iteration 48, loss = 0.02612762\n",
      "Iteration 49, loss = 0.02576386\n",
      "Iteration 50, loss = 0.02534340\n",
      "Iteration 51, loss = 0.02461968\n",
      "Iteration 52, loss = 0.02400930\n",
      "Iteration 53, loss = 0.02364013\n",
      "Iteration 54, loss = 0.02273321\n",
      "Iteration 55, loss = 0.02253240\n",
      "Iteration 56, loss = 0.02226306\n",
      "Iteration 57, loss = 0.02185713\n",
      "Iteration 58, loss = 0.02154667\n",
      "Iteration 59, loss = 0.02142096\n",
      "Iteration 60, loss = 0.02063120\n",
      "Iteration 61, loss = 0.02044827\n",
      "Iteration 62, loss = 0.01985730\n",
      "Iteration 63, loss = 0.01948981\n",
      "Iteration 64, loss = 0.01891007\n",
      "Iteration 65, loss = 0.01845115\n",
      "Iteration 66, loss = 0.01807421\n",
      "Iteration 67, loss = 0.01792086\n",
      "Iteration 68, loss = 0.01784784\n",
      "Iteration 69, loss = 0.01763288\n",
      "Iteration 70, loss = 0.01711580\n",
      "Iteration 71, loss = 0.01688752\n",
      "Iteration 72, loss = 0.01686826\n",
      "Iteration 73, loss = 0.01641924\n",
      "Iteration 74, loss = 0.01692270\n",
      "Iteration 75, loss = 0.01643788\n",
      "Iteration 76, loss = 0.01644317\n",
      "Iteration 77, loss = 0.01604669\n",
      "Iteration 78, loss = 0.01515161\n",
      "Iteration 79, loss = 0.01517231\n",
      "Iteration 80, loss = 0.01470398\n",
      "Iteration 81, loss = 0.01476756\n",
      "Iteration 82, loss = 0.01421147\n",
      "Iteration 83, loss = 0.01403838\n",
      "Iteration 84, loss = 0.01370506\n",
      "Iteration 85, loss = 0.01360042\n",
      "Iteration 86, loss = 0.01351645\n",
      "Iteration 87, loss = 0.01314904\n",
      "Iteration 88, loss = 0.01293882\n",
      "Iteration 89, loss = 0.01377386\n",
      "Iteration 90, loss = 0.01339041\n",
      "Iteration 91, loss = 0.01390508\n",
      "Iteration 92, loss = 0.01296447\n",
      "Iteration 93, loss = 0.01263346\n",
      "Iteration 94, loss = 0.01233597\n",
      "Iteration 95, loss = 0.01214417\n",
      "Iteration 96, loss = 0.01228116\n",
      "Iteration 97, loss = 0.01195066\n",
      "Iteration 98, loss = 0.01150149\n",
      "Iteration 99, loss = 0.01172006\n",
      "Iteration 100, loss = 0.01147052\n",
      "Iteration 101, loss = 0.01142687\n",
      "Iteration 102, loss = 0.01116236\n",
      "Iteration 103, loss = 0.01098384\n",
      "Iteration 104, loss = 0.01099054\n",
      "Iteration 105, loss = 0.01103101\n",
      "Iteration 106, loss = 0.01084998\n",
      "Iteration 107, loss = 0.01062267\n",
      "Iteration 108, loss = 0.01047592\n",
      "Iteration 109, loss = 0.01043650\n",
      "Iteration 110, loss = 0.01007530\n",
      "Iteration 111, loss = 0.00993955\n",
      "Iteration 112, loss = 0.00971270\n",
      "Iteration 113, loss = 0.00987696\n",
      "Iteration 114, loss = 0.00976814\n",
      "Iteration 115, loss = 0.00960481\n",
      "Iteration 116, loss = 0.00963992\n",
      "Iteration 117, loss = 0.00984093\n",
      "Iteration 118, loss = 0.00914712\n",
      "Iteration 119, loss = 0.00927861\n",
      "Iteration 120, loss = 0.00904292\n",
      "Iteration 121, loss = 0.00898791\n",
      "Iteration 122, loss = 0.00898170\n",
      "Iteration 123, loss = 0.00910183\n",
      "Iteration 124, loss = 0.00882353\n",
      "Iteration 125, loss = 0.00876056\n",
      "Iteration 126, loss = 0.00849738\n",
      "Iteration 127, loss = 0.00861928\n",
      "Iteration 128, loss = 0.00862923\n",
      "Iteration 129, loss = 0.00847884\n",
      "Iteration 130, loss = 0.00833909\n",
      "Iteration 131, loss = 0.00808457\n",
      "Iteration 132, loss = 0.00807654\n",
      "Iteration 133, loss = 0.00844142\n",
      "Iteration 134, loss = 0.00815267\n",
      "Iteration 135, loss = 0.00802705\n",
      "Iteration 136, loss = 0.00805090\n",
      "Iteration 137, loss = 0.00790186\n",
      "Iteration 138, loss = 0.00815682\n",
      "Iteration 139, loss = 0.00821828\n",
      "Iteration 140, loss = 0.00783991\n",
      "Iteration 141, loss = 0.00768177\n",
      "Iteration 142, loss = 0.00736577\n",
      "Iteration 143, loss = 0.00747831\n",
      "Iteration 144, loss = 0.00744650\n",
      "Iteration 145, loss = 0.00743350\n",
      "Iteration 146, loss = 0.00839626\n",
      "Iteration 147, loss = 0.00767264\n",
      "Iteration 148, loss = 0.00742869\n",
      "Iteration 149, loss = 0.00691353\n",
      "Iteration 150, loss = 0.00711219\n",
      "Iteration 151, loss = 0.00689707\n",
      "Iteration 152, loss = 0.00674213\n",
      "Iteration 153, loss = 0.00677914\n",
      "Iteration 154, loss = 0.00671903\n",
      "Iteration 155, loss = 0.00651070\n",
      "Iteration 156, loss = 0.00649471\n",
      "Iteration 157, loss = 0.00649057\n",
      "Iteration 158, loss = 0.00644933\n",
      "Iteration 159, loss = 0.00635896\n",
      "Iteration 160, loss = 0.00644975\n",
      "Iteration 161, loss = 0.00633365\n",
      "Iteration 162, loss = 0.00623253\n",
      "Iteration 163, loss = 0.00682250\n",
      "Iteration 164, loss = 0.00649177\n",
      "Iteration 165, loss = 0.00644902\n",
      "Iteration 166, loss = 0.00617055\n",
      "Iteration 167, loss = 0.00613544\n",
      "Iteration 168, loss = 0.00601995\n",
      "Iteration 169, loss = 0.00602775\n",
      "Iteration 170, loss = 0.00604102\n",
      "Iteration 171, loss = 0.00595112\n",
      "Iteration 172, loss = 0.00581544\n",
      "Iteration 173, loss = 0.00590388\n",
      "Iteration 174, loss = 0.00574302\n",
      "Iteration 175, loss = 0.00579553\n",
      "Iteration 176, loss = 0.00576780\n",
      "Iteration 177, loss = 0.00561668\n",
      "Iteration 178, loss = 0.00544738\n",
      "Iteration 179, loss = 0.00565721\n",
      "Iteration 180, loss = 0.00543634\n",
      "Iteration 181, loss = 0.00556707\n",
      "Iteration 182, loss = 0.00560316\n",
      "Iteration 183, loss = 0.00549599\n",
      "Iteration 184, loss = 0.00555038\n",
      "Iteration 185, loss = 0.00551867\n",
      "Iteration 186, loss = 0.00547706\n",
      "Iteration 187, loss = 0.00545656\n",
      "Iteration 188, loss = 0.00540935\n",
      "Iteration 189, loss = 0.00533272\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.41603049\n",
      "Iteration 2, loss = 0.33327896\n",
      "Iteration 3, loss = 0.29571976\n",
      "Iteration 4, loss = 0.26498146\n",
      "Iteration 5, loss = 0.23984071\n",
      "Iteration 6, loss = 0.21235728\n",
      "Iteration 7, loss = 0.19031268\n",
      "Iteration 8, loss = 0.16865000\n",
      "Iteration 9, loss = 0.14916147\n",
      "Iteration 10, loss = 0.13226910\n",
      "Iteration 11, loss = 0.11664769\n",
      "Iteration 12, loss = 0.10424684\n",
      "Iteration 13, loss = 0.09535827\n",
      "Iteration 14, loss = 0.08618732\n",
      "Iteration 15, loss = 0.07924203\n",
      "Iteration 16, loss = 0.07391710\n",
      "Iteration 17, loss = 0.06941200\n",
      "Iteration 18, loss = 0.06618390\n",
      "Iteration 19, loss = 0.06328363\n",
      "Iteration 20, loss = 0.05956393\n",
      "Iteration 21, loss = 0.05836112\n",
      "Iteration 22, loss = 0.05475058\n",
      "Iteration 23, loss = 0.05284616\n",
      "Iteration 24, loss = 0.05106433\n",
      "Iteration 25, loss = 0.04975709\n",
      "Iteration 26, loss = 0.04689322\n",
      "Iteration 27, loss = 0.04607378\n",
      "Iteration 28, loss = 0.04442021\n",
      "Iteration 29, loss = 0.04307542\n",
      "Iteration 30, loss = 0.04185707\n",
      "Iteration 31, loss = 0.04036519\n",
      "Iteration 32, loss = 0.03951509\n",
      "Iteration 33, loss = 0.03794383\n",
      "Iteration 34, loss = 0.03726112\n",
      "Iteration 35, loss = 0.03594755\n",
      "Iteration 36, loss = 0.03533874\n",
      "Iteration 37, loss = 0.03409139\n",
      "Iteration 38, loss = 0.03310318\n",
      "Iteration 39, loss = 0.03229249\n",
      "Iteration 40, loss = 0.03142646\n",
      "Iteration 41, loss = 0.03086407\n",
      "Iteration 42, loss = 0.03018104\n",
      "Iteration 43, loss = 0.02922321\n",
      "Iteration 44, loss = 0.02847509\n",
      "Iteration 45, loss = 0.02807441\n",
      "Iteration 46, loss = 0.02747584\n",
      "Iteration 47, loss = 0.02665211\n",
      "Iteration 48, loss = 0.02612762\n",
      "Iteration 49, loss = 0.02576386\n",
      "Iteration 50, loss = 0.02534340\n",
      "Iteration 51, loss = 0.02461968\n",
      "Iteration 52, loss = 0.02400930\n",
      "Iteration 53, loss = 0.02364013\n",
      "Iteration 54, loss = 0.02273321\n",
      "Iteration 55, loss = 0.02253240\n",
      "Iteration 56, loss = 0.02226306\n",
      "Iteration 57, loss = 0.02185713\n",
      "Iteration 58, loss = 0.02154667\n",
      "Iteration 59, loss = 0.02142096\n",
      "Iteration 60, loss = 0.02063120\n",
      "Iteration 61, loss = 0.02044827\n",
      "Iteration 62, loss = 0.01985730\n",
      "Iteration 63, loss = 0.01948981\n",
      "Iteration 64, loss = 0.01891007\n",
      "Iteration 65, loss = 0.01845115\n",
      "Iteration 66, loss = 0.01807421\n",
      "Iteration 67, loss = 0.01792086\n",
      "Iteration 68, loss = 0.01784784\n",
      "Iteration 69, loss = 0.01763288\n",
      "Iteration 70, loss = 0.01711580\n",
      "Iteration 71, loss = 0.01688752\n",
      "Iteration 72, loss = 0.01686826\n",
      "Iteration 73, loss = 0.01641924\n",
      "Iteration 74, loss = 0.01692270\n",
      "Iteration 75, loss = 0.01643788\n",
      "Iteration 76, loss = 0.01644317\n",
      "Iteration 77, loss = 0.01604669\n",
      "Iteration 78, loss = 0.01515161\n",
      "Iteration 79, loss = 0.01517231\n",
      "Iteration 80, loss = 0.01470398\n",
      "Iteration 81, loss = 0.01476756\n",
      "Iteration 82, loss = 0.01421147\n",
      "Iteration 83, loss = 0.01403838\n",
      "Iteration 84, loss = 0.01370506\n",
      "Iteration 85, loss = 0.01360042\n",
      "Iteration 86, loss = 0.01351645\n",
      "Iteration 87, loss = 0.01314904\n",
      "Iteration 88, loss = 0.01293882\n",
      "Iteration 89, loss = 0.01377386\n",
      "Iteration 90, loss = 0.01339041\n",
      "Iteration 91, loss = 0.01390508\n",
      "Iteration 92, loss = 0.01296447\n",
      "Iteration 93, loss = 0.01263346\n",
      "Iteration 94, loss = 0.01233597\n",
      "Iteration 95, loss = 0.01214417\n",
      "Iteration 96, loss = 0.01228116\n",
      "Iteration 97, loss = 0.01195066\n",
      "Iteration 98, loss = 0.01150149\n",
      "Iteration 99, loss = 0.01172006\n",
      "Iteration 100, loss = 0.01147052\n",
      "Iteration 101, loss = 0.01142687\n",
      "Iteration 102, loss = 0.01116236\n",
      "Iteration 103, loss = 0.01098384\n",
      "Iteration 104, loss = 0.01099054\n",
      "Iteration 105, loss = 0.01103101\n",
      "Iteration 106, loss = 0.01084998\n",
      "Iteration 107, loss = 0.01062267\n",
      "Iteration 108, loss = 0.01047592\n",
      "Iteration 109, loss = 0.01043650\n",
      "Iteration 110, loss = 0.01007530\n",
      "Iteration 111, loss = 0.00993955\n",
      "Iteration 112, loss = 0.00971270\n",
      "Iteration 113, loss = 0.00987696\n",
      "Iteration 114, loss = 0.00976814\n",
      "Iteration 115, loss = 0.00960481\n",
      "Iteration 116, loss = 0.00963992\n",
      "Iteration 117, loss = 0.00984093\n",
      "Iteration 118, loss = 0.00914712\n",
      "Iteration 119, loss = 0.00927861\n",
      "Iteration 120, loss = 0.00904292\n",
      "Iteration 121, loss = 0.00898791\n",
      "Iteration 122, loss = 0.00898170\n",
      "Iteration 123, loss = 0.00910183\n",
      "Iteration 124, loss = 0.00882353\n",
      "Iteration 125, loss = 0.00876056\n",
      "Iteration 126, loss = 0.00849738\n",
      "Iteration 127, loss = 0.00861928\n",
      "Iteration 128, loss = 0.00862923\n",
      "Iteration 129, loss = 0.00847884\n",
      "Iteration 130, loss = 0.00833909\n",
      "Iteration 131, loss = 0.00808457\n",
      "Iteration 132, loss = 0.00807654\n",
      "Iteration 133, loss = 0.00844142\n",
      "Iteration 134, loss = 0.00815267\n",
      "Iteration 135, loss = 0.00802705\n",
      "Iteration 136, loss = 0.00805090\n",
      "Iteration 137, loss = 0.00790186\n",
      "Iteration 138, loss = 0.00815682\n",
      "Iteration 139, loss = 0.00821828\n",
      "Iteration 140, loss = 0.00783991\n",
      "Iteration 141, loss = 0.00768177\n",
      "Iteration 142, loss = 0.00736577\n",
      "Iteration 143, loss = 0.00747831\n",
      "Iteration 144, loss = 0.00744650\n",
      "Iteration 145, loss = 0.00743350\n",
      "Iteration 146, loss = 0.00839626\n",
      "Iteration 147, loss = 0.00767264\n",
      "Iteration 148, loss = 0.00742869\n",
      "Iteration 149, loss = 0.00691353\n",
      "Iteration 150, loss = 0.00711219\n",
      "Iteration 151, loss = 0.00689707\n",
      "Iteration 152, loss = 0.00674213\n",
      "Iteration 153, loss = 0.00677914\n",
      "Iteration 154, loss = 0.00671903\n",
      "Iteration 155, loss = 0.00651070\n",
      "Iteration 156, loss = 0.00649471\n",
      "Iteration 157, loss = 0.00649057\n",
      "Iteration 158, loss = 0.00644933\n",
      "Iteration 159, loss = 0.00635896\n",
      "Iteration 160, loss = 0.00644975\n",
      "Iteration 161, loss = 0.00633365\n",
      "Iteration 162, loss = 0.00623253\n",
      "Iteration 163, loss = 0.00682250\n",
      "Iteration 164, loss = 0.00649177\n",
      "Iteration 165, loss = 0.00644902\n",
      "Iteration 166, loss = 0.00617055\n",
      "Iteration 167, loss = 0.00613544\n",
      "Iteration 168, loss = 0.00601995\n",
      "Iteration 169, loss = 0.00602775\n",
      "Iteration 170, loss = 0.00604102\n",
      "Iteration 171, loss = 0.00595112\n",
      "Iteration 172, loss = 0.00581544\n",
      "Iteration 173, loss = 0.00590388\n",
      "Iteration 174, loss = 0.00574302\n",
      "Iteration 175, loss = 0.00579553\n",
      "Iteration 176, loss = 0.00576780\n",
      "Iteration 177, loss = 0.00561668\n",
      "Iteration 178, loss = 0.00544738\n",
      "Iteration 179, loss = 0.00565721\n",
      "Iteration 180, loss = 0.00543634\n",
      "Iteration 181, loss = 0.00556707\n",
      "Iteration 182, loss = 0.00560316\n",
      "Iteration 183, loss = 0.00549599\n",
      "Iteration 184, loss = 0.00555038\n",
      "Iteration 185, loss = 0.00551867\n",
      "Iteration 186, loss = 0.00547706\n",
      "Iteration 187, loss = 0.00545656\n",
      "Iteration 188, loss = 0.00540935\n",
      "Iteration 189, loss = 0.00533272\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.41603049\n",
      "Iteration 2, loss = 0.33327896\n",
      "Iteration 3, loss = 0.29571976\n",
      "Iteration 4, loss = 0.26498146\n",
      "Iteration 5, loss = 0.23984071\n",
      "Iteration 6, loss = 0.21235728\n",
      "Iteration 7, loss = 0.19031268\n",
      "Iteration 8, loss = 0.16865000\n",
      "Iteration 9, loss = 0.14916147\n",
      "Iteration 10, loss = 0.13226910\n",
      "Iteration 11, loss = 0.11664769\n",
      "Iteration 12, loss = 0.10424684\n",
      "Iteration 13, loss = 0.09535827\n",
      "Iteration 14, loss = 0.08618732\n",
      "Iteration 15, loss = 0.07924203\n",
      "Iteration 16, loss = 0.07391710\n",
      "Iteration 17, loss = 0.06941200\n",
      "Iteration 18, loss = 0.06618390\n",
      "Iteration 19, loss = 0.06328363\n",
      "Iteration 20, loss = 0.05956393\n",
      "Iteration 21, loss = 0.05836112\n",
      "Iteration 22, loss = 0.05475058\n",
      "Iteration 23, loss = 0.05284616\n",
      "Iteration 24, loss = 0.05106433\n",
      "Iteration 25, loss = 0.04975709\n",
      "Iteration 26, loss = 0.04689322\n",
      "Iteration 27, loss = 0.04607378\n",
      "Iteration 28, loss = 0.04442021\n",
      "Iteration 29, loss = 0.04307542\n",
      "Iteration 30, loss = 0.04185707\n",
      "Iteration 31, loss = 0.04036519\n",
      "Iteration 32, loss = 0.03951509\n",
      "Iteration 33, loss = 0.03794383\n",
      "Iteration 34, loss = 0.03726112\n",
      "Iteration 35, loss = 0.03594755\n",
      "Iteration 36, loss = 0.03533874\n",
      "Iteration 37, loss = 0.03409139\n",
      "Iteration 38, loss = 0.03310318\n",
      "Iteration 39, loss = 0.03229249\n",
      "Iteration 40, loss = 0.03142646\n",
      "Iteration 41, loss = 0.03086407\n",
      "Iteration 42, loss = 0.03018104\n",
      "Iteration 43, loss = 0.02922321\n",
      "Iteration 44, loss = 0.02847509\n",
      "Iteration 45, loss = 0.02807441\n",
      "Iteration 46, loss = 0.02747584\n",
      "Iteration 47, loss = 0.02665211\n",
      "Iteration 48, loss = 0.02612762\n",
      "Iteration 49, loss = 0.02576386\n",
      "Iteration 50, loss = 0.02534340\n",
      "Iteration 51, loss = 0.02461968\n",
      "Iteration 52, loss = 0.02400930\n",
      "Iteration 53, loss = 0.02364013\n",
      "Iteration 54, loss = 0.02273321\n",
      "Iteration 55, loss = 0.02253240\n",
      "Iteration 56, loss = 0.02226306\n",
      "Iteration 57, loss = 0.02185713\n",
      "Iteration 58, loss = 0.02154667\n",
      "Iteration 59, loss = 0.02142096\n",
      "Iteration 60, loss = 0.02063120\n",
      "Iteration 61, loss = 0.02044827\n",
      "Iteration 62, loss = 0.01985730\n",
      "Iteration 63, loss = 0.01948981\n",
      "Iteration 64, loss = 0.01891007\n",
      "Iteration 65, loss = 0.01845115\n",
      "Iteration 66, loss = 0.01807421\n",
      "Iteration 67, loss = 0.01792086\n",
      "Iteration 68, loss = 0.01784784\n",
      "Iteration 69, loss = 0.01763288\n",
      "Iteration 70, loss = 0.01711580\n",
      "Iteration 71, loss = 0.01688752\n",
      "Iteration 72, loss = 0.01686826\n",
      "Iteration 73, loss = 0.01641924\n",
      "Iteration 74, loss = 0.01692270\n",
      "Iteration 75, loss = 0.01643788\n",
      "Iteration 76, loss = 0.01644317\n",
      "Iteration 77, loss = 0.01604669\n",
      "Iteration 78, loss = 0.01515161\n",
      "Iteration 79, loss = 0.01517231\n",
      "Iteration 80, loss = 0.01470398\n",
      "Iteration 81, loss = 0.01476756\n",
      "Iteration 82, loss = 0.01421147\n",
      "Iteration 83, loss = 0.01403838\n",
      "Iteration 84, loss = 0.01370506\n",
      "Iteration 85, loss = 0.01360042\n",
      "Iteration 86, loss = 0.01351645\n",
      "Iteration 87, loss = 0.01314904\n",
      "Iteration 88, loss = 0.01293882\n",
      "Iteration 89, loss = 0.01377386\n",
      "Iteration 90, loss = 0.01339041\n",
      "Iteration 91, loss = 0.01390508\n",
      "Iteration 92, loss = 0.01296447\n",
      "Iteration 93, loss = 0.01263346\n",
      "Iteration 94, loss = 0.01233597\n",
      "Iteration 95, loss = 0.01214417\n",
      "Iteration 96, loss = 0.01228116\n",
      "Iteration 97, loss = 0.01195066\n",
      "Iteration 98, loss = 0.01150149\n",
      "Iteration 99, loss = 0.01172006\n",
      "Iteration 100, loss = 0.01147052\n",
      "Iteration 101, loss = 0.01142687\n",
      "Iteration 102, loss = 0.01116236\n",
      "Iteration 103, loss = 0.01098384\n",
      "Iteration 104, loss = 0.01099054\n",
      "Iteration 105, loss = 0.01103101\n",
      "Iteration 106, loss = 0.01084998\n",
      "Iteration 107, loss = 0.01062267\n",
      "Iteration 108, loss = 0.01047592\n",
      "Iteration 109, loss = 0.01043650\n",
      "Iteration 110, loss = 0.01007530\n",
      "Iteration 111, loss = 0.00993955\n",
      "Iteration 112, loss = 0.00971270\n",
      "Iteration 113, loss = 0.00987696\n",
      "Iteration 114, loss = 0.00976814\n",
      "Iteration 115, loss = 0.00960481\n",
      "Iteration 116, loss = 0.00963992\n",
      "Iteration 117, loss = 0.00984093\n",
      "Iteration 118, loss = 0.00914712\n",
      "Iteration 119, loss = 0.00927861\n",
      "Iteration 120, loss = 0.00904292\n",
      "Iteration 121, loss = 0.00898791\n",
      "Iteration 122, loss = 0.00898170\n",
      "Iteration 123, loss = 0.00910183\n",
      "Iteration 124, loss = 0.00882353\n",
      "Iteration 125, loss = 0.00876056\n",
      "Iteration 126, loss = 0.00849738\n",
      "Iteration 127, loss = 0.00861928\n",
      "Iteration 128, loss = 0.00862923\n",
      "Iteration 129, loss = 0.00847884\n",
      "Iteration 130, loss = 0.00833909\n",
      "Iteration 131, loss = 0.00808457\n",
      "Iteration 132, loss = 0.00807654\n",
      "Iteration 133, loss = 0.00844142\n",
      "Iteration 134, loss = 0.00815267\n",
      "Iteration 135, loss = 0.00802705\n",
      "Iteration 136, loss = 0.00805090\n",
      "Iteration 137, loss = 0.00790186\n",
      "Iteration 138, loss = 0.00815682\n",
      "Iteration 139, loss = 0.00821828\n",
      "Iteration 140, loss = 0.00783991\n",
      "Iteration 141, loss = 0.00768177\n",
      "Iteration 142, loss = 0.00736577\n",
      "Iteration 143, loss = 0.00747831\n",
      "Iteration 144, loss = 0.00744650\n",
      "Iteration 145, loss = 0.00743350\n",
      "Iteration 146, loss = 0.00839626\n",
      "Iteration 147, loss = 0.00767264\n",
      "Iteration 148, loss = 0.00742869\n",
      "Iteration 149, loss = 0.00691353\n",
      "Iteration 150, loss = 0.00711219\n",
      "Iteration 151, loss = 0.00689707\n",
      "Iteration 152, loss = 0.00674213\n",
      "Iteration 153, loss = 0.00677914\n",
      "Iteration 154, loss = 0.00671903\n",
      "Iteration 155, loss = 0.00651070\n",
      "Iteration 156, loss = 0.00649471\n",
      "Iteration 157, loss = 0.00649057\n",
      "Iteration 158, loss = 0.00644933\n",
      "Iteration 159, loss = 0.00635896\n",
      "Iteration 160, loss = 0.00644975\n",
      "Iteration 161, loss = 0.00633365\n",
      "Iteration 162, loss = 0.00623253\n",
      "Iteration 163, loss = 0.00682250\n",
      "Iteration 164, loss = 0.00649177\n",
      "Iteration 165, loss = 0.00644902\n",
      "Iteration 166, loss = 0.00617055\n",
      "Iteration 167, loss = 0.00613544\n",
      "Iteration 168, loss = 0.00601995\n",
      "Iteration 169, loss = 0.00602775\n",
      "Iteration 170, loss = 0.00604102\n",
      "Iteration 171, loss = 0.00595112\n",
      "Iteration 172, loss = 0.00581544\n",
      "Iteration 173, loss = 0.00590388\n",
      "Iteration 174, loss = 0.00574302\n",
      "Iteration 175, loss = 0.00579553\n",
      "Iteration 176, loss = 0.00576780\n",
      "Iteration 177, loss = 0.00561668\n",
      "Iteration 178, loss = 0.00544738\n",
      "Iteration 179, loss = 0.00565721\n",
      "Iteration 180, loss = 0.00543634\n",
      "Iteration 181, loss = 0.00556707\n",
      "Iteration 182, loss = 0.00560316\n",
      "Iteration 183, loss = 0.00549599\n",
      "Iteration 184, loss = 0.00555038\n",
      "Iteration 185, loss = 0.00551867\n",
      "Iteration 186, loss = 0.00547706\n",
      "Iteration 187, loss = 0.00545656\n",
      "Iteration 188, loss = 0.00540935\n",
      "Iteration 189, loss = 0.00533272\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.40014630\n",
      "Iteration 2, loss = 0.28913371\n",
      "Iteration 3, loss = 0.23766609\n",
      "Iteration 4, loss = 0.20222977\n",
      "Iteration 5, loss = 0.17445815\n",
      "Iteration 6, loss = 0.14947353\n",
      "Iteration 7, loss = 0.12975155\n",
      "Iteration 8, loss = 0.11151645\n",
      "Iteration 9, loss = 0.09777469\n",
      "Iteration 10, loss = 0.08605908\n",
      "Iteration 11, loss = 0.07870784\n",
      "Iteration 12, loss = 0.07168201\n",
      "Iteration 13, loss = 0.06670603\n",
      "Iteration 14, loss = 0.06240145\n",
      "Iteration 15, loss = 0.06025085\n",
      "Iteration 16, loss = 0.05665067\n",
      "Iteration 17, loss = 0.05549809\n",
      "Iteration 18, loss = 0.05225649\n",
      "Iteration 19, loss = 0.05082000\n",
      "Iteration 20, loss = 0.04917935\n",
      "Iteration 21, loss = 0.04709586\n",
      "Iteration 22, loss = 0.04536512\n",
      "Iteration 23, loss = 0.04443915\n",
      "Iteration 24, loss = 0.04239959\n",
      "Iteration 25, loss = 0.04059913\n",
      "Iteration 26, loss = 0.03985264\n",
      "Iteration 27, loss = 0.03859573\n",
      "Iteration 28, loss = 0.03739840\n",
      "Iteration 29, loss = 0.03601978\n",
      "Iteration 30, loss = 0.03461756\n",
      "Iteration 31, loss = 0.03417238\n",
      "Iteration 32, loss = 0.03339091\n",
      "Iteration 33, loss = 0.03235180\n",
      "Iteration 34, loss = 0.03095052\n",
      "Iteration 35, loss = 0.03030973\n",
      "Iteration 36, loss = 0.02947775\n",
      "Iteration 37, loss = 0.02904426\n",
      "Iteration 38, loss = 0.02766008\n",
      "Iteration 39, loss = 0.02692452\n",
      "Iteration 40, loss = 0.02645017\n",
      "Iteration 41, loss = 0.02554780\n",
      "Iteration 42, loss = 0.02506471\n",
      "Iteration 43, loss = 0.02416322\n",
      "Iteration 44, loss = 0.02399160\n",
      "Iteration 45, loss = 0.02314610\n",
      "Iteration 46, loss = 0.02273775\n",
      "Iteration 47, loss = 0.02223794\n",
      "Iteration 48, loss = 0.02185287\n",
      "Iteration 49, loss = 0.02184632\n",
      "Iteration 50, loss = 0.02092397\n",
      "Iteration 51, loss = 0.02053215\n",
      "Iteration 52, loss = 0.02115215\n",
      "Iteration 53, loss = 0.02046633\n",
      "Iteration 54, loss = 0.01978139\n",
      "Iteration 55, loss = 0.01926098\n",
      "Iteration 56, loss = 0.01904627\n",
      "Iteration 57, loss = 0.01861416\n",
      "Iteration 58, loss = 0.01851721\n",
      "Iteration 59, loss = 0.01816993\n",
      "Iteration 60, loss = 0.01846985\n",
      "Iteration 61, loss = 0.01789865\n",
      "Iteration 62, loss = 0.01752445\n",
      "Iteration 63, loss = 0.01700430\n",
      "Iteration 64, loss = 0.01677644\n",
      "Iteration 65, loss = 0.01641027\n",
      "Iteration 66, loss = 0.01611383\n",
      "Iteration 67, loss = 0.01634028\n",
      "Iteration 68, loss = 0.01578516\n",
      "Iteration 69, loss = 0.01592295\n",
      "Iteration 70, loss = 0.01531160\n",
      "Iteration 71, loss = 0.01539495\n",
      "Iteration 72, loss = 0.01579010\n",
      "Iteration 73, loss = 0.01520316\n",
      "Iteration 74, loss = 0.01501597\n",
      "Iteration 75, loss = 0.01509617\n",
      "Iteration 76, loss = 0.01442856\n",
      "Iteration 77, loss = 0.01421960\n",
      "Iteration 78, loss = 0.01423354\n",
      "Iteration 79, loss = 0.01407586\n",
      "Iteration 80, loss = 0.01430957\n",
      "Iteration 81, loss = 0.01433416\n",
      "Iteration 82, loss = 0.01506574\n",
      "Iteration 83, loss = 0.01480361\n",
      "Iteration 84, loss = 0.01399845\n",
      "Iteration 85, loss = 0.01386312\n",
      "Iteration 86, loss = 0.01306009\n",
      "Iteration 87, loss = 0.01293318\n",
      "Iteration 88, loss = 0.01266423\n",
      "Iteration 89, loss = 0.01263386\n",
      "Iteration 90, loss = 0.01236248\n",
      "Iteration 91, loss = 0.01279058\n",
      "Iteration 92, loss = 0.01224391\n",
      "Iteration 93, loss = 0.01214460\n",
      "Iteration 94, loss = 0.01253133\n",
      "Iteration 95, loss = 0.01200782\n",
      "Iteration 96, loss = 0.01187654\n",
      "Iteration 97, loss = 0.01163360\n",
      "Iteration 98, loss = 0.01168933\n",
      "Iteration 99, loss = 0.01150504\n",
      "Iteration 100, loss = 0.01188385\n",
      "Iteration 101, loss = 0.01166471\n",
      "Iteration 102, loss = 0.01133163\n",
      "Iteration 103, loss = 0.01114899\n",
      "Iteration 104, loss = 0.01115328\n",
      "Iteration 105, loss = 0.01118333\n",
      "Iteration 106, loss = 0.01103177\n",
      "Iteration 107, loss = 0.01077193\n",
      "Iteration 108, loss = 0.01074541\n",
      "Iteration 109, loss = 0.01071674\n",
      "Iteration 110, loss = 0.01055016\n",
      "Iteration 111, loss = 0.01122665\n",
      "Iteration 112, loss = 0.01089649\n",
      "Iteration 113, loss = 0.01035322\n",
      "Iteration 114, loss = 0.01057514\n",
      "Iteration 115, loss = 0.01130253\n",
      "Iteration 116, loss = 0.01086528\n",
      "Iteration 117, loss = 0.01060939\n",
      "Iteration 118, loss = 0.01040359\n",
      "Iteration 119, loss = 0.01025725\n",
      "Iteration 120, loss = 0.01026123\n",
      "Iteration 121, loss = 0.00998945\n",
      "Iteration 122, loss = 0.00977293\n",
      "Iteration 123, loss = 0.00974137\n",
      "Iteration 124, loss = 0.00988199\n",
      "Iteration 125, loss = 0.00954695\n",
      "Iteration 126, loss = 0.00952185\n",
      "Iteration 127, loss = 0.00983691\n",
      "Iteration 128, loss = 0.00986736\n",
      "Iteration 129, loss = 0.00948741\n",
      "Iteration 130, loss = 0.00929927\n",
      "Iteration 131, loss = 0.00916904\n",
      "Iteration 132, loss = 0.00972862\n",
      "Iteration 133, loss = 0.00949249\n",
      "Iteration 134, loss = 0.00901108\n",
      "Iteration 135, loss = 0.00918658\n",
      "Iteration 136, loss = 0.00894473\n",
      "Iteration 137, loss = 0.00914401\n",
      "Iteration 138, loss = 0.00931160\n",
      "Iteration 139, loss = 0.00915822\n",
      "Iteration 140, loss = 0.00891404\n",
      "Iteration 141, loss = 0.00862171\n",
      "Iteration 142, loss = 0.00891759\n",
      "Iteration 143, loss = 0.00925428\n",
      "Iteration 144, loss = 0.00885963\n",
      "Iteration 145, loss = 0.00921799\n",
      "Iteration 146, loss = 0.00855256\n",
      "Iteration 147, loss = 0.00868143\n",
      "Iteration 148, loss = 0.00861421\n",
      "Iteration 149, loss = 0.00845179\n",
      "Iteration 150, loss = 0.00880248\n",
      "Iteration 151, loss = 0.00842310\n",
      "Iteration 152, loss = 0.00816912\n",
      "Iteration 153, loss = 0.00847192\n",
      "Iteration 154, loss = 0.00802279\n",
      "Iteration 155, loss = 0.00843087\n",
      "Iteration 156, loss = 0.00812029\n",
      "Iteration 157, loss = 0.00808532\n",
      "Iteration 158, loss = 0.00788489\n",
      "Iteration 159, loss = 0.00781348\n",
      "Iteration 160, loss = 0.00785927\n",
      "Iteration 161, loss = 0.00815287\n",
      "Iteration 162, loss = 0.00795081\n",
      "Iteration 163, loss = 0.00771197\n",
      "Iteration 164, loss = 0.00767496\n",
      "Iteration 165, loss = 0.00775757\n",
      "Iteration 166, loss = 0.00745166\n",
      "Iteration 167, loss = 0.00738688\n",
      "Iteration 168, loss = 0.00743771\n",
      "Iteration 169, loss = 0.00771323\n",
      "Iteration 170, loss = 0.00756054\n",
      "Iteration 171, loss = 0.00778142\n",
      "Iteration 172, loss = 0.00737084\n",
      "Iteration 173, loss = 0.00746183\n",
      "Iteration 174, loss = 0.00759359\n",
      "Iteration 175, loss = 0.00818950\n",
      "Iteration 176, loss = 0.00757311\n",
      "Iteration 177, loss = 0.00727010\n",
      "Iteration 178, loss = 0.00749355\n",
      "Iteration 179, loss = 0.00742541\n",
      "Iteration 180, loss = 0.00708622\n",
      "Iteration 181, loss = 0.00704732\n",
      "Iteration 182, loss = 0.00760710\n",
      "Iteration 183, loss = 0.00742631\n",
      "Iteration 184, loss = 0.00722722\n",
      "Iteration 185, loss = 0.00672885\n",
      "Iteration 186, loss = 0.00692224\n",
      "Iteration 187, loss = 0.00657622\n",
      "Iteration 188, loss = 0.00682319\n",
      "Iteration 189, loss = 0.00667956\n",
      "Iteration 190, loss = 0.00689037\n",
      "Iteration 191, loss = 0.00685572\n",
      "Iteration 192, loss = 0.00651764\n",
      "Iteration 193, loss = 0.00645696\n",
      "Iteration 194, loss = 0.00643828\n",
      "Iteration 195, loss = 0.00652472\n",
      "Iteration 196, loss = 0.00645906\n",
      "Iteration 197, loss = 0.00630741\n",
      "Iteration 198, loss = 0.00635483\n",
      "Iteration 199, loss = 0.00630250\n",
      "Iteration 200, loss = 0.00644467\n",
      "Iteration 201, loss = 0.00624932\n",
      "Iteration 202, loss = 0.00657175\n",
      "Iteration 203, loss = 0.00658062\n",
      "Iteration 204, loss = 0.00625252\n",
      "Iteration 205, loss = 0.00626886\n",
      "Iteration 206, loss = 0.00616763\n",
      "Iteration 207, loss = 0.00606804\n",
      "Iteration 208, loss = 0.00619510\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.40014630\n",
      "Iteration 2, loss = 0.28913371\n",
      "Iteration 3, loss = 0.23766609\n",
      "Iteration 4, loss = 0.20222977\n",
      "Iteration 5, loss = 0.17445815\n",
      "Iteration 6, loss = 0.14947353\n",
      "Iteration 7, loss = 0.12975155\n",
      "Iteration 8, loss = 0.11151645\n",
      "Iteration 9, loss = 0.09777469\n",
      "Iteration 10, loss = 0.08605908\n",
      "Iteration 11, loss = 0.07870784\n",
      "Iteration 12, loss = 0.07168201\n",
      "Iteration 13, loss = 0.06670603\n",
      "Iteration 14, loss = 0.06240145\n",
      "Iteration 15, loss = 0.06025085\n",
      "Iteration 16, loss = 0.05665067\n",
      "Iteration 17, loss = 0.05549809\n",
      "Iteration 18, loss = 0.05225649\n",
      "Iteration 19, loss = 0.05082000\n",
      "Iteration 20, loss = 0.04917935\n",
      "Iteration 21, loss = 0.04709586\n",
      "Iteration 22, loss = 0.04536512\n",
      "Iteration 23, loss = 0.04443915\n",
      "Iteration 24, loss = 0.04239959\n",
      "Iteration 25, loss = 0.04059913\n",
      "Iteration 26, loss = 0.03985264\n",
      "Iteration 27, loss = 0.03859573\n",
      "Iteration 28, loss = 0.03739840\n",
      "Iteration 29, loss = 0.03601978\n",
      "Iteration 30, loss = 0.03461756\n",
      "Iteration 31, loss = 0.03417238\n",
      "Iteration 32, loss = 0.03339091\n",
      "Iteration 33, loss = 0.03235180\n",
      "Iteration 34, loss = 0.03095052\n",
      "Iteration 35, loss = 0.03030973\n",
      "Iteration 36, loss = 0.02947775\n",
      "Iteration 37, loss = 0.02904426\n",
      "Iteration 38, loss = 0.02766008\n",
      "Iteration 39, loss = 0.02692452\n",
      "Iteration 40, loss = 0.02645017\n",
      "Iteration 41, loss = 0.02554780\n",
      "Iteration 42, loss = 0.02506471\n",
      "Iteration 43, loss = 0.02416322\n",
      "Iteration 44, loss = 0.02399160\n",
      "Iteration 45, loss = 0.02314610\n",
      "Iteration 46, loss = 0.02273775\n",
      "Iteration 47, loss = 0.02223794\n",
      "Iteration 48, loss = 0.02185287\n",
      "Iteration 49, loss = 0.02184632\n",
      "Iteration 50, loss = 0.02092397\n",
      "Iteration 51, loss = 0.02053215\n",
      "Iteration 52, loss = 0.02115215\n",
      "Iteration 53, loss = 0.02046633\n",
      "Iteration 54, loss = 0.01978139\n",
      "Iteration 55, loss = 0.01926098\n",
      "Iteration 56, loss = 0.01904627\n",
      "Iteration 57, loss = 0.01861416\n",
      "Iteration 58, loss = 0.01851721\n",
      "Iteration 59, loss = 0.01816993\n",
      "Iteration 60, loss = 0.01846985\n",
      "Iteration 61, loss = 0.01789865\n",
      "Iteration 62, loss = 0.01752445\n",
      "Iteration 63, loss = 0.01700430\n",
      "Iteration 64, loss = 0.01677644\n",
      "Iteration 65, loss = 0.01641027\n",
      "Iteration 66, loss = 0.01611383\n",
      "Iteration 67, loss = 0.01634028\n",
      "Iteration 68, loss = 0.01578516\n",
      "Iteration 69, loss = 0.01592295\n",
      "Iteration 70, loss = 0.01531160\n",
      "Iteration 71, loss = 0.01539495\n",
      "Iteration 72, loss = 0.01579010\n",
      "Iteration 73, loss = 0.01520316\n",
      "Iteration 74, loss = 0.01501597\n",
      "Iteration 75, loss = 0.01509617\n",
      "Iteration 76, loss = 0.01442856\n",
      "Iteration 77, loss = 0.01421960\n",
      "Iteration 78, loss = 0.01423354\n",
      "Iteration 79, loss = 0.01407586\n",
      "Iteration 80, loss = 0.01430957\n",
      "Iteration 81, loss = 0.01433416\n",
      "Iteration 82, loss = 0.01506574\n",
      "Iteration 83, loss = 0.01480361\n",
      "Iteration 84, loss = 0.01399845\n",
      "Iteration 85, loss = 0.01386312\n",
      "Iteration 86, loss = 0.01306009\n",
      "Iteration 87, loss = 0.01293318\n",
      "Iteration 88, loss = 0.01266423\n",
      "Iteration 89, loss = 0.01263386\n",
      "Iteration 90, loss = 0.01236248\n",
      "Iteration 91, loss = 0.01279058\n",
      "Iteration 92, loss = 0.01224391\n",
      "Iteration 93, loss = 0.01214460\n",
      "Iteration 94, loss = 0.01253133\n",
      "Iteration 95, loss = 0.01200782\n",
      "Iteration 96, loss = 0.01187654\n",
      "Iteration 97, loss = 0.01163360\n",
      "Iteration 98, loss = 0.01168933\n",
      "Iteration 99, loss = 0.01150504\n",
      "Iteration 100, loss = 0.01188385\n",
      "Iteration 101, loss = 0.01166471\n",
      "Iteration 102, loss = 0.01133163\n",
      "Iteration 103, loss = 0.01114899\n",
      "Iteration 104, loss = 0.01115328\n",
      "Iteration 105, loss = 0.01118333\n",
      "Iteration 106, loss = 0.01103177\n",
      "Iteration 107, loss = 0.01077193\n",
      "Iteration 108, loss = 0.01074541\n",
      "Iteration 109, loss = 0.01071674\n",
      "Iteration 110, loss = 0.01055016\n",
      "Iteration 111, loss = 0.01122665\n",
      "Iteration 112, loss = 0.01089649\n",
      "Iteration 113, loss = 0.01035322\n",
      "Iteration 114, loss = 0.01057514\n",
      "Iteration 115, loss = 0.01130253\n",
      "Iteration 116, loss = 0.01086528\n",
      "Iteration 117, loss = 0.01060939\n",
      "Iteration 118, loss = 0.01040359\n",
      "Iteration 119, loss = 0.01025725\n",
      "Iteration 120, loss = 0.01026123\n",
      "Iteration 121, loss = 0.00998945\n",
      "Iteration 122, loss = 0.00977293\n",
      "Iteration 123, loss = 0.00974137\n",
      "Iteration 124, loss = 0.00988199\n",
      "Iteration 125, loss = 0.00954695\n",
      "Iteration 126, loss = 0.00952185\n",
      "Iteration 127, loss = 0.00983691\n",
      "Iteration 128, loss = 0.00986736\n",
      "Iteration 129, loss = 0.00948741\n",
      "Iteration 130, loss = 0.00929927\n",
      "Iteration 131, loss = 0.00916904\n",
      "Iteration 132, loss = 0.00972862\n",
      "Iteration 133, loss = 0.00949249\n",
      "Iteration 134, loss = 0.00901108\n",
      "Iteration 135, loss = 0.00918658\n",
      "Iteration 136, loss = 0.00894473\n",
      "Iteration 137, loss = 0.00914401\n",
      "Iteration 138, loss = 0.00931160\n",
      "Iteration 139, loss = 0.00915822\n",
      "Iteration 140, loss = 0.00891404\n",
      "Iteration 141, loss = 0.00862171\n",
      "Iteration 142, loss = 0.00891759\n",
      "Iteration 143, loss = 0.00925428\n",
      "Iteration 144, loss = 0.00885963\n",
      "Iteration 145, loss = 0.00921799\n",
      "Iteration 146, loss = 0.00855256\n",
      "Iteration 147, loss = 0.00868143\n",
      "Iteration 148, loss = 0.00861421\n",
      "Iteration 149, loss = 0.00845179\n",
      "Iteration 150, loss = 0.00880248\n",
      "Iteration 151, loss = 0.00842310\n",
      "Iteration 152, loss = 0.00816912\n",
      "Iteration 153, loss = 0.00847192\n",
      "Iteration 154, loss = 0.00802279\n",
      "Iteration 155, loss = 0.00843087\n",
      "Iteration 156, loss = 0.00812029\n",
      "Iteration 157, loss = 0.00808532\n",
      "Iteration 158, loss = 0.00788489\n",
      "Iteration 159, loss = 0.00781348\n",
      "Iteration 160, loss = 0.00785927\n",
      "Iteration 161, loss = 0.00815287\n",
      "Iteration 162, loss = 0.00795081\n",
      "Iteration 163, loss = 0.00771197\n",
      "Iteration 164, loss = 0.00767496\n",
      "Iteration 165, loss = 0.00775757\n",
      "Iteration 166, loss = 0.00745166\n",
      "Iteration 167, loss = 0.00738688\n",
      "Iteration 168, loss = 0.00743771\n",
      "Iteration 169, loss = 0.00771323\n",
      "Iteration 170, loss = 0.00756054\n",
      "Iteration 171, loss = 0.00778142\n",
      "Iteration 172, loss = 0.00737084\n",
      "Iteration 173, loss = 0.00746183\n",
      "Iteration 174, loss = 0.00759359\n",
      "Iteration 175, loss = 0.00818950\n",
      "Iteration 176, loss = 0.00757311\n",
      "Iteration 177, loss = 0.00727010\n",
      "Iteration 178, loss = 0.00749355\n",
      "Iteration 179, loss = 0.00742541\n",
      "Iteration 180, loss = 0.00708622\n",
      "Iteration 181, loss = 0.00704732\n",
      "Iteration 182, loss = 0.00760710\n",
      "Iteration 183, loss = 0.00742631\n",
      "Iteration 184, loss = 0.00722722\n",
      "Iteration 185, loss = 0.00672885\n",
      "Iteration 186, loss = 0.00692224\n",
      "Iteration 187, loss = 0.00657622\n",
      "Iteration 188, loss = 0.00682319\n",
      "Iteration 189, loss = 0.00667956\n",
      "Iteration 190, loss = 0.00689037\n",
      "Iteration 191, loss = 0.00685572\n",
      "Iteration 192, loss = 0.00651764\n",
      "Iteration 193, loss = 0.00645696\n",
      "Iteration 194, loss = 0.00643828\n",
      "Iteration 195, loss = 0.00652472\n",
      "Iteration 196, loss = 0.00645906\n",
      "Iteration 197, loss = 0.00630741\n",
      "Iteration 198, loss = 0.00635483\n",
      "Iteration 199, loss = 0.00630250\n",
      "Iteration 200, loss = 0.00644467\n",
      "Iteration 201, loss = 0.00624932\n",
      "Iteration 202, loss = 0.00657175\n",
      "Iteration 203, loss = 0.00658062\n",
      "Iteration 204, loss = 0.00625252\n",
      "Iteration 205, loss = 0.00626886\n",
      "Iteration 206, loss = 0.00616763\n",
      "Iteration 207, loss = 0.00606804\n",
      "Iteration 208, loss = 0.00619510\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.40014630\n",
      "Iteration 2, loss = 0.28913371\n",
      "Iteration 3, loss = 0.23766609\n",
      "Iteration 4, loss = 0.20222977\n",
      "Iteration 5, loss = 0.17445815\n",
      "Iteration 6, loss = 0.14947353\n",
      "Iteration 7, loss = 0.12975155\n",
      "Iteration 8, loss = 0.11151645\n",
      "Iteration 9, loss = 0.09777469\n",
      "Iteration 10, loss = 0.08605908\n",
      "Iteration 11, loss = 0.07870784\n",
      "Iteration 12, loss = 0.07168201\n",
      "Iteration 13, loss = 0.06670603\n",
      "Iteration 14, loss = 0.06240145\n",
      "Iteration 15, loss = 0.06025085\n",
      "Iteration 16, loss = 0.05665067\n",
      "Iteration 17, loss = 0.05549809\n",
      "Iteration 18, loss = 0.05225649\n",
      "Iteration 19, loss = 0.05082000\n",
      "Iteration 20, loss = 0.04917935\n",
      "Iteration 21, loss = 0.04709586\n",
      "Iteration 22, loss = 0.04536512\n",
      "Iteration 23, loss = 0.04443915\n",
      "Iteration 24, loss = 0.04239959\n",
      "Iteration 25, loss = 0.04059913\n",
      "Iteration 26, loss = 0.03985264\n",
      "Iteration 27, loss = 0.03859573\n",
      "Iteration 28, loss = 0.03739840\n",
      "Iteration 29, loss = 0.03601978\n",
      "Iteration 30, loss = 0.03461756\n",
      "Iteration 31, loss = 0.03417238\n",
      "Iteration 32, loss = 0.03339091\n",
      "Iteration 33, loss = 0.03235180\n",
      "Iteration 34, loss = 0.03095052\n",
      "Iteration 35, loss = 0.03030973\n",
      "Iteration 36, loss = 0.02947775\n",
      "Iteration 37, loss = 0.02904426\n",
      "Iteration 38, loss = 0.02766008\n",
      "Iteration 39, loss = 0.02692452\n",
      "Iteration 40, loss = 0.02645017\n",
      "Iteration 41, loss = 0.02554780\n",
      "Iteration 42, loss = 0.02506471\n",
      "Iteration 43, loss = 0.02416322\n",
      "Iteration 44, loss = 0.02399160\n",
      "Iteration 45, loss = 0.02314610\n",
      "Iteration 46, loss = 0.02273775\n",
      "Iteration 47, loss = 0.02223794\n",
      "Iteration 48, loss = 0.02185287\n",
      "Iteration 49, loss = 0.02184632\n",
      "Iteration 50, loss = 0.02092397\n",
      "Iteration 51, loss = 0.02053215\n",
      "Iteration 52, loss = 0.02115215\n",
      "Iteration 53, loss = 0.02046633\n",
      "Iteration 54, loss = 0.01978139\n",
      "Iteration 55, loss = 0.01926098\n",
      "Iteration 56, loss = 0.01904627\n",
      "Iteration 57, loss = 0.01861416\n",
      "Iteration 58, loss = 0.01851721\n",
      "Iteration 59, loss = 0.01816993\n",
      "Iteration 60, loss = 0.01846985\n",
      "Iteration 61, loss = 0.01789865\n",
      "Iteration 62, loss = 0.01752445\n",
      "Iteration 63, loss = 0.01700430\n",
      "Iteration 64, loss = 0.01677644\n",
      "Iteration 65, loss = 0.01641027\n",
      "Iteration 66, loss = 0.01611383\n",
      "Iteration 67, loss = 0.01634028\n",
      "Iteration 68, loss = 0.01578516\n",
      "Iteration 69, loss = 0.01592295\n",
      "Iteration 70, loss = 0.01531160\n",
      "Iteration 71, loss = 0.01539495\n",
      "Iteration 72, loss = 0.01579010\n",
      "Iteration 73, loss = 0.01520316\n",
      "Iteration 74, loss = 0.01501597\n",
      "Iteration 75, loss = 0.01509617\n",
      "Iteration 76, loss = 0.01442856\n",
      "Iteration 77, loss = 0.01421960\n",
      "Iteration 78, loss = 0.01423354\n",
      "Iteration 79, loss = 0.01407586\n",
      "Iteration 80, loss = 0.01430957\n",
      "Iteration 81, loss = 0.01433416\n",
      "Iteration 82, loss = 0.01506574\n",
      "Iteration 83, loss = 0.01480361\n",
      "Iteration 84, loss = 0.01399845\n",
      "Iteration 85, loss = 0.01386312\n",
      "Iteration 86, loss = 0.01306009\n",
      "Iteration 87, loss = 0.01293318\n",
      "Iteration 88, loss = 0.01266423\n",
      "Iteration 89, loss = 0.01263386\n",
      "Iteration 90, loss = 0.01236248\n",
      "Iteration 91, loss = 0.01279058\n",
      "Iteration 92, loss = 0.01224391\n",
      "Iteration 93, loss = 0.01214460\n",
      "Iteration 94, loss = 0.01253133\n",
      "Iteration 95, loss = 0.01200782\n",
      "Iteration 96, loss = 0.01187654\n",
      "Iteration 97, loss = 0.01163360\n",
      "Iteration 98, loss = 0.01168933\n",
      "Iteration 99, loss = 0.01150504\n",
      "Iteration 100, loss = 0.01188385\n",
      "Iteration 101, loss = 0.01166471\n",
      "Iteration 102, loss = 0.01133163\n",
      "Iteration 103, loss = 0.01114899\n",
      "Iteration 104, loss = 0.01115328\n",
      "Iteration 105, loss = 0.01118333\n",
      "Iteration 106, loss = 0.01103177\n",
      "Iteration 107, loss = 0.01077193\n",
      "Iteration 108, loss = 0.01074541\n",
      "Iteration 109, loss = 0.01071674\n",
      "Iteration 110, loss = 0.01055016\n",
      "Iteration 111, loss = 0.01122665\n",
      "Iteration 112, loss = 0.01089649\n",
      "Iteration 113, loss = 0.01035322\n",
      "Iteration 114, loss = 0.01057514\n",
      "Iteration 115, loss = 0.01130253\n",
      "Iteration 116, loss = 0.01086528\n",
      "Iteration 117, loss = 0.01060939\n",
      "Iteration 118, loss = 0.01040359\n",
      "Iteration 119, loss = 0.01025725\n",
      "Iteration 120, loss = 0.01026123\n",
      "Iteration 121, loss = 0.00998945\n",
      "Iteration 122, loss = 0.00977293\n",
      "Iteration 123, loss = 0.00974137\n",
      "Iteration 124, loss = 0.00988199\n",
      "Iteration 125, loss = 0.00954695\n",
      "Iteration 126, loss = 0.00952185\n",
      "Iteration 127, loss = 0.00983691\n",
      "Iteration 128, loss = 0.00986736\n",
      "Iteration 129, loss = 0.00948741\n",
      "Iteration 130, loss = 0.00929927\n",
      "Iteration 131, loss = 0.00916904\n",
      "Iteration 132, loss = 0.00972862\n",
      "Iteration 133, loss = 0.00949249\n",
      "Iteration 134, loss = 0.00901108\n",
      "Iteration 135, loss = 0.00918658\n",
      "Iteration 136, loss = 0.00894473\n",
      "Iteration 137, loss = 0.00914401\n",
      "Iteration 138, loss = 0.00931160\n",
      "Iteration 139, loss = 0.00915822\n",
      "Iteration 140, loss = 0.00891404\n",
      "Iteration 141, loss = 0.00862171\n",
      "Iteration 142, loss = 0.00891759\n",
      "Iteration 143, loss = 0.00925428\n",
      "Iteration 144, loss = 0.00885963\n",
      "Iteration 145, loss = 0.00921799\n",
      "Iteration 146, loss = 0.00855256\n",
      "Iteration 147, loss = 0.00868143\n",
      "Iteration 148, loss = 0.00861421\n",
      "Iteration 149, loss = 0.00845179\n",
      "Iteration 150, loss = 0.00880248\n",
      "Iteration 151, loss = 0.00842310\n",
      "Iteration 152, loss = 0.00816912\n",
      "Iteration 153, loss = 0.00847192\n",
      "Iteration 154, loss = 0.00802279\n",
      "Iteration 155, loss = 0.00843087\n",
      "Iteration 156, loss = 0.00812029\n",
      "Iteration 157, loss = 0.00808532\n",
      "Iteration 158, loss = 0.00788489\n",
      "Iteration 159, loss = 0.00781348\n",
      "Iteration 160, loss = 0.00785927\n",
      "Iteration 161, loss = 0.00815287\n",
      "Iteration 162, loss = 0.00795081\n",
      "Iteration 163, loss = 0.00771197\n",
      "Iteration 164, loss = 0.00767496\n",
      "Iteration 165, loss = 0.00775757\n",
      "Iteration 166, loss = 0.00745166\n",
      "Iteration 167, loss = 0.00738688\n",
      "Iteration 168, loss = 0.00743771\n",
      "Iteration 169, loss = 0.00771323\n",
      "Iteration 170, loss = 0.00756054\n",
      "Iteration 171, loss = 0.00778142\n",
      "Iteration 172, loss = 0.00737084\n",
      "Iteration 173, loss = 0.00746183\n",
      "Iteration 174, loss = 0.00759359\n",
      "Iteration 175, loss = 0.00818950\n",
      "Iteration 176, loss = 0.00757311\n",
      "Iteration 177, loss = 0.00727010\n",
      "Iteration 178, loss = 0.00749355\n",
      "Iteration 179, loss = 0.00742541\n",
      "Iteration 180, loss = 0.00708622\n",
      "Iteration 181, loss = 0.00704732\n",
      "Iteration 182, loss = 0.00760710\n",
      "Iteration 183, loss = 0.00742631\n",
      "Iteration 184, loss = 0.00722722\n",
      "Iteration 185, loss = 0.00672885\n",
      "Iteration 186, loss = 0.00692224\n",
      "Iteration 187, loss = 0.00657622\n",
      "Iteration 188, loss = 0.00682319\n",
      "Iteration 189, loss = 0.00667956\n",
      "Iteration 190, loss = 0.00689037\n",
      "Iteration 191, loss = 0.00685572\n",
      "Iteration 192, loss = 0.00651764\n",
      "Iteration 193, loss = 0.00645696\n",
      "Iteration 194, loss = 0.00643828\n",
      "Iteration 195, loss = 0.00652472\n",
      "Iteration 196, loss = 0.00645906\n",
      "Iteration 197, loss = 0.00630741\n",
      "Iteration 198, loss = 0.00635483\n",
      "Iteration 199, loss = 0.00630250\n",
      "Iteration 200, loss = 0.00644467\n",
      "Iteration 201, loss = 0.00624932\n",
      "Iteration 202, loss = 0.00657175\n",
      "Iteration 203, loss = 0.00658062\n",
      "Iteration 204, loss = 0.00625252\n",
      "Iteration 205, loss = 0.00626886\n",
      "Iteration 206, loss = 0.00616763\n",
      "Iteration 207, loss = 0.00606804\n",
      "Iteration 208, loss = 0.00619510\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.40992880\n",
      "Iteration 2, loss = 0.28851566\n",
      "Iteration 3, loss = 0.24541757\n",
      "Iteration 4, loss = 0.21650288\n",
      "Iteration 5, loss = 0.19062227\n",
      "Iteration 6, loss = 0.16682182\n",
      "Iteration 7, loss = 0.14989429\n",
      "Iteration 8, loss = 0.13118527\n",
      "Iteration 9, loss = 0.11834330\n",
      "Iteration 10, loss = 0.10872712\n",
      "Iteration 11, loss = 0.09808058\n",
      "Iteration 12, loss = 0.09178890\n",
      "Iteration 13, loss = 0.08473524\n",
      "Iteration 14, loss = 0.08102660\n",
      "Iteration 15, loss = 0.07441874\n",
      "Iteration 16, loss = 0.07011872\n",
      "Iteration 17, loss = 0.06740851\n",
      "Iteration 18, loss = 0.06339350\n",
      "Iteration 19, loss = 0.06038625\n",
      "Iteration 20, loss = 0.05838224\n",
      "Iteration 21, loss = 0.05480800\n",
      "Iteration 22, loss = 0.05327996\n",
      "Iteration 23, loss = 0.05087763\n",
      "Iteration 24, loss = 0.04867136\n",
      "Iteration 25, loss = 0.04611010\n",
      "Iteration 26, loss = 0.04485110\n",
      "Iteration 27, loss = 0.04268007\n",
      "Iteration 28, loss = 0.04154237\n",
      "Iteration 29, loss = 0.04006837\n",
      "Iteration 30, loss = 0.03831015\n",
      "Iteration 31, loss = 0.03677117\n",
      "Iteration 32, loss = 0.03561615\n",
      "Iteration 33, loss = 0.03524301\n",
      "Iteration 34, loss = 0.03360456\n",
      "Iteration 35, loss = 0.03222988\n",
      "Iteration 36, loss = 0.03259980\n",
      "Iteration 37, loss = 0.03030506\n",
      "Iteration 38, loss = 0.02979340\n",
      "Iteration 39, loss = 0.02965642\n",
      "Iteration 40, loss = 0.02838446\n",
      "Iteration 41, loss = 0.02712146\n",
      "Iteration 42, loss = 0.02645296\n",
      "Iteration 43, loss = 0.02597034\n",
      "Iteration 44, loss = 0.02469986\n",
      "Iteration 45, loss = 0.02417946\n",
      "Iteration 46, loss = 0.02350990\n",
      "Iteration 47, loss = 0.02279630\n",
      "Iteration 48, loss = 0.02331554\n",
      "Iteration 49, loss = 0.02165729\n",
      "Iteration 50, loss = 0.02138570\n",
      "Iteration 51, loss = 0.02132142\n",
      "Iteration 52, loss = 0.02064784\n",
      "Iteration 53, loss = 0.02043866\n",
      "Iteration 54, loss = 0.01925061\n",
      "Iteration 55, loss = 0.01958601\n",
      "Iteration 56, loss = 0.01902521\n",
      "Iteration 57, loss = 0.01848230\n",
      "Iteration 58, loss = 0.01794403\n",
      "Iteration 59, loss = 0.01796646\n",
      "Iteration 60, loss = 0.01741784\n",
      "Iteration 61, loss = 0.01702542\n",
      "Iteration 62, loss = 0.01673782\n",
      "Iteration 63, loss = 0.01651773\n",
      "Iteration 64, loss = 0.01654825\n",
      "Iteration 65, loss = 0.01647521\n",
      "Iteration 66, loss = 0.01610199\n",
      "Iteration 67, loss = 0.01524681\n",
      "Iteration 68, loss = 0.01507602\n",
      "Iteration 69, loss = 0.01470966\n",
      "Iteration 70, loss = 0.01461961\n",
      "Iteration 71, loss = 0.01455705\n",
      "Iteration 72, loss = 0.01437905\n",
      "Iteration 73, loss = 0.01412652\n",
      "Iteration 74, loss = 0.01366958\n",
      "Iteration 75, loss = 0.01370261\n",
      "Iteration 76, loss = 0.01375356\n",
      "Iteration 77, loss = 0.01369733\n",
      "Iteration 78, loss = 0.01329613\n",
      "Iteration 79, loss = 0.01277655\n",
      "Iteration 80, loss = 0.01278128\n",
      "Iteration 81, loss = 0.01253976\n",
      "Iteration 82, loss = 0.01297465\n",
      "Iteration 83, loss = 0.01226411\n",
      "Iteration 84, loss = 0.01243391\n",
      "Iteration 85, loss = 0.01195130\n",
      "Iteration 86, loss = 0.01162314\n",
      "Iteration 87, loss = 0.01151416\n",
      "Iteration 88, loss = 0.01140624\n",
      "Iteration 89, loss = 0.01135252\n",
      "Iteration 90, loss = 0.01121684\n",
      "Iteration 91, loss = 0.01150089\n",
      "Iteration 92, loss = 0.01133755\n",
      "Iteration 93, loss = 0.01098687\n",
      "Iteration 94, loss = 0.01065537\n",
      "Iteration 95, loss = 0.01088716\n",
      "Iteration 96, loss = 0.01093731\n",
      "Iteration 97, loss = 0.01127245\n",
      "Iteration 98, loss = 0.01100563\n",
      "Iteration 99, loss = 0.01093726\n",
      "Iteration 100, loss = 0.01043625\n",
      "Iteration 101, loss = 0.01040324\n",
      "Iteration 102, loss = 0.01012904\n",
      "Iteration 103, loss = 0.00995757\n",
      "Iteration 104, loss = 0.01003337\n",
      "Iteration 105, loss = 0.00962806\n",
      "Iteration 106, loss = 0.01002541\n",
      "Iteration 107, loss = 0.00983456\n",
      "Iteration 108, loss = 0.00951512\n",
      "Iteration 109, loss = 0.00959661\n",
      "Iteration 110, loss = 0.00957471\n",
      "Iteration 111, loss = 0.00993260\n",
      "Iteration 112, loss = 0.00976509\n",
      "Iteration 113, loss = 0.00992320\n",
      "Iteration 114, loss = 0.00985165\n",
      "Iteration 115, loss = 0.00954180\n",
      "Iteration 116, loss = 0.00954298\n",
      "Iteration 117, loss = 0.00897809\n",
      "Iteration 118, loss = 0.00878916\n",
      "Iteration 119, loss = 0.00884532\n",
      "Iteration 120, loss = 0.00849309\n",
      "Iteration 121, loss = 0.00850118\n",
      "Iteration 122, loss = 0.00842543\n",
      "Iteration 123, loss = 0.00850411\n",
      "Iteration 124, loss = 0.00850287\n",
      "Iteration 125, loss = 0.00847502\n",
      "Iteration 126, loss = 0.00835174\n",
      "Iteration 127, loss = 0.00849722\n",
      "Iteration 128, loss = 0.00885578\n",
      "Iteration 129, loss = 0.00916012\n",
      "Iteration 130, loss = 0.00927269\n",
      "Iteration 131, loss = 0.00866062\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.40992880\n",
      "Iteration 2, loss = 0.28851566\n",
      "Iteration 3, loss = 0.24541757\n",
      "Iteration 4, loss = 0.21650288\n",
      "Iteration 5, loss = 0.19062227\n",
      "Iteration 6, loss = 0.16682182\n",
      "Iteration 7, loss = 0.14989429\n",
      "Iteration 8, loss = 0.13118527\n",
      "Iteration 9, loss = 0.11834330\n",
      "Iteration 10, loss = 0.10872712\n",
      "Iteration 11, loss = 0.09808058\n",
      "Iteration 12, loss = 0.09178890\n",
      "Iteration 13, loss = 0.08473524\n",
      "Iteration 14, loss = 0.08102660\n",
      "Iteration 15, loss = 0.07441874\n",
      "Iteration 16, loss = 0.07011872\n",
      "Iteration 17, loss = 0.06740851\n",
      "Iteration 18, loss = 0.06339350\n",
      "Iteration 19, loss = 0.06038625\n",
      "Iteration 20, loss = 0.05838224\n",
      "Iteration 21, loss = 0.05480800\n",
      "Iteration 22, loss = 0.05327996\n",
      "Iteration 23, loss = 0.05087763\n",
      "Iteration 24, loss = 0.04867136\n",
      "Iteration 25, loss = 0.04611010\n",
      "Iteration 26, loss = 0.04485110\n",
      "Iteration 27, loss = 0.04268007\n",
      "Iteration 28, loss = 0.04154237\n",
      "Iteration 29, loss = 0.04006837\n",
      "Iteration 30, loss = 0.03831015\n",
      "Iteration 31, loss = 0.03677117\n",
      "Iteration 32, loss = 0.03561615\n",
      "Iteration 33, loss = 0.03524301\n",
      "Iteration 34, loss = 0.03360456\n",
      "Iteration 35, loss = 0.03222988\n",
      "Iteration 36, loss = 0.03259980\n",
      "Iteration 37, loss = 0.03030506\n",
      "Iteration 38, loss = 0.02979340\n",
      "Iteration 39, loss = 0.02965642\n",
      "Iteration 40, loss = 0.02838446\n",
      "Iteration 41, loss = 0.02712146\n",
      "Iteration 42, loss = 0.02645296\n",
      "Iteration 43, loss = 0.02597034\n",
      "Iteration 44, loss = 0.02469986\n",
      "Iteration 45, loss = 0.02417946\n",
      "Iteration 46, loss = 0.02350990\n",
      "Iteration 47, loss = 0.02279630\n",
      "Iteration 48, loss = 0.02331554\n",
      "Iteration 49, loss = 0.02165729\n",
      "Iteration 50, loss = 0.02138570\n",
      "Iteration 51, loss = 0.02132142\n",
      "Iteration 52, loss = 0.02064784\n",
      "Iteration 53, loss = 0.02043866\n",
      "Iteration 54, loss = 0.01925061\n",
      "Iteration 55, loss = 0.01958601\n",
      "Iteration 56, loss = 0.01902521\n",
      "Iteration 57, loss = 0.01848230\n",
      "Iteration 58, loss = 0.01794403\n",
      "Iteration 59, loss = 0.01796646\n",
      "Iteration 60, loss = 0.01741784\n",
      "Iteration 61, loss = 0.01702542\n",
      "Iteration 62, loss = 0.01673782\n",
      "Iteration 63, loss = 0.01651773\n",
      "Iteration 64, loss = 0.01654825\n",
      "Iteration 65, loss = 0.01647521\n",
      "Iteration 66, loss = 0.01610199\n",
      "Iteration 67, loss = 0.01524681\n",
      "Iteration 68, loss = 0.01507602\n",
      "Iteration 69, loss = 0.01470966\n",
      "Iteration 70, loss = 0.01461961\n",
      "Iteration 71, loss = 0.01455705\n",
      "Iteration 72, loss = 0.01437905\n",
      "Iteration 73, loss = 0.01412652\n",
      "Iteration 74, loss = 0.01366958\n",
      "Iteration 75, loss = 0.01370261\n",
      "Iteration 76, loss = 0.01375356\n",
      "Iteration 77, loss = 0.01369733\n",
      "Iteration 78, loss = 0.01329613\n",
      "Iteration 79, loss = 0.01277655\n",
      "Iteration 80, loss = 0.01278128\n",
      "Iteration 81, loss = 0.01253976\n",
      "Iteration 82, loss = 0.01297465\n",
      "Iteration 83, loss = 0.01226411\n",
      "Iteration 84, loss = 0.01243391\n"
     ]
    },
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching getindex(::DecisionTree.RandomForestRegressor, ::Symbol)",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching getindex(::DecisionTree.RandomForestRegressor, ::Symbol)\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/git/P2H_CapacityExpansion/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X14sdnNjb2RlLXJlbW90ZQ==.jl:41"
     ]
    }
   ],
   "source": [
    "step_size = 100\n",
    "df_full = DataFrame(Method=String[],  Value=Float64[], Size=Int[]);\n",
    "n_runs = 3\n",
    "\n",
    "for n ∈ 100:step_size:size(df_raw)[1] \n",
    "    \n",
    "    # clustering the entire subspace to identify n samples\n",
    "    idx = kmeans_subset(select(df_raw, Not(:Cost)), n)\n",
    "    df = df_raw[idx,:]\n",
    "\n",
    "    ### split the data into test and training ###\n",
    "    X_train, y_train, X_test, y_test = P2H_CapacityExpansion.partitionTrainTest(df, :Cost, 0.8)\n",
    "\n",
    "    ### scale the data ###\n",
    "    X_train_scaled, μX, σX  = P2H_CapacityExpansion.scaling(X_train)\n",
    "    X_test_scaled = (X_test .- μX) ./ σX\n",
    "    y_train_scaled, μy, σy  = P2H_CapacityExpansion.scaling(y_train)\n",
    "    \n",
    "    # remove np.nan #\n",
    "    for i in eachindex(X_test_scaled)\n",
    "        if isnan(X_test_scaled[i])\n",
    "            X_test_scaled[i] = 0.0\n",
    "        end\n",
    "    end\n",
    "\n",
    "    ### train ML model and compute R2 ### \n",
    "    for (name, fun) ∈ models\n",
    "        # determine average for non-deterministic models\n",
    "        iter = name ∈ stochastic_models ? n_runs : 1\n",
    "        \n",
    "        r2 = 0\n",
    "        for k ∈ 1:iter\n",
    "            sg = fun(X_train_scaled, y_train_scaled, X_test_scaled)\n",
    "            ŷ_rescaled = sg.prediction .* σy .+ μy\n",
    "            r2 += P2H_CapacityExpansion.r2_score(y_test, ŷ_rescaled)\n",
    "\n",
    "            ### save the model ###\n",
    "            if n == last(100:step_size:size(df_raw)[1]) && k == iter\n",
    "                model = sg.model\n",
    "                # assume model is trained with ScikitLearn.jl\n",
    "                joblib.dump(model[:model], \"$(dir)$(name).pkl\")\n",
    "            end\n",
    "        end\n",
    "\n",
    "        ### add to the df ### \n",
    "        push!(df_full, (Method = name, Value = r2/iter, Size = size(X_train)[1]))\n",
    "    end \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "editable": false,
        "responsive": true,
        "scrollZoom": true,
        "showLink": false,
        "staticPlot": false
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "SVR",
         "type": "scatter",
         "x": [
          1040,
          960,
          880,
          800,
          720,
          640,
          560,
          480,
          400,
          320,
          240,
          160,
          80
         ],
         "y": [
          0.8160167860894751,
          0.8247663168562129,
          0.8081593890919172,
          0.76823913251253,
          0.6808623062569504,
          0.9029837138623951,
          0.8282269522151673,
          0.9315108300294325,
          0.8405960674800167,
          0.8391153867527228,
          0.8417935407993532,
          0.830400415443093,
          0.38635675480124254
         ]
        },
        {
         "mode": "lines+markers",
         "name": "RandomForest",
         "type": "scatter",
         "x": [
          1040,
          960,
          880,
          800,
          720,
          640,
          560,
          480,
          400,
          320,
          240,
          160,
          80
         ],
         "y": [
          0.7800504234604334,
          0.8020543095440783,
          0.7126656811834016,
          0.7941879904563341,
          0.7802949622603794,
          0.8380302695223842,
          0.8439468505380945,
          0.8600438664463016,
          0.8156674808384938,
          0.8244762451251747,
          0.7052643639474491,
          0.6620291146892634,
          0.2722096447599147
         ]
        },
        {
         "mode": "lines+markers",
         "name": "NeuralNetwork",
         "type": "scatter",
         "x": [
          1040,
          960,
          880,
          800,
          720,
          640,
          560,
          480,
          400,
          320,
          240,
          160,
          80
         ],
         "y": [
          0.8975308436669005,
          0.9638284120727647,
          0.8906213829854822,
          0.963011658149925,
          0.9309486463307994,
          0.8659708430380485,
          0.9712744187516499,
          0.913318116273459,
          0.94464845691769,
          0.9443271871714275,
          0.8632433882236102,
          0.845860734709453,
          0.8455530592822363
         ]
        },
        {
         "mode": "lines+markers",
         "name": "LinearRegression",
         "type": "scatter",
         "x": [
          1040,
          960,
          880,
          800,
          720,
          640,
          560,
          480,
          400,
          320,
          240,
          160,
          80
         ],
         "y": [
          0.6788512222147269,
          0.7340860598507928,
          0.7513126831594139,
          0.7186100586911832,
          0.7543442857547067,
          0.6569347607135796,
          0.6809961517729032,
          0.5756095530727587,
          0.7964185142308671,
          0.8299466930801841,
          0.7350010358395036,
          0.5362631388670117,
          0.5044646484614239
         ]
        },
        {
         "mode": "lines+markers",
         "name": "GaussianProcesses",
         "type": "scatter",
         "x": [
          1040,
          960,
          880,
          800,
          720,
          640,
          560,
          480,
          400,
          320,
          240,
          160,
          80
         ],
         "y": [
          0.882361392614267,
          0.9057838649013339,
          0.8840683867825072,
          0.7843835746371617,
          0.7737530988783151,
          0.9249393028354236,
          0.8172223338367981,
          0.9329863416832564,
          0.8335228398624763,
          0.8166945901406453,
          0.7497992344824158,
          0.9732990386982779,
          0.4244906298296157
         ]
        },
        {
         "mode": "lines+markers",
         "name": "DecisionTree",
         "type": "scatter",
         "x": [
          1040,
          960,
          880,
          800,
          720,
          640,
          560,
          480,
          400,
          320,
          240,
          160,
          80
         ],
         "y": [
          0.7929459633446987,
          0.6446165413839606,
          0.5659500443847109,
          0.7482800129492816,
          0.7616532400593229,
          0.7037333839127003,
          0.8542379054609436,
          0.8634209180821483,
          0.8330945361254648,
          0.8821996293751498,
          0.5586398935569491,
          0.5372592055772417,
          0.5480128381725
         ]
        }
       ],
       "frames": [],
       "layout": {
        "margin": {
         "b": 50,
         "l": 50,
         "r": 50,
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": "Value vs Size by Method",
        "xaxis": {
         "title": {
          "text": "Training Data"
         }
        },
        "yaxis": {
         "title": {
          "text": "Accuracy"
         }
        }
       }
      },
      "image/png": "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",
      "image/svg+xml": [
       "<svg class=\"main-svg\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"700\" height=\"500\" style=\"\" viewBox=\"0 0 700 500\"><rect x=\"0\" y=\"0\" width=\"700\" height=\"500\" style=\"fill: rgb(255, 255, 255); fill-opacity: 1;\"/><defs id=\"defs-067cca\"><g class=\"clips\"><clipPath id=\"clip067ccaxyplot\" class=\"plotclip\"><rect width=\"473\" height=\"387\"/></clipPath><clipPath class=\"axesclip\" id=\"clip067ccax\"><rect x=\"54\" y=\"0\" width=\"473\" height=\"500\"/></clipPath><clipPath class=\"axesclip\" id=\"clip067ccay\"><rect x=\"0\" y=\"60\" width=\"700\" height=\"387\"/></clipPath><clipPath class=\"axesclip\" id=\"clip067ccaxy\"><rect x=\"54\" y=\"60\" width=\"473\" height=\"387\"/></clipPath></g><g class=\"gradients\"/></defs><g class=\"bglayer\"><rect class=\"bg\" x=\"54\" y=\"60\" width=\"473\" height=\"387\" style=\"fill: rgb(229, 236, 246); fill-opacity: 1; stroke-width: 0;\"/></g><g class=\"layer-below\"><g class=\"imagelayer\"/><g class=\"shapelayer\"/></g><g class=\"cartesianlayer\"><g class=\"subplot xy\"><g class=\"layer-subplot\"><g class=\"shapelayer\"/><g class=\"imagelayer\"/></g><g class=\"gridlayer\"><g class=\"x\"><path class=\"xgrid crisp\" transform=\"translate(133.67000000000002,0)\" d=\"M0,60v387\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"xgrid crisp\" transform=\"translate(220.8,0)\" d=\"M0,60v387\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"xgrid crisp\" transform=\"translate(307.91999999999996,0)\" d=\"M0,60v387\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"xgrid crisp\" transform=\"translate(395.05,0)\" d=\"M0,60v387\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"xgrid crisp\" transform=\"translate(482.17,0)\" d=\"M0,60v387\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/></g><g class=\"y\"><path class=\"ygrid crisp\" transform=\"translate(0,410.39)\" d=\"M54,0h473\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"ygrid crisp\" transform=\"translate(0,361.78)\" d=\"M54,0h473\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"ygrid crisp\" transform=\"translate(0,313.16999999999996)\" d=\"M54,0h473\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"ygrid crisp\" transform=\"translate(0,264.56)\" d=\"M54,0h473\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"ygrid crisp\" transform=\"translate(0,215.95)\" d=\"M54,0h473\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"ygrid crisp\" transform=\"translate(0,167.34)\" d=\"M54,0h473\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"ygrid crisp\" transform=\"translate(0,118.72999999999999)\" d=\"M54,0h473\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"ygrid crisp\" transform=\"translate(0,70.12)\" d=\"M54,0h473\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/></g></g><g class=\"zerolinelayer\"/><path class=\"xlines-below\"/><path class=\"ylines-below\"/><g class=\"overlines-below\"/><g class=\"xaxislayer-below\"/><g class=\"yaxislayer-below\"/><g class=\"overaxes-below\"/><g class=\"plot\" transform=\"translate(54,60)\" clip-path=\"url('#clip067ccaxyplot')\"><g class=\"scatterlayer mlayer\"><g class=\"trace scatter tracea38c77\" style=\"stroke-miterlimit: 2; opacity: 1;\"><g class=\"fills\"/><g class=\"errorbars\"/><g class=\"lines\"><path class=\"js-line\" d=\"M445.6,99.56L410.75,95.3L375.9,103.37L341.05,122.78L306.2,165.25L271.35,57.28L236.5,93.62L201.65,43.41L166.8,87.61L131.95,88.33L97.1,87.02L62.25,92.56L27.4,308.41\" style=\"vector-effect: non-scaling-stroke; fill: none; stroke: rgb(99, 110, 250); stroke-opacity: 1; stroke-width: 2px; opacity: 1;\"/></g><g class=\"points\"><path class=\"point\" transform=\"translate(445.6,99.56)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(410.75,95.3)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(375.9,103.37)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(341.05,122.78)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(306.2,165.25)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(271.35,57.28)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(236.5,93.62)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(201.65,43.41)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(166.8,87.61)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(131.95,88.33)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(97.1,87.02)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(62.25,92.56)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(27.4,308.41)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g><g class=\"text\"/></g><g class=\"trace scatter trace28072b\" style=\"stroke-miterlimit: 2; opacity: 1;\"><g class=\"fills\"/><g class=\"errorbars\"/><g class=\"lines\"><path class=\"js-line\" d=\"M445.6,117.04L410.75,106.34L375.9,149.79L341.05,110.17L306.2,116.92L271.35,88.85L236.5,85.98L201.65,78.15L166.8,99.72L131.95,95.44L97.1,153.39L62.25,174.41L27.4,363.9\" style=\"vector-effect: non-scaling-stroke; fill: none; stroke: rgb(239, 85, 59); stroke-opacity: 1; stroke-width: 2px; opacity: 1;\"/></g><g class=\"points\"><path class=\"point\" transform=\"translate(445.6,117.04)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(239, 85, 59); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(410.75,106.34)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(239, 85, 59); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(375.9,149.79)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(239, 85, 59); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(341.05,110.17)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(239, 85, 59); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(306.2,116.92)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(239, 85, 59); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(271.35,88.85)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(239, 85, 59); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(236.5,85.98)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(239, 85, 59); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(201.65,78.15)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(239, 85, 59); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(166.8,99.72)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(239, 85, 59); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(131.95,95.44)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(239, 85, 59); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(97.1,153.39)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(239, 85, 59); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(62.25,174.41)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(239, 85, 59); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(27.4,363.9)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(239, 85, 59); fill-opacity: 1;\"/></g><g class=\"text\"/></g><g class=\"trace scatter trace2af98a\" style=\"stroke-miterlimit: 2; opacity: 1;\"><g class=\"fills\"/><g class=\"errorbars\"/><g class=\"lines\"><path class=\"js-line\" d=\"M445.6,59.93L410.75,27.7L375.9,63.29L341.05,28.1L306.2,43.69L271.35,75.27L236.5,24.08L201.65,52.26L166.8,37.03L131.95,37.18L97.1,76.6L62.25,85.05L27.4,85.2\" style=\"vector-effect: non-scaling-stroke; fill: none; stroke: rgb(0, 204, 150); stroke-opacity: 1; stroke-width: 2px; opacity: 1;\"/></g><g class=\"points\"><path class=\"point\" transform=\"translate(445.6,59.93)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(0, 204, 150); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(410.75,27.7)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(0, 204, 150); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(375.9,63.29)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(0, 204, 150); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(341.05,28.1)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(0, 204, 150); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(306.2,43.69)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(0, 204, 150); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(271.35,75.27)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(0, 204, 150); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(236.5,24.08)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(0, 204, 150); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(201.65,52.26)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(0, 204, 150); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(166.8,37.03)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(0, 204, 150); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(131.95,37.18)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(0, 204, 150); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(97.1,76.6)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(0, 204, 150); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(62.25,85.05)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(0, 204, 150); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(27.4,85.2)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(0, 204, 150); fill-opacity: 1;\"/></g><g class=\"text\"/></g><g class=\"trace scatter trace0641bd\" style=\"stroke-miterlimit: 2; opacity: 1;\"><g class=\"fills\"/><g class=\"errorbars\"/><g class=\"lines\"><path class=\"js-line\" d=\"M445.6,166.23L410.75,139.38L375.9,131.01L341.05,146.9L306.2,129.53L271.35,176.88L236.5,165.19L201.65,216.42L166.8,109.08L131.95,92.78L97.1,138.94L62.25,235.54L27.4,251\" style=\"vector-effect: non-scaling-stroke; fill: none; stroke: rgb(171, 99, 250); stroke-opacity: 1; stroke-width: 2px; opacity: 1;\"/></g><g class=\"points\"><path class=\"point\" transform=\"translate(445.6,166.23)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(171, 99, 250); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(410.75,139.38)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(171, 99, 250); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(375.9,131.01)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(171, 99, 250); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(341.05,146.9)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(171, 99, 250); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(306.2,129.53)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(171, 99, 250); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(271.35,176.88)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(171, 99, 250); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(236.5,165.19)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(171, 99, 250); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(201.65,216.42)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(171, 99, 250); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(166.8,109.08)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(171, 99, 250); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(131.95,92.78)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(171, 99, 250); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(97.1,138.94)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(171, 99, 250); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(62.25,235.54)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(171, 99, 250); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(27.4,251)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(171, 99, 250); fill-opacity: 1;\"/></g><g class=\"text\"/></g><g class=\"trace scatter trace3aeea4\" style=\"stroke-miterlimit: 2; opacity: 1;\"><g class=\"fills\"/><g class=\"errorbars\"/><g class=\"lines\"><path class=\"js-line\" d=\"M445.6,67.3L410.75,55.92L375.9,66.48L341.05,114.93L306.2,120.1L271.35,46.61L236.5,98.97L201.65,42.7L166.8,91.05L131.95,99.23L97.1,131.74L62.25,23.1L27.4,289.88\" style=\"vector-effect: non-scaling-stroke; fill: none; stroke: rgb(255, 161, 90); stroke-opacity: 1; stroke-width: 2px; opacity: 1;\"/></g><g class=\"points\"><path class=\"point\" transform=\"translate(445.6,67.3)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(255, 161, 90); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(410.75,55.92)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(255, 161, 90); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(375.9,66.48)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(255, 161, 90); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(341.05,114.93)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(255, 161, 90); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(306.2,120.1)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(255, 161, 90); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(271.35,46.61)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(255, 161, 90); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(236.5,98.97)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(255, 161, 90); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(201.65,42.7)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(255, 161, 90); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(166.8,91.05)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(255, 161, 90); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(131.95,99.23)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(255, 161, 90); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(97.1,131.74)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(255, 161, 90); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(62.25,23.1)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(255, 161, 90); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(27.4,289.88)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(255, 161, 90); fill-opacity: 1;\"/></g><g class=\"text\"/></g><g class=\"trace scatter trace7b103a\" style=\"stroke-miterlimit: 2; opacity: 1;\"><g class=\"fills\"/><g class=\"errorbars\"/><g class=\"lines\"><path class=\"js-line\" d=\"M445.6,110.77L410.75,182.87L375.9,221.11L341.05,132.48L306.2,125.98L271.35,154.14L236.5,80.98L201.65,76.51L166.8,91.25L131.95,67.38L97.1,224.67L62.25,235.06L27.4,229.83\" style=\"vector-effect: non-scaling-stroke; fill: none; stroke: rgb(25, 211, 243); stroke-opacity: 1; stroke-width: 2px; opacity: 1;\"/></g><g class=\"points\"><path class=\"point\" transform=\"translate(445.6,110.77)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(25, 211, 243); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(410.75,182.87)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(25, 211, 243); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(375.9,221.11)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(25, 211, 243); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(341.05,132.48)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(25, 211, 243); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(306.2,125.98)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(25, 211, 243); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(271.35,154.14)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(25, 211, 243); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(236.5,80.98)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(25, 211, 243); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(201.65,76.51)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(25, 211, 243); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(166.8,91.25)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(25, 211, 243); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(131.95,67.38)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(25, 211, 243); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(97.1,224.67)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(25, 211, 243); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(62.25,235.06)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(25, 211, 243); fill-opacity: 1;\"/><path class=\"point\" transform=\"translate(27.4,229.83)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(25, 211, 243); fill-opacity: 1;\"/></g><g class=\"text\"/></g></g></g><g class=\"overplot\"/><path class=\"xlines-above crisp\" d=\"M0,0\" style=\"fill: none;\"/><path class=\"ylines-above crisp\" d=\"M0,0\" style=\"fill: none;\"/><g class=\"overlines-above\"/><g class=\"xaxislayer-above\"><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"460\" transform=\"translate(133.67000000000002,0)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre;\">200</text></g><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"460\" transform=\"translate(220.8,0)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre;\">400</text></g><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"460\" transform=\"translate(307.91999999999996,0)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre;\">600</text></g><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"460\" transform=\"translate(395.05,0)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre;\">800</text></g><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"460\" transform=\"translate(482.17,0)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre;\">1000</text></g></g><g class=\"yaxislayer-above\"><g class=\"ytick\"><text text-anchor=\"end\" x=\"53\" y=\"4.199999999999999\" transform=\"translate(0,410.39)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre;\">0.3</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"53\" y=\"4.199999999999999\" transform=\"translate(0,361.78)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre;\">0.4</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"53\" y=\"4.199999999999999\" transform=\"translate(0,313.16999999999996)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre;\">0.5</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"53\" y=\"4.199999999999999\" transform=\"translate(0,264.56)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre;\">0.6</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"53\" y=\"4.199999999999999\" transform=\"translate(0,215.95)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre;\">0.7</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"53\" y=\"4.199999999999999\" transform=\"translate(0,167.34)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre;\">0.8</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"53\" y=\"4.199999999999999\" transform=\"translate(0,118.72999999999999)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre;\">0.9</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"53\" y=\"4.199999999999999\" transform=\"translate(0,70.12)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre;\">1</text></g></g><g class=\"overaxes-above\"/></g></g><g class=\"polarlayer\"/><g class=\"ternarylayer\"/><g class=\"geolayer\"/><g class=\"funnelarealayer\"/><g class=\"pielayer\"/><g class=\"treemaplayer\"/><g class=\"sunburstlayer\"/><g class=\"glimages\"/><defs id=\"topdefs-067cca\"><g class=\"clips\"/><clipPath id=\"legend067cca\"><rect width=\"152\" height=\"124\" x=\"0\" y=\"0\"/></clipPath></defs><g class=\"layer-above\"><g class=\"imagelayer\"/><g class=\"shapelayer\"/></g><g class=\"infolayer\"><g class=\"legend\" pointer-events=\"all\" transform=\"translate(536.46,60)\"><rect class=\"bg\" shape-rendering=\"crispEdges\" width=\"152\" height=\"124\" x=\"0\" y=\"0\" style=\"stroke: rgb(68, 68, 68); stroke-opacity: 1; fill: rgb(255, 255, 255); fill-opacity: 1; stroke-width: 0px;\"/><g class=\"scrollbox\" transform=\"\" clip-path=\"url('#legend067cca')\"><g class=\"groups\"><g class=\"traces\" transform=\"translate(0,14.5)\" style=\"opacity: 1;\"><text class=\"legendtext\" text-anchor=\"start\" x=\"40\" y=\"4.680000000000001\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre;\">SVR</text><g class=\"layers\" style=\"opacity: 1;\"><g class=\"legendfill\"/><g class=\"legendlines\"><path class=\"js-line\" d=\"M5,0h30\" style=\"fill: none; stroke: rgb(99, 110, 250); stroke-opacity: 1; stroke-width: 2px;\"/></g><g class=\"legendsymbols\"><g class=\"legendpoints\"><path class=\"scatterpts\" transform=\"translate(20,0)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g></g></g><rect class=\"legendtoggle\" x=\"0\" y=\"-9.5\" width=\"146.71875\" height=\"19\" style=\"fill: rgb(0, 0, 0); fill-opacity: 0;\"/></g><g class=\"traces\" transform=\"translate(0,33.5)\" style=\"opacity: 1;\"><text class=\"legendtext\" text-anchor=\"start\" x=\"40\" y=\"4.680000000000001\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre;\">RandomForest</text><g class=\"layers\" style=\"opacity: 1;\"><g class=\"legendfill\"/><g class=\"legendlines\"><path class=\"js-line\" d=\"M5,0h30\" style=\"fill: none; stroke: rgb(239, 85, 59); stroke-opacity: 1; stroke-width: 2px;\"/></g><g class=\"legendsymbols\"><g class=\"legendpoints\"><path class=\"scatterpts\" transform=\"translate(20,0)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(239, 85, 59); fill-opacity: 1;\"/></g></g></g><rect class=\"legendtoggle\" x=\"0\" y=\"-9.5\" width=\"146.71875\" height=\"19\" style=\"fill: rgb(0, 0, 0); fill-opacity: 0;\"/></g><g class=\"traces\" transform=\"translate(0,52.5)\" style=\"opacity: 1;\"><text class=\"legendtext\" text-anchor=\"start\" x=\"40\" y=\"4.680000000000001\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre;\">NeuralNetwork</text><g class=\"layers\" style=\"opacity: 1;\"><g class=\"legendfill\"/><g class=\"legendlines\"><path class=\"js-line\" d=\"M5,0h30\" style=\"fill: none; stroke: rgb(0, 204, 150); stroke-opacity: 1; stroke-width: 2px;\"/></g><g class=\"legendsymbols\"><g class=\"legendpoints\"><path class=\"scatterpts\" transform=\"translate(20,0)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(0, 204, 150); fill-opacity: 1;\"/></g></g></g><rect class=\"legendtoggle\" x=\"0\" y=\"-9.5\" width=\"146.71875\" height=\"19\" style=\"fill: rgb(0, 0, 0); fill-opacity: 0;\"/></g><g class=\"traces\" transform=\"translate(0,71.5)\" style=\"opacity: 1;\"><text class=\"legendtext\" text-anchor=\"start\" x=\"40\" y=\"4.680000000000001\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre;\">LinearRegression</text><g class=\"layers\" style=\"opacity: 1;\"><g class=\"legendfill\"/><g class=\"legendlines\"><path class=\"js-line\" d=\"M5,0h30\" style=\"fill: none; stroke: rgb(171, 99, 250); stroke-opacity: 1; stroke-width: 2px;\"/></g><g class=\"legendsymbols\"><g class=\"legendpoints\"><path class=\"scatterpts\" transform=\"translate(20,0)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(171, 99, 250); fill-opacity: 1;\"/></g></g></g><rect class=\"legendtoggle\" x=\"0\" y=\"-9.5\" width=\"146.71875\" height=\"19\" style=\"fill: rgb(0, 0, 0); fill-opacity: 0;\"/></g><g class=\"traces\" transform=\"translate(0,90.5)\" style=\"opacity: 1;\"><text class=\"legendtext\" text-anchor=\"start\" x=\"40\" y=\"4.680000000000001\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre;\">GaussianProcesses</text><g class=\"layers\" style=\"opacity: 1;\"><g class=\"legendfill\"/><g class=\"legendlines\"><path class=\"js-line\" d=\"M5,0h30\" style=\"fill: none; stroke: rgb(255, 161, 90); stroke-opacity: 1; stroke-width: 2px;\"/></g><g class=\"legendsymbols\"><g class=\"legendpoints\"><path class=\"scatterpts\" transform=\"translate(20,0)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(255, 161, 90); fill-opacity: 1;\"/></g></g></g><rect class=\"legendtoggle\" x=\"0\" y=\"-9.5\" width=\"146.71875\" height=\"19\" style=\"fill: rgb(0, 0, 0); fill-opacity: 0;\"/></g><g class=\"traces\" transform=\"translate(0,109.5)\" style=\"opacity: 1;\"><text class=\"legendtext\" text-anchor=\"start\" x=\"40\" y=\"4.680000000000001\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre;\">DecisionTree</text><g class=\"layers\" style=\"opacity: 1;\"><g class=\"legendfill\"/><g class=\"legendlines\"><path class=\"js-line\" d=\"M5,0h30\" style=\"fill: none; stroke: rgb(25, 211, 243); stroke-opacity: 1; stroke-width: 2px;\"/></g><g class=\"legendsymbols\"><g class=\"legendpoints\"><path class=\"scatterpts\" transform=\"translate(20,0)\" d=\"M3,0A3,3 0 1,1 0,-3A3,3 0 0,1 3,0Z\" style=\"opacity: 1; stroke-width: 0px; fill: rgb(25, 211, 243); fill-opacity: 1;\"/></g></g></g><rect class=\"legendtoggle\" x=\"0\" y=\"-9.5\" width=\"146.71875\" height=\"19\" style=\"fill: rgb(0, 0, 0); fill-opacity: 0;\"/></g></g></g><rect class=\"scrollbar\" rx=\"20\" ry=\"3\" width=\"0\" height=\"0\" x=\"0\" y=\"0\" style=\"fill: rgb(128, 139, 164); fill-opacity: 1;\"/></g><g class=\"g-gtitle\"><text class=\"gtitle\" x=\"35\" y=\"30\" text-anchor=\"start\" dy=\"0em\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 17px; fill: rgb(42, 63, 95); opacity: 1; font-weight: normal; white-space: pre;\">Value vs Size by Method</text></g><g class=\"g-xtitle\"><text class=\"xtitle\" x=\"290.5\" y=\"487.8\" text-anchor=\"middle\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 14px; fill: rgb(42, 63, 95); opacity: 1; font-weight: normal; white-space: pre;\">Training Data</text></g><g class=\"g-ytitle\" transform=\"translate(1.4873046875,0)\"><text class=\"ytitle\" transform=\"rotate(-90,11.512500000000003,253.5)\" x=\"11.512500000000003\" y=\"253.5\" text-anchor=\"middle\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 14px; fill: rgb(42, 63, 95); opacity: 1; font-weight: normal; white-space: pre;\">Accuracy</text></g></g></svg>"
      ],
      "text/html": [
       "<div\n",
       "    class=\"webio-mountpoint\"\n",
       "    data-webio-mountpoint=\"11168997356497190886\"\n",
       ">\n",
       "    <script>\n",
       "    (function(){\n",
       "    // Some integrations (namely, IJulia/Jupyter) use an alternate render pathway than\n",
       "    // just putting the html on the page. If WebIO isn't defined, then it's pretty likely\n",
       "    // that we're in one of those situations and the integration just isn't installed\n",
       "    // correctly.\n",
       "    if (typeof window.WebIO === \"undefined\") {\n",
       "        document\n",
       "            .querySelector('[data-webio-mountpoint=\"11168997356497190886\"]')\n",
       "            .innerHTML = (\n",
       "                '<div style=\"padding: 1em; background-color: #f8d6da; border: 1px solid #f5c6cb; font-weight: bold;\">' +\n",
       "                '<p><strong>WebIO not detected.</strong></p>' +\n",
       "                '<p>Please read ' +\n",
       "                '<a href=\"https://juliagizmos.github.io/WebIO.jl/latest/troubleshooting/not-detected/\" target=\"_blank\">the troubleshooting guide</a> ' +\n",
       "                'for more information on how to resolve this issue.</p>' +\n",
       "                '<p><a href=\"https://juliagizmos.github.io/WebIO.jl/latest/troubleshooting/not-detected/\" target=\"_blank\">https://juliagizmos.github.io/WebIO.jl/latest/troubleshooting/not-detected/</a></p>' +\n",
       "                '</div>'\n",
       "            );\n",
       "        return;\n",
       "    }\n",
       "    WebIO.mount(\n",
       "        document.querySelector('[data-webio-mountpoint=\"11168997356497190886\"]'),\n",
       "        {\"props\":{},\"nodeType\":\"Scope\",\"type\":\"node\",\"instanceArgs\":{\"imports\":{\"data\":[{\"name\":\"Plotly\",\"type\":\"js\",\"url\":\"\\/assetserver\\/bc3df212c35a76f539ba6c33f43b7e3bf302eaec-plotly.min.js\"},{\"name\":null,\"type\":\"js\",\"url\":\"\\/assetserver\\/7132a0fdf8ff1c416f8a22af8125ab6d49152e49-plotly_webio.bundle.js\"}],\"type\":\"async_block\"},\"id\":\"4259106914597055782\",\"handlers\":{\"_toImage\":[\"(function (options){return this.Plotly.toImage(this.plotElem,options).then((function (data){return WebIO.setval({\\\"name\\\":\\\"image\\\",\\\"scope\\\":\\\"4259106914597055782\\\",\\\"id\\\":\\\"15\\\",\\\"type\\\":\\\"observable\\\"},data)}))})\"],\"__get_gd_contents\":[\"(function (prop){prop==\\\"data\\\" ? (WebIO.setval({\\\"name\\\":\\\"__gd_contents\\\",\\\"scope\\\":\\\"4259106914597055782\\\",\\\"id\\\":\\\"16\\\",\\\"type\\\":\\\"observable\\\"},this.plotElem.data)) : undefined; return prop==\\\"layout\\\" ? (WebIO.setval({\\\"name\\\":\\\"__gd_contents\\\",\\\"scope\\\":\\\"4259106914597055782\\\",\\\"id\\\":\\\"16\\\",\\\"type\\\":\\\"observable\\\"},this.plotElem.layout)) : undefined})\"],\"_downloadImage\":[\"(function (options){return this.Plotly.downloadImage(this.plotElem,options)})\"],\"_commands\":[\"(function (args){var fn=args.shift(); var elem=this.plotElem; var Plotly=this.Plotly; args.unshift(elem); return Plotly[fn].apply(this,args)})\"]},\"systemjs_options\":null,\"mount_callbacks\":[\"function () {\\n    var handler = ((function (Plotly,PlotlyWebIO){PlotlyWebIO.init(WebIO); var gd=this.dom.querySelector(\\\"#plot-1b011ec5-db6c-45fd-acfa-3ee2da4878c1\\\"); this.plotElem=gd; this.Plotly=Plotly; (window.Blink!==undefined) ? (gd.style.width=\\\"100%\\\", gd.style.height=\\\"100vh\\\", gd.style.marginLeft=\\\"0%\\\", gd.style.marginTop=\\\"0vh\\\") : undefined; window.onresize=(function (){return Plotly.Plots.resize(gd)}); Plotly.newPlot(gd,[{\\\"mode\\\":\\\"lines+markers\\\",\\\"y\\\":[0.8160167860894751,0.8247663168562129,0.8081593890919172,0.76823913251253,0.6808623062569504,0.9029837138623951,0.8282269522151673,0.9315108300294325,0.8405960674800167,0.8391153867527228,0.8417935407993532,0.830400415443093,0.38635675480124254],\\\"type\\\":\\\"scatter\\\",\\\"name\\\":\\\"SVR\\\",\\\"x\\\":[1040,960,880,800,720,640,560,480,400,320,240,160,80]},{\\\"mode\\\":\\\"lines+markers\\\",\\\"y\\\":[0.7800504234604334,0.8020543095440783,0.7126656811834016,0.7941879904563341,0.7802949622603794,0.8380302695223842,0.8439468505380945,0.8600438664463016,0.8156674808384938,0.8244762451251747,0.7052643639474491,0.6620291146892634,0.2722096447599147],\\\"type\\\":\\\"scatter\\\",\\\"name\\\":\\\"RandomForest\\\",\\\"x\\\":[1040,960,880,800,720,640,560,480,400,320,240,160,80]},{\\\"mode\\\":\\\"lines+markers\\\",\\\"y\\\":[0.8975308436669005,0.9638284120727647,0.8906213829854822,0.963011658149925,0.9309486463307994,0.8659708430380485,0.9712744187516499,0.913318116273459,0.94464845691769,0.9443271871714275,0.8632433882236102,0.845860734709453,0.8455530592822363],\\\"type\\\":\\\"scatter\\\",\\\"name\\\":\\\"NeuralNetwork\\\",\\\"x\\\":[1040,960,880,800,720,640,560,480,400,320,240,160,80]},{\\\"mode\\\":\\\"lines+markers\\\",\\\"y\\\":[0.6788512222147269,0.7340860598507928,0.7513126831594139,0.7186100586911832,0.7543442857547067,0.6569347607135796,0.6809961517729032,0.5756095530727587,0.7964185142308671,0.8299466930801841,0.7350010358395036,0.5362631388670117,0.5044646484614239],\\\"type\\\":\\\"scatter\\\",\\\"name\\\":\\\"LinearRegression\\\",\\\"x\\\":[1040,960,880,800,720,640,560,480,400,320,240,160,80]},{\\\"mode\\\":\\\"lines+markers\\\",\\\"y\\\":[0.882361392614267,0.9057838649013339,0.8840683867825072,0.7843835746371617,0.7737530988783151,0.9249393028354236,0.8172223338367981,0.9329863416832564,0.8335228398624763,0.8166945901406453,0.7497992344824158,0.9732990386982779,0.4244906298296157],\\\"type\\\":\\\"scatter\\\",\\\"name\\\":\\\"GaussianProcesses\\\",\\\"x\\\":[1040,960,880,800,720,640,560,480,400,320,240,160,80]},{\\\"mode\\\":\\\"lines+markers\\\",\\\"y\\\":[0.7929459633446987,0.6446165413839606,0.5659500443847109,0.7482800129492816,0.7616532400593229,0.7037333839127003,0.8542379054609436,0.8634209180821483,0.8330945361254648,0.8821996293751498,0.5586398935569491,0.5372592055772417,0.5480128381725],\\\"type\\\":\\\"scatter\\\",\\\"name\\\":\\\"DecisionTree\\\",\\\"x\\\":[1040,960,880,800,720,640,560,480,400,320,240,160,80]}],{\\\"xaxis\\\":{\\\"title\\\":{\\\"text\\\":\\\"Training Data\\\"}},\\\"template\\\":{\\\"layout\\\":{\\\"coloraxis\\\":{\\\"colorbar\\\":{\\\"ticks\\\":\\\"\\\",\\\"outlinewidth\\\":0}},\\\"xaxis\\\":{\\\"gridcolor\\\":\\\"white\\\",\\\"zerolinewidth\\\":2,\\\"title\\\":{\\\"standoff\\\":15},\\\"ticks\\\":\\\"\\\",\\\"zerolinecolor\\\":\\\"white\\\",\\\"automargin\\\":true,\\\"linecolor\\\":\\\"white\\\"},\\\"hovermode\\\":\\\"closest\\\",\\\"paper_bgcolor\\\":\\\"white\\\",\\\"geo\\\":{\\\"showlakes\\\":true,\\\"showland\\\":true,\\\"landcolor\\\":\\\"#E5ECF6\\\",\\\"bgcolor\\\":\\\"white\\\",\\\"subunitcolor\\\":\\\"white\\\",\\\"lakecolor\\\":\\\"white\\\"},\\\"colorscale\\\":{\\\"sequential\\\":[[0.0,\\\"#0d0887\\\"],[0.1111111111111111,\\\"#46039f\\\"],[0.2222222222222222,\\\"#7201a8\\\"],[0.3333333333333333,\\\"#9c179e\\\"],[0.4444444444444444,\\\"#bd3786\\\"],[0.5555555555555556,\\\"#d8576b\\\"],[0.6666666666666666,\\\"#ed7953\\\"],[0.7777777777777778,\\\"#fb9f3a\\\"],[0.8888888888888888,\\\"#fdca26\\\"],[1.0,\\\"#f0f921\\\"]],\\\"diverging\\\":[[0,\\\"#8e0152\\\"],[0.1,\\\"#c51b7d\\\"],[0.2,\\\"#de77ae\\\"],[0.3,\\\"#f1b6da\\\"],[0.4,\\\"#fde0ef\\\"],[0.5,\\\"#f7f7f7\\\"],[0.6,\\\"#e6f5d0\\\"],[0.7,\\\"#b8e186\\\"],[0.8,\\\"#7fbc41\\\"],[0.9,\\\"#4d9221\\\"],[1,\\\"#276419\\\"]],\\\"sequentialminus\\\":[[0.0,\\\"#0d0887\\\"],[0.1111111111111111,\\\"#46039f\\\"],[0.2222222222222222,\\\"#7201a8\\\"],[0.3333333333333333,\\\"#9c179e\\\"],[0.4444444444444444,\\\"#bd3786\\\"],[0.5555555555555556,\\\"#d8576b\\\"],[0.6666666666666666,\\\"#ed7953\\\"],[0.7777777777777778,\\\"#fb9f3a\\\"],[0.8888888888888888,\\\"#fdca26\\\"],[1.0,\\\"#f0f921\\\"]]},\\\"yaxis\\\":{\\\"gridcolor\\\":\\\"white\\\",\\\"zerolinewidth\\\":2,\\\"title\\\":{\\\"standoff\\\":15},\\\"ticks\\\":\\\"\\\",\\\"zerolinecolor\\\":\\\"white\\\",\\\"automargin\\\":true,\\\"linecolor\\\":\\\"white\\\"},\\\"shapedefaults\\\":{\\\"line\\\":{\\\"color\\\":\\\"#2a3f5f\\\"}},\\\"hoverlabel\\\":{\\\"align\\\":\\\"left\\\"},\\\"mapbox\\\":{\\\"style\\\":\\\"light\\\"},\\\"polar\\\":{\\\"angularaxis\\\":{\\\"gridcolor\\\":\\\"white\\\",\\\"ticks\\\":\\\"\\\",\\\"linecolor\\\":\\\"white\\\"},\\\"bgcolor\\\":\\\"#E5ECF6\\\",\\\"radialaxis\\\":{\\\"gridcolor\\\":\\\"white\\\",\\\"ticks\\\":\\\"\\\",\\\"linecolor\\\":\\\"white\\\"}},\\\"autotypenumbers\\\":\\\"strict\\\",\\\"font\\\":{\\\"color\\\":\\\"#2a3f5f\\\"},\\\"ternary\\\":{\\\"baxis\\\":{\\\"gridcolor\\\":\\\"white\\\",\\\"ticks\\\":\\\"\\\",\\\"linecolor\\\":\\\"white\\\"},\\\"bgcolor\\\":\\\"#E5ECF6\\\",\\\"caxis\\\":{\\\"gridcolor\\\":\\\"white\\\",\\\"ticks\\\":\\\"\\\",\\\"linecolor\\\":\\\"white\\\"},\\\"aaxis\\\":{\\\"gridcolor\\\":\\\"white\\\",\\\"ticks\\\":\\\"\\\",\\\"linecolor\\\":\\\"white\\\"}},\\\"annotationdefaults\\\":{\\\"arrowhead\\\":0,\\\"arrowwidth\\\":1,\\\"arrowcolor\\\":\\\"#2a3f5f\\\"},\\\"plot_bgcolor\\\":\\\"#E5ECF6\\\",\\\"title\\\":{\\\"x\\\":0.05},\\\"scene\\\":{\\\"xaxis\\\":{\\\"gridcolor\\\":\\\"white\\\",\\\"gridwidth\\\":2,\\\"backgroundcolor\\\":\\\"#E5ECF6\\\",\\\"ticks\\\":\\\"\\\",\\\"showbackground\\\":true,\\\"zerolinecolor\\\":\\\"white\\\",\\\"linecolor\\\":\\\"white\\\"},\\\"zaxis\\\":{\\\"gridcolor\\\":\\\"white\\\",\\\"gridwidth\\\":2,\\\"backgroundcolor\\\":\\\"#E5ECF6\\\",\\\"ticks\\\":\\\"\\\",\\\"showbackground\\\":true,\\\"zerolinecolor\\\":\\\"white\\\",\\\"linecolor\\\":\\\"white\\\"},\\\"yaxis\\\":{\\\"gridcolor\\\":\\\"white\\\",\\\"gridwidth\\\":2,\\\"backgroundcolor\\\":\\\"#E5ECF6\\\",\\\"ticks\\\":\\\"\\\",\\\"showbackground\\\":true,\\\"zerolinecolor\\\":\\\"white\\\",\\\"linecolor\\\":\\\"white\\\"}},\\\"colorway\\\":[\\\"#636efa\\\",\\\"#EF553B\\\",\\\"#00cc96\\\",\\\"#ab63fa\\\",\\\"#FFA15A\\\",\\\"#19d3f3\\\",\\\"#FF6692\\\",\\\"#B6E880\\\",\\\"#FF97FF\\\",\\\"#FECB52\\\"]},\\\"data\\\":{\\\"barpolar\\\":[{\\\"type\\\":\\\"barpolar\\\",\\\"marker\\\":{\\\"line\\\":{\\\"color\\\":\\\"#E5ECF6\\\",\\\"width\\\":0.5}}}],\\\"carpet\\\":[{\\\"aaxis\\\":{\\\"gridcolor\\\":\\\"white\\\",\\\"endlinecolor\\\":\\\"#2a3f5f\\\",\\\"minorgridcolor\\\":\\\"white\\\",\\\"startlinecolor\\\":\\\"#2a3f5f\\\",\\\"linecolor\\\":\\\"white\\\"},\\\"type\\\":\\\"carpet\\\",\\\"baxis\\\":{\\\"gridcolor\\\":\\\"white\\\",\\\"endlinecolor\\\":\\\"#2a3f5f\\\",\\\"minorgridcolor\\\":\\\"white\\\",\\\"startlinecolor\\\":\\\"#2a3f5f\\\",\\\"linecolor\\\":\\\"white\\\"}}],\\\"scatterpolar\\\":[{\\\"type\\\":\\\"scatterpolar\\\",\\\"marker\\\":{\\\"colorbar\\\":{\\\"ticks\\\":\\\"\\\",\\\"outlinewidth\\\":0}}}],\\\"parcoords\\\":[{\\\"line\\\":{\\\"colorbar\\\":{\\\"ticks\\\":\\\"\\\",\\\"outlinewidth\\\":0}},\\\"type\\\":\\\"parcoords\\\"}],\\\"scatter\\\":[{\\\"type\\\":\\\"scatter\\\",\\\"marker\\\":{\\\"colorbar\\\":{\\\"ticks\\\":\\\"\\\",\\\"outlinewidth\\\":0}}}],\\\"histogram2dcontour\\\":[{\\\"colorbar\\\":{\\\"ticks\\\":\\\"\\\",\\\"outlinewidth\\\":0},\\\"type\\\":\\\"histogram2dcontour\\\",\\\"colorscale\\\":[[0.0,\\\"#0d0887\\\"],[0.1111111111111111,\\\"#46039f\\\"],[0.2222222222222222,\\\"#7201a8\\\"],[0.3333333333333333,\\\"#9c179e\\\"],[0.4444444444444444,\\\"#bd3786\\\"],[0.5555555555555556,\\\"#d8576b\\\"],[0.6666666666666666,\\\"#ed7953\\\"],[0.7777777777777778,\\\"#fb9f3a\\\"],[0.8888888888888888,\\\"#fdca26\\\"],[1.0,\\\"#f0f921\\\"]]}],\\\"contour\\\":[{\\\"colorbar\\\":{\\\"ticks\\\":\\\"\\\",\\\"outlinewidth\\\":0},\\\"type\\\":\\\"contour\\\",\\\"colorscale\\\":[[0.0,\\\"#0d0887\\\"],[0.1111111111111111,\\\"#46039f\\\"],[0.2222222222222222,\\\"#7201a8\\\"],[0.3333333333333333,\\\"#9c179e\\\"],[0.4444444444444444,\\\"#bd3786\\\"],[0.5555555555555556,\\\"#d8576b\\\"],[0.6666666666666666,\\\"#ed7953\\\"],[0.7777777777777778,\\\"#fb9f3a\\\"],[0.8888888888888888,\\\"#fdca26\\\"],[1.0,\\\"#f0f921\\\"]]}],\\\"scattercarpet\\\":[{\\\"type\\\":\\\"scattercarpet\\\",\\\"marker\\\":{\\\"colorbar\\\":{\\\"ticks\\\":\\\"\\\",\\\"outlinewidth\\\":0}}}],\\\"mesh3d\\\":[{\\\"colorbar\\\":{\\\"ticks\\\":\\\"\\\",\\\"outlinewidth\\\":0},\\\"type\\\":\\\"mesh3d\\\"}],\\\"surface\\\":[{\\\"colorbar\\\":{\\\"ticks\\\":\\\"\\\",\\\"outlinewidth\\\":0},\\\"type\\\":\\\"surface\\\",\\\"colorscale\\\":[[0.0,\\\"#0d0887\\\"],[0.1111111111111111,\\\"#46039f\\\"],[0.2222222222222222,\\\"#7201a8\\\"],[0.3333333333333333,\\\"#9c179e\\\"],[0.4444444444444444,\\\"#bd3786\\\"],[0.5555555555555556,\\\"#d8576b\\\"],[0.6666666666666666,\\\"#ed7953\\\"],[0.7777777777777778,\\\"#fb9f3a\\\"],[0.8888888888888888,\\\"#fdca26\\\"],[1.0,\\\"#f0f921\\\"]]}],\\\"scattermapbox\\\":[{\\\"type\\\":\\\"scattermapbox\\\",\\\"marker\\\":{\\\"colorbar\\\":{\\\"ticks\\\":\\\"\\\",\\\"outlinewidth\\\":0}}}],\\\"scattergeo\\\":[{\\\"type\\\":\\\"scattergeo\\\",\\\"marker\\\":{\\\"colorbar\\\":{\\\"ticks\\\":\\\"\\\",\\\"outlinewidth\\\":0}}}],\\\"histogram\\\":[{\\\"type\\\":\\\"histogram\\\",\\\"marker\\\":{\\\"colorbar\\\":{\\\"ticks\\\":\\\"\\\",\\\"outlinewidth\\\":0}}}],\\\"pie\\\":[{\\\"type\\\":\\\"pie\\\",\\\"automargin\\\":true}],\\\"choropleth\\\":[{\\\"colorbar\\\":{\\\"ticks\\\":\\\"\\\",\\\"outlinewidth\\\":0},\\\"type\\\":\\\"choropleth\\\"}],\\\"heatmapgl\\\":[{\\\"colorbar\\\":{\\\"ticks\\\":\\\"\\\",\\\"outlinewidth\\\":0},\\\"type\\\":\\\"heatmapgl\\\",\\\"colorscale\\\":[[0.0,\\\"#0d0887\\\"],[0.1111111111111111,\\\"#46039f\\\"],[0.2222222222222222,\\\"#7201a8\\\"],[0.3333333333333333,\\\"#9c179e\\\"],[0.4444444444444444,\\\"#bd3786\\\"],[0.5555555555555556,\\\"#d8576b\\\"],[0.6666666666666666,\\\"#ed7953\\\"],[0.7777777777777778,\\\"#fb9f3a\\\"],[0.8888888888888888,\\\"#fdca26\\\"],[1.0,\\\"#f0f921\\\"]]}],\\\"bar\\\":[{\\\"type\\\":\\\"bar\\\",\\\"error_y\\\":{\\\"color\\\":\\\"#2a3f5f\\\"},\\\"error_x\\\":{\\\"color\\\":\\\"#2a3f5f\\\"},\\\"marker\\\":{\\\"line\\\":{\\\"color\\\":\\\"#E5ECF6\\\",\\\"width\\\":0.5}}}],\\\"heatmap\\\":[{\\\"colorbar\\\":{\\\"ticks\\\":\\\"\\\",\\\"outlinewidth\\\":0},\\\"type\\\":\\\"heatmap\\\",\\\"colorscale\\\":[[0.0,\\\"#0d0887\\\"],[0.1111111111111111,\\\"#46039f\\\"],[0.2222222222222222,\\\"#7201a8\\\"],[0.3333333333333333,\\\"#9c179e\\\"],[0.4444444444444444,\\\"#bd3786\\\"],[0.5555555555555556,\\\"#d8576b\\\"],[0.6666666666666666,\\\"#ed7953\\\"],[0.7777777777777778,\\\"#fb9f3a\\\"],[0.8888888888888888,\\\"#fdca26\\\"],[1.0,\\\"#f0f921\\\"]]}],\\\"contourcarpet\\\":[{\\\"colorbar\\\":{\\\"ticks\\\":\\\"\\\",\\\"outlinewidth\\\":0},\\\"type\\\":\\\"contourcarpet\\\"}],\\\"table\\\":[{\\\"type\\\":\\\"table\\\",\\\"header\\\":{\\\"line\\\":{\\\"color\\\":\\\"white\\\"},\\\"fill\\\":{\\\"color\\\":\\\"#C8D4E3\\\"}},\\\"cells\\\":{\\\"line\\\":{\\\"color\\\":\\\"white\\\"},\\\"fill\\\":{\\\"color\\\":\\\"#EBF0F8\\\"}}}],\\\"scatter3d\\\":[{\\\"line\\\":{\\\"colorbar\\\":{\\\"ticks\\\":\\\"\\\",\\\"outlinewidth\\\":0}},\\\"type\\\":\\\"scatter3d\\\",\\\"marker\\\":{\\\"colorbar\\\":{\\\"ticks\\\":\\\"\\\",\\\"outlinewidth\\\":0}}}],\\\"scattergl\\\":[{\\\"type\\\":\\\"scattergl\\\",\\\"marker\\\":{\\\"colorbar\\\":{\\\"ticks\\\":\\\"\\\",\\\"outlinewidth\\\":0}}}],\\\"histogram2d\\\":[{\\\"colorbar\\\":{\\\"ticks\\\":\\\"\\\",\\\"outlinewidth\\\":0},\\\"type\\\":\\\"histogram2d\\\",\\\"colorscale\\\":[[0.0,\\\"#0d0887\\\"],[0.1111111111111111,\\\"#46039f\\\"],[0.2222222222222222,\\\"#7201a8\\\"],[0.3333333333333333,\\\"#9c179e\\\"],[0.4444444444444444,\\\"#bd3786\\\"],[0.5555555555555556,\\\"#d8576b\\\"],[0.6666666666666666,\\\"#ed7953\\\"],[0.7777777777777778,\\\"#fb9f3a\\\"],[0.8888888888888888,\\\"#fdca26\\\"],[1.0,\\\"#f0f921\\\"]]}],\\\"scatterternary\\\":[{\\\"type\\\":\\\"scatterternary\\\",\\\"marker\\\":{\\\"colorbar\\\":{\\\"ticks\\\":\\\"\\\",\\\"outlinewidth\\\":0}}}],\\\"scatterpolargl\\\":[{\\\"type\\\":\\\"scatterpolargl\\\",\\\"marker\\\":{\\\"colorbar\\\":{\\\"ticks\\\":\\\"\\\",\\\"outlinewidth\\\":0}}}]}},\\\"margin\\\":{\\\"l\\\":50,\\\"b\\\":50,\\\"r\\\":50,\\\"t\\\":60},\\\"title\\\":\\\"Value vs Size by Method\\\",\\\"yaxis\\\":{\\\"title\\\":{\\\"text\\\":\\\"Accuracy\\\"}}},{\\\"showLink\\\":false,\\\"editable\\\":false,\\\"responsive\\\":true,\\\"staticPlot\\\":false,\\\"scrollZoom\\\":true}); gd.on(\\\"plotly_hover\\\",(function (data){var filtered_data=WebIO.PlotlyCommands.filterEventData(gd,data,\\\"hover\\\"); return !(filtered_data.isnil) ? (WebIO.setval({\\\"name\\\":\\\"hover\\\",\\\"scope\\\":\\\"4259106914597055782\\\",\\\"id\\\":\\\"11\\\",\\\"type\\\":\\\"observable\\\"},filtered_data.out)) : undefined})); gd.on(\\\"plotly_unhover\\\",(function (){return WebIO.setval({\\\"name\\\":\\\"hover\\\",\\\"scope\\\":\\\"4259106914597055782\\\",\\\"id\\\":\\\"11\\\",\\\"type\\\":\\\"observable\\\"},{})})); gd.on(\\\"plotly_selected\\\",(function (data){var filtered_data=WebIO.PlotlyCommands.filterEventData(gd,data,\\\"selected\\\"); return !(filtered_data.isnil) ? (WebIO.setval({\\\"name\\\":\\\"selected\\\",\\\"scope\\\":\\\"4259106914597055782\\\",\\\"id\\\":\\\"12\\\",\\\"type\\\":\\\"observable\\\"},filtered_data.out)) : undefined})); gd.on(\\\"plotly_deselect\\\",(function (){return WebIO.setval({\\\"name\\\":\\\"selected\\\",\\\"scope\\\":\\\"4259106914597055782\\\",\\\"id\\\":\\\"12\\\",\\\"type\\\":\\\"observable\\\"},{})})); gd.on(\\\"plotly_relayout\\\",(function (data){var filtered_data=WebIO.PlotlyCommands.filterEventData(gd,data,\\\"relayout\\\"); return !(filtered_data.isnil) ? (WebIO.setval({\\\"name\\\":\\\"relayout\\\",\\\"scope\\\":\\\"4259106914597055782\\\",\\\"id\\\":\\\"14\\\",\\\"type\\\":\\\"observable\\\"},filtered_data.out)) : undefined})); return gd.on(\\\"plotly_click\\\",(function (data){var filtered_data=WebIO.PlotlyCommands.filterEventData(gd,data,\\\"click\\\"); return !(filtered_data.isnil) ? (WebIO.setval({\\\"name\\\":\\\"click\\\",\\\"scope\\\":\\\"4259106914597055782\\\",\\\"id\\\":\\\"13\\\",\\\"type\\\":\\\"observable\\\"},filtered_data.out)) : undefined}))}));\\n    (WebIO.importBlock({\\\"data\\\":[{\\\"name\\\":\\\"Plotly\\\",\\\"type\\\":\\\"js\\\",\\\"url\\\":\\\"\\/assetserver\\/bc3df212c35a76f539ba6c33f43b7e3bf302eaec-plotly.min.js\\\"},{\\\"name\\\":null,\\\"type\\\":\\\"js\\\",\\\"url\\\":\\\"\\/assetserver\\/7132a0fdf8ff1c416f8a22af8125ab6d49152e49-plotly_webio.bundle.js\\\"}],\\\"type\\\":\\\"async_block\\\"})).then((imports) => handler.apply(this, imports));\\n}\\n\"],\"observables\":{\"_toImage\":{\"sync\":false,\"id\":\"18\",\"value\":{}},\"hover\":{\"sync\":false,\"id\":\"11\",\"value\":{}},\"selected\":{\"sync\":false,\"id\":\"12\",\"value\":{}},\"__gd_contents\":{\"sync\":false,\"id\":\"16\",\"value\":{}},\"click\":{\"sync\":false,\"id\":\"13\",\"value\":{}},\"image\":{\"sync\":true,\"id\":\"15\",\"value\":\"\"},\"__get_gd_contents\":{\"sync\":false,\"id\":\"20\",\"value\":\"\"},\"_downloadImage\":{\"sync\":false,\"id\":\"19\",\"value\":{}},\"relayout\":{\"sync\":false,\"id\":\"14\",\"value\":{}},\"_commands\":{\"sync\":false,\"id\":\"17\",\"value\":[]}}},\"children\":[{\"props\":{\"id\":\"plot-1b011ec5-db6c-45fd-acfa-3ee2da4878c1\"},\"nodeType\":\"DOM\",\"type\":\"node\",\"instanceArgs\":{\"namespace\":\"html\",\"tag\":\"div\"},\"children\":[]}]},\n",
       "        window,\n",
       "    );\n",
       "    })()\n",
       "    </script>\n",
       "</div>\n"
      ],
      "text/plain": [
       "data: [\n",
       "  \"scatter with fields mode, name, type, x, and y\",\n",
       "  \"scatter with fields mode, name, type, x, and y\",\n",
       "  \"scatter with fields mode, name, type, x, and y\",\n",
       "  \"scatter with fields mode, name, type, x, and y\",\n",
       "  \"scatter with fields mode, name, type, x, and y\",\n",
       "  \"scatter with fields mode, name, type, x, and y\"\n",
       "]\n",
       "\n",
       "layout: \"layout with fields margin, template, title, xaxis, and yaxis\"\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "traces = AbstractTrace[]  # Correct type for individual traces\n",
    "sort!(df_full, [:Method, :Size], rev=true)\n",
    "\n",
    "for m in unique(df_full.Method)\n",
    "    subdf = filter(:Method => ==(m), df_full)\n",
    "    trace = scatter(\n",
    "        x = subdf.Size,\n",
    "        y = subdf.Value,\n",
    "        mode = \"lines+markers\",\n",
    "        name = string(m)\n",
    "    )\n",
    "    push!(traces, trace)\n",
    "end\n",
    "\n",
    "# Define the layout\n",
    "layout = Layout(\n",
    "    title = \"Value vs Size by Method\",\n",
    "    xaxis_title = \"Training Data\",\n",
    "    yaxis_title = \"Accuracy\"\n",
    ")\n",
    "\n",
    "# Create a single Plot from traces and layout\n",
    "plt = plot(traces, layout)\n",
    "\n",
    "# Show the plot\n",
    "display(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-sampling strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = filter(f -> endswith(f, \".txt\"), readdir(dir, join=true));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 50\n",
    "df_full = DataFrame(Method=String[],  Value=Float64[], Size=Int[]);\n",
    "\n",
    "\n",
    "### ALTERNATIVE 1: RANDOM SAMPLING ### \n",
    "for n in 140:step_size:size(df_raw)[1] #unique(ceil(Int, size(df_raw)[1]/n) for n in 140:step_size:size(df_raw)[1])\n",
    "    df = df_raw[StatsBase.sample(1:nrow(df_raw), n; replace=false), :]\n",
    "end\n",
    "\n",
    "### ALTERNATIVE 2: EQUAL SELECTION SAMPLING ### \n",
    "for n in unique(ceil(Int, size(df_raw)[1]/n) for n in 140:step_size:size(df_raw)[1])\n",
    "    df = df_raw[1:n:end, :]\n",
    "    df = select(df, names(df)[[sum(df[!, col]) != 0 for col in names(df)]])\n",
    "end\n",
    "\n",
    "### ALTERNATIVE 3: LHS ### \n",
    "for f in files_list\n",
    "    df = P2H_CapacityExpansion.read_txt_file(f)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read, normalize and transpose the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = P2H_CapacityExpansion.read_txt_file(file);\n",
    "df_raw = select(df_raw, Not(:Cost))\n",
    "m = Matrix(df_raw)\n",
    "x_norm, μ, σ = P2H_CapacityExpansion.scaling(m)\n",
    "X = x_norm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate PCA model\n",
    "model = fit(PCA, X; maxoutdim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose the data back again \n",
    "X_transform = MultivariateStats.transform(model, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca =   DataFrame(permutedims(X_transform), :auto)\n",
    "df_pca.Cost = df_raw.Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.reddit.com/r/deeplearning/comments/14vnfe8/how_to_decrease_high_loss_values/\n",
    "https://discourse.julialang.org/t/how-to-efficiently-and-precisely-fit-a-function-with-neural-networks/73726\n",
    "https://stackoverflow.com/questions/59153248/why-is-my-neural-network-stuck-at-high-loss-value-after-the-first-epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = DataFrame(Method=String[],  Value=Float64[], Size=Int[]);\n",
    "\n",
    "df = P2H_CapacityExpansion.read_txt_file(file);\n",
    "\n",
    "### split the data into test and training ###\n",
    "X_train, y_train, X_test, y_test = P2H_CapacityExpansion.partitionTrainTest(df, :Cost, 0.7)\n",
    "\n",
    "### scale the data ###\n",
    "X_train_scaled, μX, σX  = P2H_CapacityExpansion.scaling(X_train)\n",
    "X_test_scaled = (X_test .- μX) ./ σX\n",
    "\n",
    "    # remove np.nan #\n",
    "for i in eachindex(X_test_scaled)\n",
    "    if isnan(X_test_scaled[i])\n",
    "        X_test_scaled[i] = 0.0\n",
    "    end\n",
    "end\n",
    "y_train_scaled, μy, σy  = P2H_CapacityExpansion.scaling(y_train)\n",
    "\n",
    "### train ML model and compute R2 ### \n",
    "for (name,fun) ∈ models\n",
    "    sg = fun(X_train_scaled, y_train_scaled, X_test_scaled)\n",
    "    ŷ_rescaled = sg.prediction .* σy .+ μy\n",
    "    r2 = P2H_CapacityExpansion.r2_score(y_test, ŷ_rescaled)\n",
    "\n",
    "    ### add to the df ### \n",
    "    push!(df_full, (Method = name, Value = r2, Size = size(X_train)[1]))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = DataFrame(Method=String[],  Value=Float64[], Size=Int[]);\n",
    "\n",
    "df = df_pca\n",
    "\n",
    "### split the data into test and training ###\n",
    "X_train, y_train, X_test, y_test = P2H_CapacityExpansion.partitionTrainTest(df, :Cost, 0.7)\n",
    "\n",
    "### scale the data ###\n",
    "X_train_scaled, μX, σX  = P2H_CapacityExpansion.scaling(X_train)\n",
    "X_test_scaled = (X_test .- μX) ./ σX\n",
    "\n",
    "# remove np.nan #\n",
    "for i in eachindex(X_test_scaled)\n",
    "    if isnan(X_test_scaled[i])\n",
    "        X_test_scaled[i] = 0.0\n",
    "    end\n",
    "end\n",
    "y_train_scaled, μy, σy  = P2H_CapacityExpansion.scaling(y_train)\n",
    "\n",
    "### train ML model and compute R2 ### \n",
    "for (name,fun) ∈ models\n",
    "    sg = fun(X_train_scaled, y_train_scaled, X_test_scaled)\n",
    "    ŷ_rescaled = sg.prediction .* σy .+ μy\n",
    "    r2 = P2H_CapacityExpansion.r2_score(y_test, ŷ_rescaled)\n",
    "\n",
    "    ### add to the df ### \n",
    "    push!(df_full, (Method = name, Value = r2, Size = size(X_train)[1]))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new sampling technique\n",
    "iterate more often through the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Loop through files with index\n",
    "\n",
    "# Parameters\n",
    "techs = setdiff([key for (key, val) ∈ config[\"techs\"] if get(val, \"inv\", \"\")  == true], [key for (key, val) ∈ config[\"techs\"] if get(val, \"tech_group\", \"\")  == \"transmission\"] ) \n",
    "years = config[\"year\"]\n",
    "scenarios = 1:length(txt_files)\n",
    "\n",
    "# Column names\n",
    "columns = [:scenario, :year, :cost] ∪ Symbol.(techs)\n",
    "\n",
    "df = DataFrame(;\n",
    "    :scenario => repeat(scenarios, inner=length(years)),\n",
    "    :year => repeat(years, outer=length(txt_files)),\n",
    "    :cost => fill(0.0, length(txt_files)*length(years)),\n",
    ")\n",
    "\n",
    "# Add technology columns, initialized to 0.0\n",
    "for tech in techs\n",
    "    df[!, Symbol(tech)] = fill(0.0, length(txt_files)*length(years))\n",
    "end\n",
    "\n",
    "## fill in the values\n",
    "for file in txt_files\n",
    "    lines = readlines(file)\n",
    "    \n",
    "    i = parse(Int64, split(split(file, \"/\")[end], \"_\")[1])\n",
    "\n",
    "    # Parse technology capacities\n",
    "    for line in lines\n",
    "        if occursin(\"TotalCapacityAnnual\", line)\n",
    "            g = split(line, \",\")[2]   \n",
    "            if g in techs\n",
    "                val = parse(Float64, strip(split(line, \"=\")[2]))\n",
    "                y = parse(Int64, split(split(line, \"]\")[1], \",\")[end])\n",
    "\n",
    "                # insert into the dataframe\n",
    "                idx = findfirst((df.year .== y) .& (df.scenario .== i))\n",
    "                df[idx, Symbol(g)] = val\n",
    "            end\n",
    "\n",
    "        \n",
    "        elseif occursin(\"COSTvar\", line)\n",
    "            y = parse(Int64, line[8:12])\n",
    "            val = parse(Float64, strip(split(line, \"=\")[2]))\n",
    "            # insert into the dataframe\n",
    "            idx = findfirst((df.year .== y) .& (df.scenario .== i))\n",
    "            df[idx, :cost] = val\n",
    "        end\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = df[:, Not(:year, :cost, :scenario)]\n",
    "X = transpose(Matrix(df_agg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ub = [1.25, 620, 460, 300, 0.06, 0.93, 20]\n",
    "lb = [0.75, 420, 260, 100, 0.015, 0.56, 13]\n",
    "\n",
    "\n",
    "# Number of samples\n",
    "n = 3\n",
    "\n",
    "# Latin Hypercube Sampling\n",
    "scenarios = Surrogates.sample(n,lb,ub, Surrogates.LatinHypercubeSample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. TRAIN THE MODEL #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function perclass_splits(y, percent)\n",
    "    uniq_class = unique(y)\n",
    "    keep_index = []\n",
    "    for class in uniq_class\n",
    "        class_index = findall(y .== class)\n",
    "        row_index = randsubseq(class_index, percent)\n",
    "        push!(keep_index, row_index...)\n",
    "    end\n",
    "    return keep_index\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[!, :cost]\n",
    "\n",
    "# split data between train and test\n",
    "Random.seed!(1)\n",
    "train_index = perclass_splits(y, 0.67)\n",
    "test_index = setdiff(1:length(y), train_index)\n",
    "\n",
    "# spit features\n",
    "X_train = X[:, train_index]\n",
    "X_test = X[:, test_index]\n",
    "\n",
    "# split classes\n",
    "y_train = transpose(Array{Float64}(y[train_index]))\n",
    "y_test = transpose(Array{Float64}(y[test_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Chain(\n",
    "    Dense(12, 32, relu),\n",
    "    Dense(32, 1)  # output: a single float\n",
    ")\n",
    "\n",
    "loss(x, y) = Flux.Losses.mse(model(x), y)  # or Flux.Losses.mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track parameters\n",
    "ps = Flux.params(model)\n",
    " # select an optimizer\n",
    "learning_rate = 0.01\n",
    "opt = ADAM(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "loss_history = []\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "for epoch in 1:epochs\n",
    "    # train the model\n",
    "    train!(loss, ps, [(X_train, y_train)], opt)\n",
    "    # print report\n",
    "    train_loss = loss(X_train, y_train)\n",
    "    push!(loss_history, train_loss)\n",
    "    println(\"Epoch = $epoch : Training loss = $train_loss\")\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a combined scenario-year column\n",
    "df_long[!, :scenario_year] = string.(df_long.scenario, \"_\", df_long.year)\n",
    "\n",
    "# Step 2: Pivot wide: rows = technology, columns = scenario_year, values = value\n",
    "df_wide = unstack(df_long, :technology, :scenario_year, :value)\n",
    "\n",
    "# Show the result as a matrix\n",
    "X = Matrix(df_wide[:, Not(:technology)])\n",
    "X = coalesce.(X, 0.0)\n",
    "#https://medium.com/@mandarangchekar7/a-neural-network-explained-and-implemented-in-julia-1fbfe4aaf0df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_hat_raw = model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = onecold(y_hat_raw) .- 1\n",
    "y = y_test_raw\n",
    "mean(y_hat .== y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
